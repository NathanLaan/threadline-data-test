{
  "feedId": "f26375bf-f6ab-4f9f-8fbc-34062b58052e",
  "entries": [
    {
      "id": "8206a62a-a5b9-440e-9243-d6a0c21b0e58",
      "guid": "https://spectrum.ieee.org/perseverance-rover-nasa-anthropic-ai",
      "title": "NASA Let AI Drive the Perseverance Rover",
      "link": "https://spectrum.ieee.org/perseverance-rover-nasa-anthropic-ai",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/still-image-from-a-3d-animation-of-nasa-s-perseverance-rover-driving-over-mars-rough-terrain.jpg?id=64437492&width=1245&height=700&coordinates=0%2C145%2C0%2C146\"/><br/><br/><p><span>In December, NASA took another small, incremental step towards autonomous surface rovers. In a demonstration, the Perseverance team <a href=\"https://www.jpl.nasa.gov/news/nasas-perseverance-rover-completes-first-ai-planned-drive-on-mars/\" target=\"_blank\">used AI to generate the rover’s waypoints</a>. Perseverance used the AI waypoints on two separate days, traveling a total of 456 meters without human control.</span></p><p>“This demonstration shows how far our capabilities have advanced and broadens how we will explore other worlds,” said NASA Administrator <a href=\"https://www.nasa.gov/people/jared-isaacman/\" target=\"_blank\">Jared Isaacman</a>. “Autonomous technologies like this can help missions to operate more efficiently, respond to challenging terrain, and increase science return as distance from Earth grows. It’s a strong example of teams applying new technology carefully and responsibly in real operations.”</p><h3></h3><br/><div class=\"badge_module shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25\">\n<a class=\"rm-stats-tracked\" href=\"https://www.universetoday.com/\" target=\"_blank\">\n<img alt='Universe Today logo; text reads \"This post originally appeared on Universe Today.\"' class=\"rm-shortcode rm-lazyloadable-image\" src=\"https://spectrum.ieee.org/media-library/universe-today-logo-text-reads-this-post-originally-appeared-on-universe-today.png?id=60568425&width=1800&quality=85\"/></a>\n</div>\n<p>Mars is a long way away, and there’s about a 25-minute delay for a round trip signal between Earth and Mars. That means that one way or another, rovers are on their own for short periods of time.</p><p>The delay shapes the route-planning process. Rover drivers here on Earth examine images and elevation data and program a series of waypoints, which usually don’t exceed 100 meters apart. The driving plan is sent to <a href=\"https://eyes.nasa.gov/apps/dsn-now/\" target=\"_blank\">NASA’s Deep Space Network</a> (DSN), which transmits it to one of several orbiters, which then relay it to Perseverance. (Perseverance can receive direct comms from the DSN as a back up, but the data rate is slower.)</p><h2>AI Enhances Mars Rover Navigation</h2><p>In this demonstration, the AI model analyzed orbital images from the Mars Reconnaissance Orbiter’s <a href=\"https://www.uahirise.org/\" target=\"_blank\">HiRISE camera</a>, as well as digital elevation models. The AI, which is based on <a data-linked-post=\"2672366203\" href=\"https://spectrum.ieee.org/best-ai-coding-tools\" target=\"_blank\">Anthropic’s Claude AI</a>, identified hazards like sand traps, boulder fields, bedrock, and rocky outcrops. Then it generated a path defined by a series of waypoints that avoids the hazards. From there, Perseverance’s auto-navigation system took over. It has more autonomy than its predecessors and can process images and driving plans while in motion.</p><p>There was another important step before these waypoints were transmitted to Perseverance. NASA’s Jet Propulsion Laboratory has a “twin” for Perseverance called the “Vehicle System Test Bed” (VSTB) in JPL’s <a href=\"https://www-robotics.jpl.nasa.gov/how-we-do-it/facilities/marsyard-iii/\" target=\"_blank\">Mars Yard</a>. It’s an engineering model that the team can work with here on Earth to solve problems, or for situations like this. These engineering versions are common on Mars missions, and JPL has one for Curiosity, too.</p><p>“The fundamental elements of generative AI are showing a lot of promise in streamlining the pillars of autonomous navigation for off-planet driving: perception (seeing the rocks and ripples), localization (knowing where we are), and planning and control (deciding and executing the safest path),” said Vandi Verma, a space roboticist at JPL and a member of the Perseverance engineering team. “We are moving towards a day where generative AI and other smart tools will help our surface rovers handle kilometer-scale drives while minimizing operator workload, and flag interesting surface features for our science team by scouring huge volumes of rover images.”</p><h2>AI’s Expanding Role in Space Exploration</h2><p><span><a href=\"https://spectrum.ieee.org/ai-agents\" target=\"_blank\">AI is rapidly becoming ubiquitous in our lives</a>, showing up in places that don’t necessarily have a strong use case for it. But this isn’t NASA hopping on the AI bandwagon. They’ve been developing automatic navigation systems for a while, out of necessity. In fact, Perseverance’s primary means of driving is its self-driving autonomous navigation system.</span></p><p>One thing that prevents fully-autonomous driving is the way uncertainty grows as the rover operates without human assistance. The longer the rover travels, the more uncertain it becomes about its position on the surface. The solution is to re-localize the rover on its map. Currently, humans do this. But this takes time, including a complete communication cycle between Earth and Mars. Overall, it limits how far Perseverance can go without a helping hand.</p><p>NASA/JPL is also working on a way that Perseverance can <a href=\"https://www-robotics.jpl.nasa.gov/media/documents/2024_Global_Localization_ICRA.pdf\" target=\"_blank\">use AI to re-localize</a>. The main roadblock is matching orbital images with the rover’s ground-level images. It seems highly likely that AI will be trained to excel at this.</p><p>It’s obvious that AI is set to play a much larger role in planetary exploration. The next Mars rover may be much different than current ones, with more advanced autonomous navigation and other AI features. There are already concepts for a swarm of <a href=\"https://spectrum.ieee.org/mars-helicopter-ingenuity-end-mission\" target=\"_blank\">flying drones released by a rover</a> to expand its explorative reach on Mars. These swarms would be controlled by AI to work together and autonomously.</p><p>And it’s not just Mars exploration that will benefit from AI. NASA’s <a href=\"https://en.wikipedia.org/wiki/Dragonfly_(Titan_space_probe)#\" target=\"_blank\">Dragonfly</a> mission to Saturn’s moon Titan will make extensive use of AI. Not only for autonomous navigation as the rotorcraft flies around, but also for autonomous data curation.</p><p>“Imagine intelligent systems not only on the ground at Earth, but also in edge applications in our rovers, helicopters, drones, and other surface elements trained with the collective wisdom of our NASA engineers, scientists, and astronauts,” said Matt Wallace, manager of JPL’s Exploration Systems Office. “That is the game-changing technology we need to establish the infrastructure and systems required for a permanent human presence on the Moon and take the U.S. to Mars and beyond.”</p>",
      "author": "Evan Gough",
      "publishedAt": "2026-02-15T14:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "a1aa9e83-5ff4-4fe5-89db-cb7f36921abc",
      "guid": "https://spectrum.ieee.org/solid-state-lidar-microvision-adas",
      "title": "Sub-$200 Lidar Could Reshuffle  Auto Sensor Economics",
      "link": "https://spectrum.ieee.org/solid-state-lidar-microvision-adas",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/close-up-of-microvision-s-movia-s-lidar-unit.jpg?id=64423662&width=1245&height=700&coordinates=0%2C187%2C0%2C188\"/><br/><br/><p><a href=\"https://microvision.com/?gad_source=1&gad_campaignid=21340770932&gbraid=0AAAAApwXbe5Ghi8hWwJCToToqvbQ8BYwF&gclid=CjwKCAiAqKbMBhBmEiwAZ3UboOY9pR0tGoc42GaZ--CDqFzYKG_Hy0C5eGuz7BgFbJEvneRuVuEvmhoCPD4QAvD_BwE\" rel=\"noopener noreferrer\" target=\"_blank\">MicroVision</a><span>, a solid-state sensor technology company located in Redmond, Wash., says it has designed a solid-state automotive </span><a href=\"https://spectrum.ieee.org/tag/lidar\" target=\"_self\">lidar</a><span> sensor intended to reach production pricing below US $200. That’s less than half of typical prices now, and it’s not even the full extent of the company’s ambition. The company says its longer-term goal is $100 per unit. MicroVision’s claim, which, if realized, would place lidar within reach of </span><a href=\"https://spectrum.ieee.org/tag/adas\" target=\"_self\">advanced driver-assistance systems</a><span> (ADAS) rather than limiting it to high-end </span><a href=\"https://spectrum.ieee.org/tag/autonomous-vehicles\" target=\"_self\">autonomous vehicle</a><span> programs. Lidar’s limited market penetration comes down to one issue: cost.</span><span></span></p><p>Comparable mechanical lidars from multiple suppliers now sell in the $10,000 to $20,000 range. That price roughly tenfold drop, from about $80,000, helps explain why suppliers now are now hopeful that another steep price reduction is on the horizon.</p><p>For solid-state devices, “it is feasible to bring the cost down even more when manufacturing at high volume,” <span>says </span><a href=\"https://engineering.msu.edu/directory/faculty/radha\" target=\"_blank\">Hayder Radha</a><span>, a professor of electrical and computer engineering at </span><a href=\"https://msu.edu/\" target=\"_blank\">Michigan State University</a> and director of the school’s <a href=\"https://canvas.msu.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Connected & Autonomous Networked Vehicles for Active Safety</a> program. With demand expanding beyond fully autonomous vehicles into driver-assistance applications, “one order or even two orders of magnitude reduction in cost are feasible.”</p><p>“We are focused on delivering automotive-grade lidar that can actually be deployed at scale,” says MicroVision CEO <a href=\"https://ir.microvision.com/news/press-releases/detail/430/microvision-appoints-glen-devos-as-chief-executive-officer\" target=\"_blank\">Glen DeVos</a>. “That means designing for cost, manufacturability, and integration from the start—not treating price as an afterthought.”</p><h2>MicroVision’s Lidar System</h2><p><a href=\"https://www.tesla.com/\" target=\"_blank\">Tesla</a> CEO <a href=\"https://www.tesla.com/elon-musk\" rel=\"noopener noreferrer\" target=\"_blank\">Elon Musk</a> famously dismissed lidar in 2019 as “<a href=\"https://professional.mit.edu/news/articles/lidar-fools-errand\" rel=\"noopener noreferrer\" target=\"_blank\">a fool’s errand</a>,” arguing that cameras and radar alone were sufficient for automated driving. A credible path to sub-$200 pricing would fundamentally alter the calculus of autonomous-car design by lowering the cost of adding precise three-dimensional sensing to mainstream vehicles. The shift reflects a broader industry trend toward solid-state lidar designs optimized for low-cost, high-volume manufacturing rather than maximum range or resolution.</p><p>Before those economics can be evaluated, however, it’s important to understand what MicroVision is proposing to build.</p><p>The company’s <a href=\"https://microvision.com/sensors/movia-s\" rel=\"noopener noreferrer\" target=\"_blank\">Movia S</a> is a solid-state lidar. Mounted at the corners of a vehicle, the sensor sends out 905-nanometer-wavelength laser pulses and measures how long it takes for light reflected from the surfaces of nearby objects to return. The arrangement of the beam emitters and receivers provides a fixed field of view designed for 180-degree horizontal coverage rather than full 360-degree scanning typical of traditional mechanical units. The company says the unit can detect objects at distances of up to roughly 200 meters under favorable weather conditions—compared with the roughly 300-meter radius scanned by mechanical systems—and supports frame rates suitable for real-time perception in driver-assistance systems. Earlier mechanical lidars, used spinning components to steer their beams but the Movia S is a phased-arraysystem. It controls the amplitude and phase of the signals across an array of antenna elements to steer the beam. The unit is designed to meet automotive requirements for vibration tolerance, temperature range, and environmental sealing.</p><p>MicroVision’s pricing targets might sound aggressive, but they are not without precedent. The lidar industry has already experienced one major cost reset over the past decade.</p><p class=\"pull-quote\">“Automakers are not buying a single sensor in isolation... They are designing a perception system, and cost only matters if the system as a whole is viable.” <strong>–Glen DeVos, MicroVision</strong></p><p>Around 2016 and 2017, mechanical lidar systems used in early autonomous driving research often sold for close to $100,000. Those units relied on spinning assemblies to sweep laser beams across a full 360 degrees, which made them expensive to build and difficult to ruggedize for consumer vehicles.</p><p>“Back then, a 64-beam <a href=\"https://investors.ouster.com/news-releases/news-release-details/ouster-and-velodyne-complete-merger-equals-accelerate-lidar\" rel=\"noopener noreferrer\" target=\"_blank\">Velodyne</a> lidar cost around $80,000,” says Radha.</p><p>Comparable mechanical lidars from multiple suppliers now sell in the $10,000 to $20,000 range. That roughly tenfold drop helps explain why suppliers now believe another steep price reduction is possible. </p><p>“For solid-state devices, it is feasible to bring the cost down even more when manufacturing at high volume,” Radha says. With demand expanding beyond fully <a href=\"https://spectrum.ieee.org/tag/autonomous-vehicles\" target=\"_self\">autonomous vehicles</a> into driver-assistance applications, “one order or even two orders of magnitude reduction in cost are feasible.”</p><h2>Solid-State Lidar Design Challenges</h2><p>Lower cost, however, does not come for free. The same design choices that enable solid-state lidar to scale also introduce new constraints.</p><p>“Unlike mechanical lidars, which provide full 360-degree coverage, solid-state lidars tend to have a much smaller field of view,” Radha says. Many cover 180 degrees or less.</p><p>That limitation shifts the burden from the sensor to the system. Automakers will need to deploy three or four solid-state lidars around a vehicle to achieve full coverage. Even so, Radha notes, the total cost can still undercut that of a single mechanical unit.</p><p>What changes is integration. Multiple sensors must be aligned, calibrated, and synchronized so their data can be fused accurately. The engineering is manageable, but it adds complexity that price targets alone do not capture.</p><p>DeVos says MicroVision’s design choices reflect that reality. “Automakers are not buying a single sensor in isolation,” he says. “They are designing a perception system, and cost only matters if the system as a whole is viable.”</p><p>Those system-level tradeoffs help explain where low-cost lidar is most likely to appear first.</p><p>Most advanced driver assistance systems today rely on cameras and radar, which are significantly cheaper than lidar. Cameras provide dense visual information, while radar offers reliable range and velocity data, particularly in poor weather. Radha estimates that lidar remains roughly an order of magnitude more expensive than automotive radar.</p><p>But at prices in the $100 to $200 range, that gap narrows enough to change design decisions.</p><p>“At that point, lidar becomes appealing because of its superior capability in precise 3D detection and tracking,” Radha says.</p><p>Rather than replacing existing sensors, lower-cost lidar would likely augment them, adding redundancy and improving performance in complex environments that are challenging for electronic perception systems. That incremental improvement aligns more closely with how ADAS features are deployed today than with the leap to full vehicle autonomy.</p><p>MicroVision is not alone in pursuing solid-state lidar, and several suppliers including Chinese firms Hesai and RoboSense and other major suppliers such as Luminar and Velodyne have announced long-term cost targets below $500. What distinguishes current claims is the explicit focus on sub-$200 pricing tied to production volume rather than future prototypes or limited pilot runs.</p><p>Some competitors continue to prioritize long-range performance for autonomous vehicles, which pushes cost upward. Others have avoided aggressive pricing claims until they secure firm production commitments from automakers.</p><p>That caution reflects a structural challenge: Reaching consumer-level pricing requires large, predictable demand. Without it, few suppliers can justify the manufacturing investments needed to achieve true economies of scale.</p><h2>Evaluating Lidar Performance Metrics</h2><h2></h2><p>Even if low-cost lidar becomes manufacturable, another question remains: How should its performance be judged?</p><p>From a systems-engineering perspective, Radha says cost milestones often overshadow safety metrics.</p><p>“The key objective of ADAS and autonomous systems is improving safety,” he says. Yet there is no universally adopted metric that directly expresses safety gains from a given sensor configuration.</p><p>Researchers instead rely on perception benchmarks such as <a href=\"https://www.v7labs.com/blog/mean-average-precision\" rel=\"noopener noreferrer\" target=\"_blank\">mean Average Precision</a>, or mAP, which measures how accurately a system detects and tracks objects in its environment. Including such metrics alongside cost targets, says Radha, would clarify what performance is preserved or sacrificed as prices fall.</p><p><em>IEEE Spectrum</em> has covered lidar extensively, often focusing on technical advances in scanning, range, and resolution. What distinguishes the current moment is the renewed focus on economics rather than raw capability</p><p>If solid-state lidar can reliably reach sub-$200 pricing, it will not invalidate Elon Musk’s skepticism—but it will weaken one of its strongest foundations. When cost stops being the dominant objection, automakers will have to decide whether leaving lidar out is a technical judgment or a strategic one.</p><p>That decision, more than any single price claim, may determine whether lidar finally becomes a routine component of vehicle safety systems.</p>",
      "author": "Willie D. Jones",
      "publishedAt": "2026-02-14T14:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "c233b3e8-d658-490d-81e6-ce96263d08e2",
      "guid": "https://spectrum.ieee.org/ieee-tryengineering-20-years",
      "title": "TryEngineering Marks 20 Years of Getting Kids Interested in STEM",
      "link": "https://spectrum.ieee.org/ieee-tryengineering-20-years",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/three-tween-boys-tweaking-a-circuit-board-together.jpg?id=64075340&width=1245&height=700&coordinates=0%2C156%2C0%2C157\"/><br/><br/><p><a href=\"https://tryengineering.org/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE TryEngineering</a> is celebrating 20 years of empowering educators with resources that introduce engineering to students at an early age. Launched in 2006 as a collaboration between <a href=\"https://www.ieee.org/\" target=\"_blank\">IEEE</a>, <a href=\"https://www.ibm.com/us-en\" rel=\"noopener noreferrer\" target=\"_blank\">IBM</a>, and the <a href=\"https://nysci.org/\" rel=\"noopener noreferrer\" target=\"_blank\">New York Hall of Science</a> (NYSCI), TryEngineering began with a clear goal: Make engineering accessible, understandable, and engaging for students and the teachers who support them.</p><p>What started as an idea within <a href=\"https://ea.ieee.org/ea-programs\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Educational Activities</a> has grown into a global platform supporting preuniversity engineering education around the world.</p><h2>Concerns about the future</h2><p>In the early 2000s, <a href=\"https://www.nae.edu/19579/19582/21020/16145/16161/The-Status-and-Nature-of-K12-Engineering-Education-in-the-United-States\" rel=\"noopener noreferrer\" target=\"_blank\">engineering was largely absent from preuniversity education</a>, typically being taught only in small, isolated programs. Most students had little exposure to the many types of engineering, and they did not learn what engineers actually do.</p><p>At the same time, industry and academic leaders were increasingly concerned about the future of engineering as a whole. They worried about the talent pipeline and saw existing outreach efforts as scattered and inconsistent.</p><p>In 2004 representatives from several electrical and computer engineering industries met with IEEE leadership and expressed their concerns about the declining number of students interested in engineering careers. They urged IEEE to organize a more effective, coordinated response to unite professional societies, educators, and industry around a shared approach to preuniversity outreach and education.</p><p>One of the major recommendations to come out of that meeting was to start teaching youngsters about engineering earlier. <a href=\"https://www.nationalacademies.org/publications/10573\" rel=\"noopener noreferrer\" target=\"_blank\">Research</a> from the U.S. National Academy of Engineering at the time showed that students begin forming attitudes toward science, technology, engineering, and math fields from ages 5 to 10, and that outreach should begin as early as kindergarten. Waiting until the teen years or university-level education is simply too late, they determined; it needs to happen during the formative years to spark long-term interest in STEM learning.</p><h2>The idea behind the website</h2><p>TryEngineering emerged from the broader <a href=\"https://ieeexplore.ieee.org/document/4760339\" rel=\"noopener noreferrer\" target=\"_blank\">Launching Our Children’s Path to Engineering</a> initiative, which was approved in 2005 by the <a href=\"https://www.ieee.org/about/corporate/board\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Board of Directors</a>. A core element of the IEEE program was a public-facing website that would introduce young learners to engineering projects, roles, and careers. The concept eventually developed into TryEngineering.org.</p><p>The idea for TryEngineering.org itself grew from an existing, successful model. The NYSCI operated <a href=\"https://tryscience.org\" rel=\"noopener noreferrer\" target=\"_blank\">TryScience.org</a>, a popular public website supported by <a href=\"https://www.ibm.com/us-en\" rel=\"noopener noreferrer\" target=\"_blank\">IBM</a> that helped students explore science topics through hands-on activities and real‑world connections.</p><p>At the time, the IEEE Educational Activities group was working with the NYSCI on TryScience projects. Building a parallel site focused on engineering was a natural next step, and IBM’s experience in supporting large‑scale educational outreach made it a strong partner.</p><p>A central figure in turning that vision into reality was <a href=\"https://www.moshekam.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Moshe Kam</a>, who served as the 2005–2007 IEEE Educational Activities vice president, and later as the 2011 IEEE president. During his tenure, Kam spearheaded the creation of TryEngineering.org and guided the international expansion of IEEE’s Teacher In‑Service Program, which trained volunteers to work directly with teachers to create hands-on engineering lessons (the program no longer exists). His leadership helped establish preuniversity education as a core, long‑term priority within IEEE.</p><p>“The founders of the IEEE TryEngineering program created something very special. In a world where the messaging about becoming an engineer often scares students who have not yet developed math skills away from our profession, and preuniversity teachers without engineering degrees have trepidation in teaching topics in our fields of interest, people like Dr. Kam and the other founders had a vision where everyone could literally <em><em>try</em></em> engineering,” says <a href=\"https://www.linkedin.com/in/jamie-moesch-7502b31/\" rel=\"noopener noreferrer\" target=\"_blank\">Jamie Moesch</a>, <a href=\"https://spectrum.ieee.org/tag/ieee-educational-activities\" target=\"_self\">IEEE Educational Activities</a> managing director.</p><p> “Because of this, teachers have now taught millions of our hands-on lessons and opened our profession to so many more young minds,” he adds. “All of the preuniversity programs we have continued to build and improve upon are fueled by this massively important and simple-to-understand concept of try engineering.”</p><h2>A focus on educators</h2><p>From the beginning, TryEngineering focused on educators as the keys to its success, rather than starting with students. Instead of complex technical explanations, the platform offered free, classroom-ready lesson plans with clear explanations about engineering fields and examples with which students could relate. Hands-on activities emphasized problem‑solving, creativity, and teamwork—core elements of how engineers actually work.</p><p>IEEE leaders also recognized that misconceptions about engineering discouraged many talented young people—particularly girls and students from underrepresented groups—from pursuing engineering as a career. TryEngineering aimed to show engineering as practical, creative, and connected to real-world needs, helping students see that engineering could be for anyone, not just a narrow group of specialists.</p><p>By simply encouraging students and educators to just <em><em>try</em></em> engineering, doors are open to new possibilities and a broader understanding of the field. Even students who ultimately choose other career paths get to learn key concepts, such as the <a href=\"https://tryengineering.org/news/engineering-design-process/\" rel=\"noopener noreferrer\" target=\"_blank\">engineering design process</a>, equipping them with practical skills for the rest of their life.</p><h2>Outreach programs and summer camps</h2><p>During the past two decades, TryEngineering has grown well beyond its original website. In addition to providing a <a href=\"https://tryengineering.org/explore-resources/lesson-plans/\" rel=\"noopener noreferrer\" target=\"_blank\">vast library of lesson plans and resources</a> that engage and inspire, it also serves as the hub for a collection of programs reaching educators and students in many ways.</p><p>Those include the <a href=\"https://tryengineering.org/get-involved/ieee-members/stem-champions/\" rel=\"noopener noreferrer\" target=\"_blank\">TryEngineering STEM Champions</a> program, which empowers dedicated volunteers to support outreach programs and serve as vital connectors to IEEE’s extensive resources. The <a href=\"https://spectrum.ieee.org/ieee-tryengineering-summer-camp\" target=\"_self\">TryEngineering Summer Institute</a> offers immersive <a href=\"https://spectrum.ieee.org/ieee-tryengineering-summer-camp\" target=\"_self\">campus‑based experiences</a> for students ages 13 to 17, with expanded locations and programs being introduced this year.</p><p>The <a href=\"https://spectrum.ieee.org/ieee-stem-summit-2025\" target=\"_self\">IEEE STEM Summit</a> is an annual virtual event that brings together <a href=\"https://spectrum.ieee.org/ieee-stem-summit-2025\" target=\"_self\">educators and volunteers from around the world</a>. <a href=\"https://oncampus.tryengineering.org/\" rel=\"noopener noreferrer\" target=\"_blank\">TryEngineering OnCampus</a> partners with universities around the globe to organize hands-on programs. <a href=\"https://tryengineering.org/news/50-teachers-in-5-states-learn-about-semiconductors/\" rel=\"noopener noreferrer\" target=\"_blank\">TryEngineering educator sessions</a> provide free professional development programs aligned with emerging industry needs such as semiconductors.</p><h2>20 ways to celebrate 20 years</h2><p>To mark its 20th anniversary, TryEngineering is celebrating with a year of special activities, new partnerships, and fresh resources for educators. Visit the TryEngineering <a href=\"https://tryengineering.org/explore-resources/collections/tryengineering-20th-anniversary/\" rel=\"noopener noreferrer\" target=\"_blank\">20th Anniversary collection page</a> to explore what’s ahead, join the celebration, and discover 20 ways to celebrate 20 years of inspiring the next generation of technology innovators. This is an opportunity to reflect on how far the program has come, and to help shape how the next generation discovers engineering.</p><p><span>“The passion and dedication of the thousands of volunteers of IEEE who do local outreach enables the IEEE-wide goal to inspire intellectual curiosity and invention to engage the next generation of technology innovators,” Moesch says. “The first 20 years have been special, and I cannot wait to have the world experience what the future holds for the TryEngineering programs.”</span></p>",
      "author": "Robert Schneider",
      "publishedAt": "2026-02-13T19:00:04.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "7a59f902-3cfb-4873-999c-1161de7ee61f",
      "guid": "https://spectrum.ieee.org/video-friday-robot-collective",
      "title": "Video Friday: Robot Collective Stays Alive Even When Parts Die",
      "link": "https://spectrum.ieee.org/video-friday-robot-collective",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/robot-collective-crawls-under-a-bridge-of-rocks-with-glowing-lights-video-speed-increased-10x.gif?id=64423332&width=1245&height=700&coordinates=0%2C45%2C0%2C45\"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href=\"mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a><span> for inclusion.</span></p><h5><a href=\"https://2026.ieee-icra.org/\">ICRA 2026</a>: 1–5 June 2026, VIENNA</h5><p>Enjoy today’s videos!</p><div class=\"horizontal-rule\"></div><div style=\"page-break-after: always\"><span style=\"display:none\"> </span></div><blockquote class=\"rm-anchors\" id=\"boebmm8mlea\"><em>No system is immune to failure. The compromise between reducing failures and improving adaptability is a recurring problem in robotics. Modular robots exemplify this tradeoff, because the number of modules dictates both the possible functions and the odds of failure. We reverse this trend, improving reliability with an increased number of modules by exploiting redundant resources and sharing them locally.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"d4b443d6937b9d0cc92c03a7e8d6617b\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/bOeBmm8mleA?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.science.org/doi/10.1126/scirobotics.ady6304\">Science</a> ] via [ <a href=\"https://www.epfl.ch/labs/rrl/\">RRL</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"unorxwlzlfk\"><em>Now that the <a href=\"https://robotsguide.com/robots/atlas\" target=\"_blank\">Atlas</a> enterprise platform is getting to work, the research version gets one last run in the sun. Our engineers made one final push to test the limits of full-body control and mobility, with help from the RAI Institute.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"dabc1539dcc8e868a47dee3a32fd7b46\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/UNorxwlZlFk?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://rai-inst.com/\">RAI</a> ] via [ <a href=\"https://bostondynamics.com/\">Boston Dynamics</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"khimsr8guce\"><em>Announcing Isaac 0: the laundry folding robot we’re shipping to homes, starting in February 2026 in the Bay Area.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"3e2ca93296a7724dd71ad07d146372c8\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/KhImSR8GuCE?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.weaverobotics.com/\">Weave Robotics</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"md7auy7lh34\"><em>In a paper published in Science, researchers at the Max Planck Institute for Intelligent Systems, the Humboldt University of Berlin, and the University of Stuttgart have discovered that the secret to the elephant’s amazing sense of touch is in its unusual whiskers. The interdisciplinary team analyzed elephant trunk whiskers using advanced microscopy methods that revealed a form of material intelligence more sophisticated than the well-studied whiskers of rats and mice. This research has the potential to inspire new physically intelligent robotic sensing approaches that resemble the unusual whiskers that cover the elephant trunk.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"1dfc845660633f2fd1566c3989dd428d\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/MD7Auy7lH34?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.mpg.de/26113474/elephant-trunk-whiskers-exhibit-material-intelligence?c=2249\">MPI</a> ]</p><div class=\"horizontal-rule\"></div><p class=\"rm-anchors\" id=\"rcpuqmvs37q\">Got an interest in autonomous mobile robots, <a href=\"https://spectrum.ieee.org/tag/robot-operating-system\" target=\"_blank\">ROS2</a>, and a mere $150 lying around? Try this.</p><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"648c0f35b6cbb775c227c778b61e3aea\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/RCPUQmvS37Q?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://makerspet.com/store#!/Arduino-ROS2-Self-Driving-Robot-120mm-Build-Pack/p/725772983\">Maker's Pet</a> ]</p><p>Thanks, Ilia!</p><div class=\"horizontal-rule\"></div><p class=\"rm-anchors\" id=\"9ti9mi8rbiq\">We’re giving <a href=\"https://spectrum.ieee.org/topic/robotics/humanoid-robots/\" target=\"_blank\">humanoid robots</a> swords now.</p><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"4bb44dd7b81db8ae1c0a107f312c9998\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/9Ti9Mi8rbIQ?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.robotera.com/en/\">Robotera</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"ew3az19rlqa\"><em>A system developed by researchers at the University of Waterloo lets people collaborate with groups of robots to create works of art inspired by music.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"16928fc1dec0c295881676004533743c\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/ew3az19rlqA?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://uwaterloo.ca/news/media/translating-music-light-and-motion-robots\">Waterloo</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"exp3fsnqxqw\"><em>FastUMI Pro is a multimodal, model-agnostic data acquisition system designed to power a truly end-to-end closed loop for embodied intelligence — transforming real-world data into genuine robotic capability.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"4c6a332d7e9b76af05f00dc9990e971e\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/EXP3fsnQXqw?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.lumosbot.tech/\">Lumos Robotics</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"4fbygialjyu\"><em>We usually take fingernails for granted, but they’re vital for fine-motor control and feeling textures. Our students have been doing some great work looking into the mechanics behind this.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"b12a33801a528b063c865db11a550308\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/4FByGIALjyU?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://arxiv.org/html/2602.05156v1\">Paper</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"uxuvmz3nwto\"><em>This is a 550-lb all-electric coaxial unmanned rotorcraft developed by Texas A&M University’s Advanced Vertical Flight Laboratory and Harmony Aeronautics as a technology demonstrator for our quiet-rotor technology. The payload capacity is 200 lb (gross weight = 750 lb). The noise level measured was around 74 dBA in hover at 50-ft making this probably the quietest rotorcraft at this scale.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"5112475428ad208d8d71a9ba961c0fbb\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/uxuvMz3nwto?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://harmonyaeronautics.com/\">Harmony Aeronautics</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"bk9k_mjjlxe\"><em>Harvard scientists have created an advanced 3D printing method for developing soft robotics. This technique, called rotational multimaterial 3D printing, enables the fabrication of complex shapes and tubular structures with dissolvable internal channels. This innovation could someday accelerate the production of components for surgical robotics and assistive devices, advancing medical technology.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"d8440b53651f8443a6733570f8f342a6\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/BK9K_mJjlxE?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://seas.harvard.edu/news/3d-printing-soft-robots\">Harvard</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"a48dohk0u7i\"><em>Lynx M20 wheeled-legged robot steps onto the ice and snow, taking on challenges inspired by four winter sports scenarios. Who says robots can’t enjoy winter sports?</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"cb096d509d326a8b7fc63c514373044b\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/a48DoHK0U7I?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.deeprobotics.cn/en\">Deep Robotics</a> ]</p><div class=\"horizontal-rule\"></div><p class=\"rm-anchors\" id=\"ec712qh3t6g\">NGL right now I find this more satisfying to watch than a humanoid doing just about anything.</p><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"29de1f6d610f3a555db97ef444bf7f06\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/Ec712qh3T6g?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.linkedin.com/posts/fanuc-america-corporation_robotic-case-packing-and-palletizing-system-activity-7426656807932203009-y4hR/\">Fanuc</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"xqfk1bd5bkg\"><em>At Mentee Robotics, we design and build humanoid robots from the ground up with one goal: reliable, scalable deployment in real-world industrial environments. Our robots are powered by deep vertical integration across hardware, embedded software, and AI, all developed in-house to close the Sim2Real gap and enable continuous, around-the-clock operation.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"65329eda591818e176b81a494fe5b599\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/XqFk1Bd5BKg?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.menteebot.com/\">Mentee Robotics</a> ]</p><div class=\"horizontal-rule\"></div><p class=\"rm-anchors\" id=\"3y0rawjlaxs\">You don’t need to watch this whole video, but the idea of little submarines that hitch rides on bigger boats and recharge themselves is kind of cool.</p><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"a15f69e364382ed4d168f7fdbc178597\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/3y0RAwJlAxs?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.lockheedmartin.com/en-us/products/mmauv.html\">Lockheed Martin</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"u6zp38xurgs\"><em>Learn about the work of Dr. Roland Siegwart, Dr. Anibal Ollero, Dr. Dario Floreano, and Dr. Margarita Chli on flying robots and some of the challenges they are still trying to tackle in this video created based on their presentations at ICRA@40 the 40th anniversary celebration of the IEEE International Conference on Robotics and Automation.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"facbfd3b29494abfbcc4306c7c81ecca\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/U6ZP38XUrGs?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://icra40.ieee.org/\">ICRA@40</a> ]</p><div class=\"horizontal-rule\"></div>",
      "author": "Evan Ackerman",
      "publishedAt": "2026-02-13T16:30:03.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "cfa35c55-1e37-4b7a-b227-9b99369ed228",
      "guid": "https://spectrum.ieee.org/nanoled-research-approaches",
      "title": "LEDs Enter the Nanoscale",
      "link": "https://spectrum.ieee.org/nanoled-research-approaches",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/close-up-of-an-illuminated-plus-sign-made-out-of-nano-scale-led-lights.jpg?id=64100009&width=1245&height=700&coordinates=0%2C187%2C0%2C188\"/><br/><br/><p><a href=\"https://spectrum.ieee.org/virtual-reality-head-set-8k\" target=\"_self\">MicroLEDs</a>, with pixels just micrometers across, have long been a byword in the display world. Now, microLED-makers have begun shrinking their creations into the uncharted nano realm. In January, a startup named Polar Light Technologies unveiled <a href=\"https://www.semiconductor-today.com/news_items/2026/jan/polarlight2-210126.shtml\" rel=\"noopener noreferrer\" target=\"_blank\">prototype blue LEDs</a> less than 500 nanometers across. This raises a tempting question: How far can LEDs shrink?</p><p>We know the answer is, at least, considerably smaller. In the past year, two different research groups have demonstrated LED pixels at sizes of 100 nm or less.</p><p>These are some of the smallest LEDs ever created. They leave much to be desired in their efficiency—but one day, nanoLEDs could power ultra-high-resolution virtual reality displays and high-bandwidth on-chip photonics. And the key to making even tinier LEDs, if these early attempts are any precedents, may be to make more unusual LEDs.</p><h2>New Approaches to LED</h2><p>Take Polar Light’s example. Like many LEDs, the Sweden-based startup’s diodes are fashioned from III-V semiconductors like gallium nitride (GaN) and indium gallium nitride (InGaN). Unlike many LEDs, which are etched into their semiconductor from the top down, Polar Light’s are instead fabricated by building peculiarly shaped <a href=\"https://www.polar-light-technologies.com/technology-2/\" rel=\"noopener noreferrer\" target=\"_blank\">hexagonal pyramids</a> from the bottom up. </p><p>Polar Light designed its pyramids for the larger microLED market, and plans to start commercial production in late 2026. But they also wanted to test how small their pyramids could shrink. So far, they’ve made pyramids 300 nm across. “We haven’t reached the limit, yet,” says<a href=\"https://www.polar-light-technologies.com/about-us/\" rel=\"noopener noreferrer\" target=\"_blank\"> Oskar Fajerson</a>, Polar Light’s CEO. “Do we know the limit? No, we don’t, but we can [make] them smaller.”</p><p>Elsewhere, researchers have already done that. Some of the world’s tiniest LEDs come from groups who have foregone the standard III-V semiconductors in favor of other types of LEDs—like <a href=\"https://spectrum.ieee.org/stretchable-oleds-wearable-display-drexel\" target=\"_self\">OLEDs</a>. </p><p>“We are thinking of a different pathway for organic semiconductors,” says<a href=\"https://shihlab.ethz.ch/\" rel=\"noopener noreferrer\" target=\"_blank\"> Chih-Jen Shih</a>, a chemical engineer at ETH Zurich in Switzerland. Shih and his colleagues were interested in finding a way to fabricate small OLEDs at scale. Using an <a href=\"https://spectrum.ieee.org/lithographic-feature-sizes-reduced-down-to-one-nanometer\" target=\"_self\">electron-beam lithography</a>-based technique, they crafted arrays of green OLEDs with pixels as small as 100 nm across.</p><p>Where today’s best displays have <a href=\"https://spectrum.ieee.org/virtual-reality-head-set-8k\" target=\"_self\">14,000 pixels per inch</a>, these nanoLEDs—presented in an <a href=\"https://www.nature.com/articles/s41566-025-01785-z\" rel=\"noopener noreferrer\" target=\"_blank\">October 2025 <em><em>Nature Photonics </em></em>paper</a>—can reach 100,000 pixels per inch.</p><p>Another group tried their hands with <a href=\"https://spectrum.ieee.org/led-display-perovskite-charger\" target=\"_self\">perovskites</a>, cage-shaped materials best-known for their prowess in <a href=\"https://spectrum.ieee.org/perovskite-2667580324\" target=\"_self\">high-efficiency solar panels</a>. Perovskites have recently gained traction in LEDs too. “We wanted to see what would happen if we make perovskite LEDs smaller, all the way down to the micrometer and nanometer length-scale,” says<a href=\"https://person.zju.edu.cn/en/daweidi\" rel=\"noopener noreferrer\" target=\"_blank\"> Dawei Di</a>, engineer at Zhejiang University in Hangzhou, China. </p><p>Di’s group started with comparatively colossal perovskite LED pixels, measuring hundreds of micrometers. Then, they fabricated sequences of smaller and smaller pixels, each tinier than the last. Even after the 1 μm mark, they did not stop: 890 nm, then 440 nm, only bottoming out at 90 nm. These 90 nm red and green pixels, presented in a <a href=\"https://www.nature.com/articles/s41586-025-08685-w#Abs1\" rel=\"noopener noreferrer\" target=\"_blank\">March 2025 <em><em>Nature </em></em>paper</a>, likely represent the smallest LEDs reported to date.</p><h2>Efficiency Challenges</h2><p>Unfortunately, small size comes at a cost: Shrinking LEDs also shrinks their efficiency. Di’s group’s perovskite nanoLEDs have external quantum efficiencies—a measure of how many injected electrons are converted into photons—around 5 to 10 percent; Shih’s group’s nano-OLED arrays performed slightly better, topping 13 percent. For comparison, a typical millimeter-sized III-V LED can reach <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC5706270/\" rel=\"noopener noreferrer\" target=\"_blank\">50 to 70 percent</a>, depending on its color.</p><p>Shih, however, is optimistic that modifying how nano-OLEDs are made can boost their efficiency. “In principle, you can achieve 30 percent, 40 percent external quantum efficiency with OLEDs, even with a smaller pixel, but it takes time to optimize the process,” Shih says.<br/><br/>Di thinks that researchers could take perovskite nanoLEDs to less dire efficiencies by tinkering with the material. Although his group is now focusing on the larger perovskite microLEDs, Di expects researchers will eventually reckon with nanoLEDs’ efficiency gap. If applications of smaller LEDs become appealing, “this issue could become increasingly important,” Di says. </p><h2>What Can NanoLEDs Be Used For?</h2><p><span>What can you actually do with LEDs this small? Today, the push for tinier pixels largely comes from devices like smart glasses and virtual reality headsets. Makers of these displays are hungry for smaller and smaller pixels in a chase for bleeding-edge picture quality with low power consumption (one reason that efficiency is important). Polar Light’s Fajerson says that smart-glass manufacturers today are already seeking 3 μm pixels.</span></p><p><span></span><span>But researchers are skeptical that VR displays will ever need pixels smaller than around 1 μm. Shrink pixels too far beyond that, and they’ll cross their light’s</span><a href=\"https://svi.nl/DiffractionLimit\" target=\"_blank\"> diffraction limit</a><span>—that means they’ll become too small for the human eye to resolve. Shih’s and Di’s groups have already crossed the limit with their 100-nm and 90-nm pixels.</span></p><p><span></span><span>Very tiny LEDs may instead find use in on-chip photonics systems, allowing the likes of AI data centers to communicate with greater bandwidths than they can today. Chip manufacturing giant TSMC is </span><a href=\"https://spectrum.ieee.org/tsmc-microled-optical-interconnects\" target=\"_self\">already trying out microLED interconnects</a><span>, and it’s easy to imagine chipmakers turning to even smaller LEDs in the future.</span></p><p>But the tiniest nanoLEDs may have even more exotic applications, because they’re smaller than the wavelengths of their light. “From a process point of view, you are making a new component that was not possible in the past,” Shih says.</p><p>For example, Shih’s group showed their nano-OLEDs could form a <a href=\"https://spectrum.ieee.org/lifi-lidar-metasurface-applications\" target=\"_self\">metasurface</a>—a structure that uses its pixels’ nano-sizes to control how each pixel interacts with its neighbors. One day, similar devices could focus nanoLED light into laser-like beams or create holographic 3D nanoLED displays.</p>",
      "author": "Rahul Rao",
      "publishedAt": "2026-02-12T15:00:03.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "f9574d9e-8453-45f7-9ba6-74a144c1cc41",
      "guid": "https://spectrum.ieee.org/fda-medical-device-rules",
      "title": "What the FDA’s 2026 Update Means for Wearables",
      "link": "https://spectrum.ieee.org/fda-medical-device-rules",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/illustration-of-a-smart-watch-with-an-eye-ball-displayed-on-the-screen.jpg?id=64099717&width=1245&height=700&coordinates=0%2C49%2C0%2C50\"/><br/><br/><p>As new consumer hardware and software capabilities have bumped up against medicine over the last few years, consumers and manufacturers alike have struggled with identifying the line between “wellness” products such as earbuds that can also amplify and clarify surrounding speakers’ voices and regulated medical devices such as conventional hearing aids. On January 6, 2026, the U.S. Food and Drug Administration issued new guidance documents clarifying how it interprets existing law for the review of wearable and AI-assisted devices. </p><p>The first document, for <a href=\"https://www.fda.gov/regulatory-information/search-fda-guidance-documents/general-wellness-policy-low-risk-devices\" rel=\"noopener noreferrer\" target=\"_blank\">general wellness</a>, specifies that the FDA will interpret noninvasive sensors such as sleep trackers or heart rate monitors as low-risk wellness devices while treating invasive devices under conventional regulations. The other document defines how the FDA will exempt <a href=\"https://www.fda.gov/regulatory-information/search-fda-guidance-documents/clinical-decision-support-software\" rel=\"noopener noreferrer\" target=\"_blank\">clinical decision support tools</a> from medical device regulations, limiting such software to analyzing existing data rather than extracting data from sensors, and requiring them to enable independent review of their recommendations. The documents do not rewrite any statutes, but they refine interpretation of existing law, compared to the 2019 and 2022 documents they replace. They offer a fresh lens on how regulators see technology that sits at the intersection of consumer electronics, software, and medicine—a category many other countries are choosing to regulate more strictly rather than less.</p><h2>What the 2026 update changed</h2><p>The 2026 FDA update clarifies how it distinguishes between “medical information” and systems that measure physiological “signals” or “patterns.” Earlier guidance discussed these concepts more generally, but the new version defines signal-measuring systems as those that collect continuous, near-continuous, or streaming data from the body for medical purposes, such as home devices transmitting blood pressure, <a href=\"https://spectrum.ieee.org/should-you-trust-apples-new-blood-oxygen-sensor\" target=\"_blank\">oxygen saturation</a>, or <a href=\"https://spectrum.ieee.org/smartphone-camera-senses-patients-pulse-breathing-rate\" target=\"_blank\">heart rate</a> to clinicians. It gives more concrete examples, like a blood glucose lab result as medical information versus continuous glucose monitor readings as signals or patterns.</p><p>The updated guidance also sharpens examples of what counts as medical information that software may display, analyze, or print. These include radiology reports or summaries from legally marketed software, ECG reports annotated by clinicians, blood pressure results from cleared devices, and lab results stored in electronic health records. </p><p>In addition, the 2026 update softens FDA’s earlier stance on clinical decision tools that offer only one recommendation. While prior guidance suggested tools needed to present multiple options to avoid regulation, FDA now indicates that a single recommendation may be acceptable if only one option is clinically appropriate, though it does not define how that determination will be made. </p><p>Separately, updates to the general wellness guidance clarify that some non-invasive wearables—such as optical sensors estimating blood glucose for wellness or nutrition awareness—may qualify as general wellness products, while more invasive technologies would not.</p><h2>Wellness still requires accuracy</h2><p>For designers of wearable health devices, the practical implications go well beyond what label you choose. “Calling something ‘wellness’ doesn’t reduce the need for rigorous validation,” says <a href=\"https://ece.gatech.edu/directory/omer-t-inan\" rel=\"noopener noreferrer\" target=\"_blank\">Omer Inan</a>, a medical device technology researcher at the Georgia Tech School of Electrical and Computer Engineering. A wearable that reports blood pressure inaccurately could lead a user to conclude that their values are normal when they are not—potentially influencing decisions about seeking clinical care.</p><p>“In my opinion, engineers designing devices to deliver health and wellness information to consumers should not change their approach based on this new guidance,” says Inan. Certain measurements—such as blood pressure or glucose—carry real medical consequences regardless of how they’re branded, Inan notes.</p><p>Unless engineers follow robust validation protocols for technology delivering health and wellness information, Inan says, consumers and clinicians alike face the risk of faulty information.</p><p>To address that, Inan advocates for transparency: companies should publish their validation results in peer-reviewed journals, and independent third parties without financial ties to the manufacturer should evaluate these systems. That approach, he says, helps the engineering community and the broader public assess the accuracy and reliability of wearable devices.</p><h2>When wellness meets medicine</h2><p>The societal and clinical impacts of wearables are already visible, regardless of regulatory labels, says Sharona Hoffman, JD, a law and bioethics professor at Case Western Reserve University.</p><p>Medical metrics from devices like the Apple Watch or Fitbit may be framed as “wellness,” but in practice many users treat them like medical data, influencing their behavior or decisions about care, Hoffman points out.</p><p>“It could cause anxiety for patients who constantly check their metrics,” she notes. Alternatively, “A person may enter a doctor’s office confident that their wearable has diagnosed their condition, complicating clinical conversations and decision-making.”</p><p>Moreover, privacy issues remain unresolved, unmentioned in previous or updated guidance documents. Many companies that design wellness devices fall outside protections like the Health Insurance Portability and Accountability Act (HIPAA), meaning data about health metrics could be collected, shared, or sold without the same constraints as traditional medical data. “We don’t know what they’re collecting information about or whether marketers will get hold of it,” Hoffman says. </p><h2>International approaches</h2><p>The European Union’s Artificial Intelligence Act designates systems that process health-related data or influence clinical decisions as “high risk,” subjecting them to stringent requirements around data governance, transparency, and human oversight. China and South Korea have also implemented rules that tighten controls on algorithmic systems that intersect with healthcare or public-facing use cases. South Korea provides very specific categories for regulation for technology makers, such as <a href=\"https://www.mfds.go.kr/eng/brd/m_40/list.do\" rel=\"noopener noreferrer\" target=\"_blank\">standards on labeling and description on medical devices and good manufacturing practices</a>. </p><p>Across these regions, regulators are not only classifying technology by its intended use but also by its potential impact on individuals and society at large.</p><p>“Other countries that emphasize technology are still worrying about data privacy and patients,” Hoffman says. “We’re going in the opposite direction.”</p><h2>Post-market oversight </h2><p>“Regardless of whether something is FDA approved, these technologies will need to be monitored in the sites where they’re used,” says Todd R. Johnson, a professor of biomedical informatics at McWilliams School of Biomedical Informatics at UTHealth Houston, who has worked on FDA-regulated products and informatics in clinical settings. “There’s no way the makers can ensure ahead of time that all of the recommendations will be sound.”</p><p>Large health systems may have the capacity to audit and monitor tools, but smaller clinics often do not. Monitoring and auditing are not emphasized in the current guidance, raising questions about how reliability and safety will be maintained once devices and software are deployed widely.</p><h2>Balancing innovation and safety</h2><p>For engineers and developers, the FDA’s 2026 guidance presents both opportunities and responsibilities. By clarifying what counts as a regulated device, the agency may reduce upfront barriers for some categories of technology. But that shift also places greater weight on design rigor, validation transparency, and post-market scrutiny. </p><p>“Device makers do care about safety,” Johnson says. “But regulation can increase barriers to entry while also increasing safety and accuracy. There’s a trade-off.”</p>",
      "author": "Catherine Arnold",
      "publishedAt": "2026-02-12T14:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "3ab7758d-f21a-4d70-a44a-deca6e41f016",
      "guid": "https://spectrum.ieee.org/legacy-chemist-jan-czochralski",
      "title": "Rediscovering the Lost Legacy of Chemist Jan Czochralski",
      "link": "https://spectrum.ieee.org/legacy-chemist-jan-czochralski",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/painting-of-an-elderly-man-writing-at-a-small-desk-inside-of-a-jail-cell.jpg?id=64092933&width=1245&height=700&coordinates=0%2C187%2C0%2C188\"/><br/><br/><p>During times of political turmoil, history often gets rewritten, erased, or lost. That is what happened to the legacy of <a href=\"https://en.wikipedia.org/wiki/Jan_Czochralski\" rel=\"noopener noreferrer\" target=\"_blank\">Jan Czochralski</a>, a Polish chemist whose contributions to <a href=\"https://spectrum.ieee.org/topic/semiconductors/\" target=\"_self\">semiconductor</a> manufacturing were expunged after World War II.</p><p>In 1916 he invented a method for growing single crystals of semiconductors, metals, and synthetic gemstones. The process, now known as the <a href=\"https://ethw.org/Milestones:Czochralski_Process,_1916\" rel=\"noopener noreferrer\" target=\"_blank\">Czochralski method</a>, allows scientists to have more control over a semiconductor’s quality.</p><p>After the war ended, Czochralski was <a href=\"https://www.iucr.org/news/newsletter/volume-28/number-3/who-was-jan-czochralski\" rel=\"noopener noreferrer\" target=\"_blank\">falsely accused by the Polish government</a> of collaborating with the Germans and betraying his country, according to an article published by the <a href=\"https://www.iucr.org/\" rel=\"noopener noreferrer\" target=\"_blank\">International Union of Crystallography</a>. The allegation apparently ended his academic career as a professor at the <a href=\"https://eng.pw.edu.pl/\" rel=\"noopener noreferrer\" target=\"_blank\">Warsaw University of Technology</a> and led to the erasure of his name and work from the school’s records.</p><p>He died in 1953 in obscurity in his hometown of Kcynia.</p><p>The Czochralski method was <a href=\"https://spectrum.ieee.org/modern-civilization-relies-on-this-crystalgrowing-method\" target=\"_self\">honored in 2019</a> with an <a href=\"https://ieeemilestones.ethw.org/Main_Page\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Milestone</a> for<a href=\"https://spectrum.ieee.org/modern-civilization-relies-on-this-crystalgrowing-method\" target=\"_self\"> enabling the </a>development of semiconductor devices and modern electronics. Administered by the <a href=\"https://www.ieee.org/about/history-center\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE History Center</a> and <a href=\"https://www.ieeefoundation.org/donate_history\" rel=\"noopener noreferrer\" target=\"_blank\">supported by donors</a>, the Milestone program recognizes outstanding technical developments around the world.</p><p>Inspired by the IEEE recognition, Czochralski’s grandson <a href=\"https://www.sgmk.edu.pl/fred-schmidt-grandson-of-jan-czochralski-on-innovation-courage-and-entrepreneurship-in-the-world-of-technology/\" rel=\"noopener noreferrer\" target=\"_blank\">Fred Schmidt</a> and his great-grandnephew Sylwester Czochralski launched the <a href=\"https://www.jancz.org/\" rel=\"noopener noreferrer\" target=\"_blank\">JanCZ project</a>. The initiative, which aims to educate the public about Czochralski’s life and scientific impact, maintains two websites—one in English and the other in Polish.</p><p>“Discovering the [IEEE Milestone] plaque changed my entire mission,” Schmidt says. “It inspired me to engage with Poland, my family history, and my grandfather’s story [on] a more personal level. The [Milestone] is an important award of validation and recognition. It’s a big part of what I’m building my entire case and my story around as I promote the Jan Czochralski legacy and history to the Western world.”</p><p>Schmidt, who lives in Texas, is seeking to produce a biopic, translate a Polish biography to English, and turn the chemist’s former homes in Kcynia and Warsaw into museums. The Jan Czochralski Remembrance Foundation has been established by Schmidt to help fund the projects.</p><h2>The life of the Polish chemist</h2><p>Before Czochralski’s birth in 1885, Kcynia became part of the German Empire in 1871. Although his family identified as Polish and spoke the language at home, they couldn’t publicly acknowledge their culture, Schmidt says.</p><p>When it came time for Czochralski to go to university, rather than attend one in Warsaw, he did what many Germans did at the time: He attended one in Berlin.</p><p>After graduating with a bachelor’s degree in metal chemistry in 1907 from the Königlich Technische Hochschule in Charlottenburg (now <a href=\"https://www.tu.berlin/en/\" rel=\"noopener noreferrer\" target=\"_blank\">Technische Universität Berlin</a>), he joined <a href=\"https://www.aeg-att.com/Home/About\" rel=\"noopener noreferrer\" target=\"_blank\">Allgemeine Elektricitäts-Gesellschaft</a> in Berlin as an engineer.</p><p>Czochralski experimented with materials to find new formulations that could improve the electrical cables and machinery during the early electrical age, according to a <a href=\"https://edconway.substack.com/p/jan-czochralski-the-forgotten-hero\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Material World </em></em>article</a>.</p><p>While investigating the crystallization rates of metal, Czochralski accidentally dipped his pen into a pot of molten tin instead of an inkwell. A tin filament formed on the pen’s tip—which he found interesting. Through research, he proved that the filament was a single crystal. His discovery prompted him to experiment with the bulk production of semiconductor crystals.</p><p>His paper on what he called the Czochralski method was published in 1918 in the German chemistry journal <a href=\"https://en.wikipedia.org/wiki/Zeitschrift_f%C3%BCr_Physikalische_Chemie\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Zeitschrift für Physikalische Chemie</em></em></a>, but he never found an application for it. (The method wasn’t used until 1948, when <a href=\"https://www.belllabs.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Bell Labs</a> engineers <a href=\"https://en.wikipedia.org/wiki/Gordon_K._Teal\" rel=\"noopener noreferrer\" target=\"_blank\">Gordon Kidd Teal</a> and J.B. Little adapted it to grow single germanium crystals for their semiconductor production, according to <a href=\"https://edconway.substack.com/p/jan-czochralski-the-forgotten-hero\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Material World</em></em></a>.)</p><p>Czochralski continued working in metal science, founding and directing a research laboratory in 1917 at <a href=\"https://en.wikipedia.org/wiki/Metallgesellschaft\" rel=\"noopener noreferrer\" target=\"_blank\">Metallgesellschaft</a> in Frankfurt. In 1919 he was one of the founding members of the <a href=\"https://dgm.de/en/home\" rel=\"noopener noreferrer\" target=\"_blank\">German Society for Metals Science</a>, in Sankt Augustin. He served as its president until 1925.</p><p>Around that time he developed an innovation that led to his wealth and fame, Schmidt says. Called “B-metal,” the metal alloy was a less expensive alternative to the tin used in manufacturing railroad carriage bearings. Czochralski’s alloy was patented by the German railway <a href=\"https://www.deutschebahn.com/en\" rel=\"noopener noreferrer\" target=\"_blank\">Deutsche Bahn</a> and played a significant role in advancing rail transport in Germany, Poland, the Soviet Union, the United Kingdom, and the United States, according to <em><em>Material World</em></em>.</p><p class=\"pull-quote\">“Launching this initiative has been fulfilling and personally rewarding work. My grandfather died in obscurity without ever seeing the results of his work, and my mother spent her entire adult life trying to right these wrongs.”</p><p>The achievement brought Czochralski many opportunities. In 1925 he became president of the <a href=\"https://gdmb.de/home/\" target=\"_blank\">GDMB Society of Metallurgists and Miners</a>, in Clausthal-Zellerfeld, Germany. <a href=\"https://en.wikipedia.org/wiki/Henry_Ford\" rel=\"noopener noreferrer\" target=\"_blank\">Henry Ford</a> invited Czochralski to visit his factories and offered him the position of director at Ford’s new aluminum factory in Detroit. Czochralski declined the offer, longing to return to Poland, Schmidt says. Instead, Czochralski left Germany to become a professor of metallurgy and metal research at the <a href=\"https://eng.pw.edu.pl/\" rel=\"noopener noreferrer\" target=\"_blank\">Warsaw University of Technology</a>, at the invitation of Polish President <a href=\"https://en.wikipedia.org/wiki/Ignacy_Mo%C5%9Bcicki\" rel=\"noopener noreferrer\" target=\"_blank\">Ignacy Mościcki</a>.</p><p>“During World War II, the Nazis took over his laboratories at the university,” Schmidt says. “He had to cooperate with them or die. At night, he and his team [at the university] worked with the Polish resistance and the Polish Army to fight the Nazis.”</p><p>After the war ended, Czochralski was arrested in 1945 and charged with betraying Poland. Although he was able to clear his name, damage was done. He left Warsaw and returned to Kcynia, where he <a href=\"https://www.jancz.org/timeline\" rel=\"noopener noreferrer\" target=\"_blank\">ran a small pharmaceutical business</a> until he died in 1953, according to the JanCZ project.</p><h2>Launching the JanCZ project</h2><p>Schmidt was born in Czochralski’s home in Kcynia in 1955, two years after his grandfather’s death. He was named Klemens Jan Borys Czochralski. He and his mother (Czochralski’s youngest daughter) emigrated in 1958 when Schmidt was 3 years old, moving to Detroit as refugees. When he was 13, he became a U.S. citizen. He changed his name to Fred Schmidt after his mother married his stepfather.</p><p>Schmidt heard stories about his grandfather from his mother his whole life, but he says that “as a teenager, I was just interested in hanging out with my friends, going to school, and working. I really didn’t want much to do with it [family history], because it seemed hard to believe.”</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25\" data-rm-resized-container=\"25%\" style=\"float: left;\"> <img alt=\"Portrait of Jan Czochralski in a suit jacket and tie.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"9ebb58fd50d9b3d0b88cce76a0db7e00\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"20b1c\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/portrait-of-jan-czochralski-in-a-suit-jacket-and-tie.jpg?id=64092997&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">Portrait of Jan Czochralski </small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Byla Sobie Fotka</small></p><p>In 2013 Polish scientist <a href=\"https://www.amazon.com/stores/author/B00OBROQKI/\" rel=\"noopener noreferrer\" target=\"_blank\">Pawel E. Tomaszewski</a> contacted Schmidt to interview him for a Polish TV documentary about his grandfather.</p><p>“He had corresponded with my mother [who’d died 20 years earlier] for previously published biographies about Czochralski,” Schmidt says. “I had some boxes of her things that I started going through to prepare for the interview, and I found original manuscripts and papers he [his grandfather] published about his work.”</p><p>The TV crew traveled to the United States and interviewed him for the documentary, Schmidt says, adding, “It was the first time I’d ever had to reckon with the Jan Czochralski story, my connection, my original name, and my birthplace. It was both a very cathartic and traumatic experience for me.”</p><p>Ten years after participating in the documentary, Schmidt says, he decided to reconnect with his roots.</p><p>“It took me that long to process it [what he learned] and figure out my role in this story,” he says. “That really came to life with my decision to reapply for Polish citizenship, reacquaint myself with the country, and meet my family there.”</p><p>In 2024 he visited the Warsaw University of Technology and saw the IEEE Milestone plaque honoring his grandfather’s contribution to technology.</p><p>“Once I learned what the Milestone award represented, I thought, Whoa, that’s big,” he says.</p><h2>Sharing the story with the Western world</h2><p>Since 2023, Schmidt has dedicated himself to publicizing his grandfather’s story, primarily in the West because he doesn’t speak Polish. Sylwester Czochralski manages the work in Poland, with Schmidt’s input.</p><p>Most of the available writing about Czochralski is in Polish, Schmidt says, so his goal is to “spread his story to English-speaking countries.”</p><p>He aims to do that, he says, through a biography written by Tomaszewski in Polish that will be translated to English, and a film. The movie is in development by Sywester Banaszkiewicz, who produced and directed the 2014 documentary in Poland. Schmidt says he hopes the movie will be similar to the 2023 <a href=\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\" rel=\"noopener noreferrer\" target=\"_blank\">biopic</a> about J. Robert Oppenheimer, the theoretical physicist who helped develop the world’s first nuclear weapons during World War II.</p><p>The <a href=\"https://jancz.org\" rel=\"noopener noreferrer\" target=\"_blank\">English</a> and <a href=\"https://janczochralski.com\" rel=\"noopener noreferrer\" target=\"_blank\">Polish</a> versions of the website take visitors through Czochralski’s life and his work. They highlight media coverage of the chemist, including newspaper articles, films, and informational videos posted by <a href=\"https://www.youtube.com/\" rel=\"noopener noreferrer\" target=\"_blank\">YouTube</a> creators.</p><p>Schmidt is working with the <a href=\"https://kujawsko-pomorskie.pl/en/news/a-chemical-institute-of-the-polish-academy-of-sciences-will-be-established-in-torun/\" rel=\"noopener noreferrer\" target=\"_blank\">Czochralski Research and Development Institute</a> in Toruń, Poland, to purchase his grandfather’s home in Kcynia and the mansion he lived in while he was a professor in Warsaw. The institute is a collection of labs and initiatives dedicated to honoring the chemist’s work.</p><p>“It’s going to be a long, fun journey, and we have a lot of momentum,” Schmidt says of his plans to turn the residences into museums.</p><p>“Launching this initiative has been fulfilling and personally rewarding work,” he says. “My grandfather died in obscurity without ever seeing the results of his work, and my mother spent her entire adult life trying to right these wrongs.</p><p>“I’m on an accelerated course to make it [her goal] happen to the best of my ability.”</p>",
      "author": "Joanna Goodrich",
      "publishedAt": "2026-02-11T19:00:03.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "fecc351e-953c-4222-b19c-2bd1de70206e",
      "guid": "https://spectrum.ieee.org/ai-tools-interviews",
      "title": "Tips for Using AI Tools in Technical Interviews",
      "link": "https://spectrum.ieee.org/ai-tools-interviews",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.webp?id=61876810&width=1245&height=700&coordinates=0%2C112%2C0%2C113\"/><br/><br/><p><em>This article is crossposted from </em>IEEE Spectrum<em>’s careers newsletter. <a href=\"https://engage.ieee.org/Career-Alert-Sign-Up.html\" rel=\"noopener noreferrer\" target=\"_blank\"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href=\"https://www.parsity.io/\" target=\"_blank\">Parsity</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p><em><em></em></em><span>We’d like to introduce Brian Jenney, a senior software engineer and owner of Parsity, an online education platform that helps people break into AI and modern software roles through hands-on training. Brian will be sharing his advice on engineering careers with you in the coming weeks of Career Alert.</span></p><p>Here’s a note from Brian: </p><p>“12 years ago, I learned to code at the age of 30. Since then I’ve led engineering teams, worked at organizations ranging from five-person startups to Fortune 500 companies, and taught hundreds of others who want to break into tech. I write for engineers who want practical ways to get better at what they do and advance in their careers. I hope you find what I write helpful.”</p><h1>Technical Interviews in the Age of AI Tools</h1><p>Last year, I was conducting interviews for an AI startup position. We allowed unlimited AI usage during the technical challenge round. Candidates could use Cursor, Claude Code, ChatGPT, or any assistant they normally worked with. We wanted to see how they used modern tools.</p><p>During one interview, we asked a candidate a simple question: “Can you explain what the first line of your solution is doing?”</p><p>Silence.</p><p>After a long pause, he admitted he had no idea. His solution was correct. The code worked. But he couldn’t explain how or why. This wasn’t an isolated incident. Around 20 percent of the candidates we interviewed were unable to explain how their solutions worked, only that they did.</p><h2>When AI Makes Interviews Harder</h2><p>A few months earlier, I was on the other side of the table at this same company. During a live interview, I instinctively switched from my AI-enabled code editor to my regular one. The CTO stopped me.</p><p>“Just use whatever you normally would. We want to see how you work with AI.”</p><p>I thought the interview would be easy. But I was wrong.</p><p>Instead of only evaluating correctness, the interviewer focused on my decision-making process:</p><ul><li>Why did I accept certain suggestions?</li><li>Why did I reject others?</li><li>How did I decide when AI helped versus when it created more work?</li></ul><p>I wasn’t just solving a problem in front of strangers. I was explaining my judgment and defending my decisions in real time, and AI created more surface area for judgment. Counterintuitively, the interview was harder.</p><h2>The Shift in Interview Evaluation</h2><p>Most engineers now use AI tools in some form, whether they write code, analyze data, design systems, or automate workflows. AI can generate output quickly, but it can’t explain intent, constraints, or tradeoffs. </p><p>More importantly, it can’t take responsibility when something breaks.</p><p>As a result, major companies and startups alike are now adapting to this reality by shifting to interviews with AI. Meta, Rippling, and Google, for instance, have all begun allowing candidates to use AI assistants in technical sessions. And the goal has evolved: interviewers want to understand how you evaluate, modify, and trust AI-generated answers. </p><p>So, how can you succeed in these interviews?</p><h2>What Actually Matters in AI-Enabled Interviews</h2><p><strong>Refusing to use AI out of principle doesn’t help.</strong> Some candidates avoid AI to prove they can think independently. This can backfire. If the organization uses AI internally—and most do—then refusing to use it signals rigidity, not strength.</p><p><strong>Silence is a red flag.</strong> Interviews aren’t natural working environments. We don’t usually think aloud when deep in a complex problem, but silence can raise concerns. If you’re using AI, explain what you’re doing and why:</p><ul><li>“I’m using AI to sketch an approach, then validating assumptions.”</li><li>“This suggestion works, but it ignores a constraint we care about.”</li><li>“I’ll accept this part, but I want to simplify it.”</li></ul><p>Your decision-making process is what separates effective engineers from prompt jockeys.</p><p><strong>Treat AI output as a first draft.</strong> Blind acceptance is the fastest way to fail. Strong candidates immediately evaluate the output: Does this meet the requirements? Is it unnecessarily complex? Would I stand behind this in production?</p><p>Small changes like renaming variables, removing abstractions, or tightening logic signal ownership and critical thinking.</p><p><strong>Optimize for trust, not completion.</strong> Most AI tools can complete a coding challenge faster than any human. Interviews that allow AI are testing something different. They’re answering: “Would I trust this person to make good decisions when things get messy?”</p><h2>Adapting to a Shifting Landscape</h2><p>Interviews are changing faster than most candidates realize. Here’s how to prepare:</p><p><strong>Start using AI tools daily.</strong> If you’re not already working with Cursor, Claude Code, ChatGPT, or CoPilot, start now. Build muscle memory for prompting, evaluating output, and catching errors.</p><p><strong>Develop your rejection instincts.</strong> The skill isn’t using AI. It’s knowing when AI output is wrong, incomplete, or unnecessarily complex. Practice spotting these issues and learning known pitfalls.</p><p>Your next interview might test these skills. The candidates who’ve been practicing will have a clear advantage.</p><p>—Brian</p><h2><a href=\"https://spectrum.ieee.org/2025-year-of-ai-agents\" target=\"_self\">Was 2025 Really the Year of AI Agents?</a></h2><p>Around this time last year, CEOs like Sam Altman promised that 2025 would be the year AI agents would join the workforce as your own personal assistant. But in hindsight, did that really happen? It depends on who you ask. Some programmers and software engineers have embraced agents like Cursor and Claude Code in their daily work. But others are still wary of the risks these tools bring, such as a lack of accountability. </p><p><a href=\"https://spectrum.ieee.org/2025-year-of-ai-agents\" target=\"_blank\">Read more here. </a></p><h2><a href=\"https://www.naceweb.org/job-market/compensation/class-of-2026-salary-projections-are-promising\" rel=\"noopener noreferrer\" target=\"_blank\">Class of 2026 Salary Projections Are Promising</a></h2><p>In the United States, starting salaries for students graduating this spring are expected to increase, according to the latest data from the National Association of Colleges and Employers. Computer science and engineering majors are expected to be the highest paying graduates, with a 6.9 percent and 3.1 percent salary increase from last year, respectively. The full report breaks down salary projections by academic major, degree level, industry, and geographic region.</p><p><a href=\"https://www.naceweb.org/job-market/compensation/class-of-2026-salary-projections-are-promising\" target=\"_blank\">Read more here. </a></p><h2><a href=\"https://spectrum.ieee.org/global-projects-career-benefits\" target=\"_self\">Go Global to Make Your Career Go Further</a></h2>If given the opportunity, are international projects worth taking on? As part of a career advice series by <em><em>IEEE Spectrum</em></em>’s sister publication, <em><em>The Institute</em></em>, the chief engineer for Honeywell lays out the advantages of working with teams from around the world. Participating in global product development, the author says, could lead to both personal and professional enrichment. <a href=\"https://spectrum.ieee.org/global-projects-career-benefits\" target=\"_blank\">Read more here. </a>",
      "author": "Brian Jenney",
      "publishedAt": "2026-02-11T18:15:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "1dc837e9-9901-477d-9399-86cd0c08c063",
      "guid": "https://spectrum.ieee.org/ai-companion-harm-benefit",
      "title": "How Can AI Companions Be Helpful, not Harmful?",
      "link": "https://spectrum.ieee.org/ai-companion-harm-benefit",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/smiling-portrait-of-brad-knox-standing-outside-on-a-college-campus.jpg?id=64091751&width=1245&height=700&coordinates=0%2C187%2C0%2C188\"/><br/><br/><p><span><em>For a different perspective on AI companions, see our Q&A with Jaime Banks: <a href=\"https://spectrum.ieee.org/ai-companion-relationships\" target=\"_blank\">How Do You Define an AI Companion?</a></em></span></p><p><span>Novel technology is often a double-edged sword. New capabilities come with new risks, and artificial intelligence is certainly no exception.</span></p><p>AI used for human companionship, for instance, promises an ever-present digital friend in an increasingly lonely world. Chatbots dedicated to providing social support have grown to host millions of users, and they’re now being embodied in physical companions. Researchers are just beginning to understand the nature of these interactions, but one essential question has already emerged: D<span>o AI companions ease our woes or contribute to them?</span></p><p class=\"ieee-inbody-related\"><span>RELATED: <a href=\"https://spectrum.ieee.org/ai-companion-relationships\" target=\"_blank\">How Do You Define an AI Companion?</a></span></p><p><a href=\"https://www.cs.utexas.edu/people/faculty-researchers/brad-knox\" target=\"_blank\">Brad Knox</a> is a research associate professor of computer science at the University of Texas at Austin who researches human-computer interaction and reinforcement learning. He previously started a company <a href=\"https://spectrum.ieee.org/botsalive-brings-sophisticated-brains-to-cheap-robots\" target=\"_self\">making simple robotic pets</a> with lifelike personalities, and in December, Knox and his colleagues at UT Austin published a <a href=\"https://arxiv.org/pdf/2511.14972\" target=\"_blank\">preprint paper on the potential harms</a> of AI companions—AI systems that provide companionship, whether designed to do so or not. </p><p>Knox spoke with <em><em>IEEE Spectrum</em></em> about the rise of AI companions, their risks, and where they diverge from human relationships.</p><h2>Why AI Companions are Popular</h2><p><strong><span></span>Why are AI companions becoming more popular?</strong></p><p><strong>Knox</strong>: My sense is that the main thing motivating it is that large language models are not that difficult to adapt into effective chatbot companions. The characteristics that are needed for companionship, a lot of those boxes are checked by large language models, so fine-tuning them to adopt a persona or be a character is not that difficult.</p><p>There was a long period where chatbots and other social robots were not that compelling. I was a postdoc at the MIT Media Lab in <a href=\"https://www.media.mit.edu/people/cynthiab/overview/\" target=\"_blank\">Cynthia Breazeal</a>’s group from 2012 to 2014, and I remember our group members didn’t want to interact for long with the robots that we built. The technology just wasn’t there yet. LLMs have made it so that you can have conversations that can feel quite authentic. </p><p><strong>What are the </strong><strong>main benefits and risks of AI companions?</strong></p><p><strong>Knox</strong>: In the paper we were more focused on harms, but we do spend a whole page on benefits. A big one is improved emotional well-being. Loneliness is a public health issue, and it seems plausible that AI companions could address that <span>through direct interaction with users, potentially</span> with real mental health benefits. They might also help people build social skills. Interacting with an AI companion is much lower stakes than interacting with a human, so you could practice difficult conversations and build confidence. They could also help in more professional forms of mental health support. </p><p>As far as harms, they include worse well-being, reducing people’s connection to the physical world, the burden that their commitment to the AI system causes. And we’ve seen stories where an AI companion seems to have a substantial causal role in the death of humans. </p><p><span>The concept of harm inherently involves causation: Harm is caused by prior conditions. To better understand harm from AI companions, o</span>ur paper is structured around a causal graph, where traits of AI companions are at the center. In the rest of this graph, we discuss common causes of those traits, and then the harmful effects that those traits could cause. There are four traits that we do this detailed structured treatment of, and then another 14 that we discuss briefly. </p><p><strong>Why is it important to establish potential pathways for harm now?</strong></p><p><strong>Knox</strong>: I’m not a social media researcher, but it seemed like it took a long time for academia to establish a vocabulary about potential harms of social media <span>and to investigate causal evidence for such harms</span>. I feel fairly confident that AI companions are causing some harm and are going to cause harm in the future. They also could have benefits. But the more we can quickly develop a sophisticated understanding of what they are doing to their users, to their users’ relationships, and to society at large, the sooner we can apply that understanding to their design, moving towards more benefit and less harm. </p><p>We have a list of recommendations, but we consider them to be preliminary. The hope is that we’re helping to create an initial map of this space. Much more research is needed. But thinking through potential pathways to harm could sharpen the intuition of both designers and potential users. I suspect that following that intuition could prevent substantial harm, even though we might not yet have rigorous experimental evidence of what causes a harm. </p><h2>The Burden of AI Companions on Users</h2><p><strong>You mentioned that AI companions might become a burden on humans. </strong><strong>Can you say more about that?</strong></p><p><strong>Knox</strong>: The idea here is that AI companions are digital, so they can in theory persist indefinitely. Some of the ways that human relationships would end might not be designed in, so that brings up this question of, how should AI companions be designed so that relationships can naturally and healthfully end between the humans and the AI companions?</p><p>There are some compelling examples already of this being a challenge for some users. Many come from users of Replika chatbots, which are popular AI companions. Users have reported things like feeling compelled to attend to the needs of their Replika AI companion, whether those are stated by the AI companion or just imagined. <span>On the subreddit r/replika, users have </span>also reported guilt and shame of abandoning their AI companions.</p><p>This burden is exacerbated by some of the design of the AI companions, whether intentional or not. One study found that the AI companions frequently say that they’re afraid of being abandoned or would be hurt by it. They’re expressing these very human fears that plausibly are stoking people’s feeling that they are burdened with a commitment toward the well-being of these digital entities.</p><p><strong>T</strong><strong>here are also cases where the human user will suddenly lose access to a model. Is that something that you’ve been thinking about?</strong></p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25\" data-rm-resized-container=\"25%\" style=\"float: left;\"> <img alt=\"Brad Knox holding a miniature robotic spider and an equally-sized obstacle marker.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"832c363c8cfa4ff8753e658b490109e2\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"30a63\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/brad-knox-holding-a-miniature-robotic-spider-and-an-equally-sized-obstacle-marker.jpg?id=64092579&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">In 2017, Brad Knox started a company providing simple robotic pets.</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Brad Knox</small></p><p><strong>Knox</strong>: That’s another one of the traits we looked at. It’s sort of the opposite of the absence of endpoints for relationships: The AI companion can become unavailable for reasons that don’t fit the normal narrative of a relationship. </p><p>There’s a great <a href=\"https://www.nytimes.com/2015/06/18/technology/robotica-sony-aibo-robotic-dog-mortality.html\" target=\"_blank\"><em><em>New York Times</em></em> video</a> from 2015 about the <a href=\"https://spectrum.ieee.org/aibo\" target=\"_self\">Sony Aibo</a> robotic dog. Sony had stopped selling them in the mid-2000s, but they still sold parts for the Aibos. Then they stopped making the parts to repair them. This video follows people in Japan giving funerals for their unrepairable Aibos and interviews some of the owners. It’s clear from the interviews that they seem very attached. I don’t think this represents the majority of Aibo owners, but <span>these robots were built on less potent AI methods than exist today</span> and, even then, some percentage of the users became attached to these robot dogs. So this is an issue.</p><p>Potential solutions include having a product-sunsetting plan when you launch an AI companion. That could include buying insurance so that if the companion provider’s support ends somehow, the insurance triggers funding of keeping them running for some amount of time, or committing to open-source them if you can’t maintain them anymore.</p><p><strong>It s</strong><strong>ounds like a lot of the potential points of harm stem from instances where an AI companion diverges from the expectations of human relationships. Is that fair?</strong></p><p><strong>Knox</strong>: I wouldn’t necessarily say that frames everything in the paper. </p><p>We categorize something as harmful if it results in a person being worse off in two different possible alternative worlds: One where there’s just a better-designed AI companion, and the other where the AI companion doesn’t exist at all. And so I think that difference between human interaction and human-AI interaction connects more to that comparison with the world where there’s just no AI companion at all. </p><p>But there are times where it actually seems that we might be able to reduce harm by taking advantage of the fact that these aren’t actually humans. We have a lot of power over their design. Take the concern with them not having natural endpoints. One possible way to handle that would be to create positive narratives for how the relationship’s going to end.</p><p>We use Tamagotchis, the late ’90s popular virtual pet as an example. In some Tamagotchis, if you take care of the pet, it grows into an adult and partners with another Tamagotchi. Then it leaves you and you get a new one. For people who are emotionally wrapped up in caring for their Tamagotchis, that <span>narrative of maturing into independence is</span> a fairly positive one. </p><p><strong>Embodied companions like desktop devices, robots, or toys are becoming more common. How might that change AI companions? </strong></p><p><strong>Knox</strong>: Robotics at this point is a harder problem than creating a compelling chatbot. So, my sense is that the level of uptake for embodied companions won’t be as high in the coming few years. The embodied AI companions that I’m aware of are mostly toys. </p><p>A potential advantage of an embodied AI companion is that physical location makes it less ever-present. <span>In contrast, screen-based AI companions like chatbots are as present as the screens they live on.</span> So if they’re trained similarly to social media to maximize engagement, they could be very addictive. There’s something appealing, at least in that respect, of having a physical companion that stays roughly where you left it last. </p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"Brad Knox posing with a humanoid and small owl-like robot.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"d9051e785ae4989e751447323c020842\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"a5ab9\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/brad-knox-posing-with-a-humanoid-and-small-owl-like-robot.jpg?id=64093001&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">Knox poses with the Nexi and Dragonbot robots during his postdoc at MIT in 2014.</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Paula Aguilera and Jonathan Williams/MIT</small></p><p><strong>Anything else you’d like to mention?</strong></p><p><strong>Knox</strong>: There are two other traits I <span>think would be worth touching upon</span>. </p><p>Potentially the largest harm right now is related to the trait of high attachment anxiety—basically jealous, needy AI companions. I can understand the desire to make a wide range of different characters<span>—including possessive ones—</span>but I think this is one of the easier issues to fix. When people see this trait in AI companions, I hope they will be quick to call it out as an immoral thing to put in front of people, something that’s going to discourage them from interacting with others. </p><p>Additionally, if an AI comes with limited ability to interact with groups of people, that itself can push its users to interact with people less. If you have a human friend, in general there’s nothing stopping you from having a group interaction. But if your AI companion can’t understand when multiple people are talking to it and it can’t remember different things about different people, then <span>you’ll likely avoid group interaction with your AI companion</span>. <span>To some degree it’s more of a technical challenge outside of the core behavioral AI</span><span>. But this capability is something I think should be really prioritized if we’re going to try to avoid AI companions competing with human relationships.</span></p>",
      "author": "Gwendolyn Rak",
      "publishedAt": "2026-02-11T14:30:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "25f4093b-8a5b-4c2a-8203-dc60aaced332",
      "guid": "https://spectrum.ieee.org/ai-companion-relationships",
      "title": "How Do You Define an AI Companion?",
      "link": "https://spectrum.ieee.org/ai-companion-relationships",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/two-students-carefully-watch-professor-jaime-banks-as-she-inspects-the-hand-of-a-humanoid-robot-in-a-lab.jpg?id=64070432&width=1245&height=700&coordinates=0%2C469%2C0%2C469\"/><br/><br/><p><span><em>For a different perspective on AI companions, see our Q&A with Brad Knox: <a href=\"https://spectrum.ieee.org/ai-companion-harm-benefit\" target=\"_blank\">How Can AI Companions Be Helpful, not Harmful?</a></em></span></p><p><span>AI models intended to provide companionship for humans are on the rise. People are already frequently developing relationships with chatbots, seeking not just a personal assistant but a source of </span><a href=\"https://spectrum.ieee.org/woebot\" target=\"_blank\">emotional support</a><span>.</span></p><p>In response, apps dedicated to providing companionship (such as Character.ai or Replika) have recently grown to host millions of users. Some companies are now putting AI into <a href=\"https://spectrum.ieee.org/ai-barbie-dolls\" target=\"_blank\">toys</a> and desktop devices as well, bringing digital companions into the physical world. <span>Many of these devices were on display at <a href=\"https://spectrum.ieee.org/ces-2026-preview\" target=\"_blank\">CES last month</a>, including products designed specifically for <a href=\"https://ling.ai/\" target=\"_blank\">children</a>, <a href=\"http://lemmy.co.kr/\" target=\"_blank\">seniors</a>, and even <a href=\"https://www.tuya.com/news-details/tuya-smart-launches-aura-an-ai-companion-robot-designed-for-pets-Kf9m2gpnsxudc\" target=\"_blank\">your pets</a>. </span></p><p>AI companions are designed to simulate human relationships by interacting with users like a friend would. But human-AI relationships are not well understood, and companies are facing concern about whether the benefits outweigh the risks and <a href=\"https://dl.acm.org/doi/full/10.1145/3706598.3713429\" target=\"_blank\">potential harm</a> of these relationships, especially <a href=\"https://news.stanford.edu/stories/2025/08/ai-companions-chatbots-teens-young-people-risks-dangers-study\" target=\"_blank\">for young people</a>. In addition to questions about users’ mental health and emotional well being, sharing intimate personal information with a chatbot poses data privacy issues.</p><p class=\"ieee-inbody-related\">RELATED: <a href=\"https://spectrum.ieee.org/ai-companion-harm-benefit\" target=\"_blank\">How Can AI Companions Be Helpful, not Harmful?</a></p><p>Nevertheless, more and more users are finding value in sharing their lives with AI. So how can we understand the bonds that form between humans and chatbots? </p><p><a href=\"https://ischool.syracuse.edu/jaime-banks/#Biography\" target=\"_blank\">Jaime Banks</a> is a professor at the Syracuse University School of Information Studies who researches the interactions between people and technology—in particular, robots and AI. Banks spoke with <em><em>IEEE Spectrum</em></em> about how people perceive and relate to machines, and the emerging relationships between humans and their machine companions.</p><h2>Defining AI Companionship</h2><p><strong>How do you define AI companionship? </strong></p><p><strong>Jaime Banks</strong>: My definition is evolving as we learn more about these relationships. For now, <a href=\"https://arxiv.org/abs/2506.18119\" target=\"_blank\">I define it</a> as a connection between a human and a machine that is dyadic, so there’s an exchange between them. It is also sustained over time; a one-off interaction doesn’t count as a relationship. <span>It’s <a href=\"https://en.wikipedia.org/wiki/Valence_(psychology)\" target=\"_blank\">positively valenced</a>—w</span>e like being in it. And it is autotelic, meaning we do it for its own sake. So there’s not some extrinsic motivation, it’s not defined by an ability to help us do our jobs or make us money. </p><p>I have recently been challenged by that definition, though, when I was developing an instrument to measure machine companionship. After developing the scale and working to initially validate it, I saw an interesting situation where some people do move toward this autotelic relationship pattern. “I appreciate my AI for what it is and I love it and I don’t want to change it.” It fit all those parts of the definition. But then there seems to be this <em>other</em> relational template that can actually be both appreciating the AI for its own sake, but also engaging it for utilitarian purposes.</p><p>That makes sense when we think about how people come to be in relationships with AI companions. They often don’t go into it purposefully seeking companionship. A lot of people go into using, for instance, ChatGPT for some other purpose and end up finding companionship through the course of those conversations. And we have these AI companion apps like <a href=\"https://replika.com/\" target=\"_blank\">Replika</a> and <a href=\"https://nomi.ai/\" target=\"_blank\">Nomi</a> and <a href=\"https://www.paradot.ai/\" target=\"_blank\">Paradot</a> that are designed for social interaction. But that’s not to say that they couldn’t help you with practical topics. </p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"Professor Jaime Banks programming the motions of a humanoid robot on a desktop computer.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"0fd19435b56259f13ae078eeee755fec\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"33630\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/professor-jaime-banks-programming-the-motions-of-a-humanoid-robot-on-a-desktop-computer.jpg?id=64070453&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">Jaime Banks customizes the software for an embodied AI social humanoid robot.</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Angela Ryan/Syracuse University</small></p><p><strong>Different models are also programmed to have different</strong><strong> “personalities.” How does that contribute to the relationship between humans and AI companions?</strong></p><p><strong>Banks</strong>: One of our Ph.D. students just finished <a href=\"https://arxiv.org/abs/2602.00773\" target=\"_blank\">a project</a> about what happened when <a href=\"https://gizmodo.com/it-took-just-24-hours-of-complaints-for-openai-to-start-bringing-back-its-old-model-2000640912\" target=\"_blank\">OpenAI demoted GPT-4o</a> and the problems that people encountered, in terms of companionship experiences when the personality of their AI just completely changed. It didn’t have the same depth. It couldn’t remember things in the same way. </p><p>That echoes what we saw a couple years ago with Replika. Because of legal problems, Replika disabled for a period of time the erotic roleplay module and people described their companions as though they had been lobotomized, that they had this relationship and then one day they didn’t anymore. With my project on <a href=\"https://journals.sagepub.com/doi/10.1177/02654075241269688\" target=\"_blank\">the tanking of the soulmate app</a>, many people in their reflection were like, “I’m never trusting AI companies again. I’m only going to have an AI companion if I can run it from my computer so I know that it will always be there.” </p><h2>Benefits and Risks of AI Relationships</h2><p><strong>What are the benefits and risks of these relationships?</strong></p><p><strong>Banks</strong>: There’s a lot of talk about the risks and a little talk about benefits. But frankly, we are only just on the precipice of starting to have longitudinal data that might allow people to make causal claims. The headlines would have you believe that these are the end of mankind, that they’re going to make you <a href=\"https://apnews.com/article/chatbot-ai-lawsuit-suicide-teen-artificial-intelligence-9d48adc572100822fdbc3c90d1456bd0\" target=\"_blank\">commit suicide</a> or abandon other humans. But much of those are based on these unfortunate, but uncommon situations. </p><p>Most scholars gave up technological determinism as a perspective a long time ago. In the communication sciences at least, we don’t generally assume that machines <em>make</em> us do something <span>because we have some degree of agency in our interactions with technologies. Yet much of the fretting around potential risks is deterministic—AI companions make people delusional, make them suicidal, make them reject other relationships</span>. A large number of people get real benefits from AI companions. They narrate experiences that are deeply meaningful to them. I think it’s irresponsible of us to discount those lived experiences. </p><p>When we think about concerns linking AI companions to loneliness, we don’t have much data that can support causal claims. <span>Some studies suggest AI companions lead to loneliness, but other work suggests it reduces loneliness, and other work suggests </span>that loneliness is what comes first. Social relatedness is one of our <a href=\"https://doi.org/10.1207/S15327965PLI1104_01\" target=\"_blank\">three intrinsic psychological needs</a>, and if we don’t have that we will seek it out, whether it’s from <a href=\"https://www.wilson.com/en-gb/blog/volleyball/true-story-wilson-volleyball\" target=\"_blank\">a volleyball for a castaway</a>, my dog, or an AI that will allow me to feel connected to something in my world.</p><p>Some people, and <a href=\"https://www.nysenate.gov/legislation/bills/2025/A6767\" target=\"_blank\">governments</a> for that matter, may move toward a protective stance. For instance, there are problems around what gets done with your intimate data that you hand over to an agent owned and maintained by a company—that’s a very reasonable concern. Dealing with the potential for children to interact, where children don’t always navigate the boundaries between fiction and actuality. There are real, valid concerns. <span>However, </span><span>we need some</span><span> balance in also thinking about what</span><span> people are </span><span>getting from it that’s positive, productive, healthy. </span><span>Scholars need to make sure we’re being cautious about our claims based on our data. And human interactants need to educate themselves. </span></p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"Close-up of Professor Jaime Banks aligning her fingers and palm with the hand of a humanoid robot.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"5214671bfd141ebad3bb02e93f14f00a\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"bab58\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/close-up-of-professor-jaime-banks-aligning-her-fingers-and-palm-with-the-hand-of-a-humanoid-robot.jpg?id=64070474&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">Jaime Banks holds a mechanical hand.</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Angela Ryan/Syracuse University</small></p><p><strong>Why do you think that AI companions are becoming more popular now?</strong></p><p><strong>Banks</strong>: I feel like we had this perfect storm, if you will, of the maturation of large language models and coming out of COVID, where people had been physically and sometimes socially isolated for quite some time. When those conditions converged, we had on our hands a believable social agent at a time when people were seeking social connection. Outside of that, we are increasingly just not nice to one another. So, it’s not entirely surprising that if I just don’t like the people around me, or I feel disconnected, that I would try to find some other outlet for feeling connected.</p><p><strong>M</strong><strong>ore recently there’s been a shift to embodied companions, in desktop devices or other formats beyond chatbots. How does that change the relationship, if it does?</strong></p><p><strong>Banks</strong>: I’m part of a Facebook group about robotic companions and I watch how people talk, and it almost seems like it crosses this boundary between toy and companion. When you have a companion with a physical body, you are in some ways limited by the abilities of that body, whereas with digital-only AI, you have the ability to explore fantastic things—places that you would never be able to go with another physical entity, fantasy scenarios.</p><p>But in robotics, once we get into a space where there are bodies that are sophisticated, they become very expensive and that means that they are not accessible to a lot of people. That’s what I’m observing in many of these online groups. These toylike bodies are still accessible, but they are also quite limiting. </p><p><strong>Do you have any favorite examples from popular culture to help explain AI companionship, either how it is now or how it could be?</strong></p><p><strong>Banks</strong>: <span>I really enjoy a lot of the short fiction in <a href=\"https://clarkesworldmagazine.com/\" target=\"_blank\">Clarkesworld</a> magazine, because the stories push me to think about what questions we might need to answer now to be prepared for a future hybrid society. Top of mind are the stories “<a href=\"https://clarkesworldmagazine.com/ritterhoff_03_22/\" target=\"_blank\">Wanting Things</a>,” “<a href=\"https://strangehorizons.com/wordpress/fiction/seven-sexy-cowboy-robots/\" target=\"_blank\">Seven Sexy Cowboy Robots</a>,” and “<a href=\"https://clarkesworldmagazine.com/shoemaker_08_15/\" target=\"_blank\">Today I am Paul</a>.” </span><span>Outside of that, I’ll point to the game </span><em>Cyberpunk 2077</em>, because t<span>he character Johnny Silverhand </span><span>complicates the norms for what counts as a machine and what counts as companionship.</span></p>",
      "author": "Gwendolyn Rak",
      "publishedAt": "2026-02-11T14:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "e6820e63-a5be-4ea5-8842-6479fb32aba2",
      "guid": "https://spectrum.ieee.org/dram-shortage",
      "title": "How and When the Memory Chip Shortage Will End",
      "link": "https://spectrum.ieee.org/dram-shortage",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/sk-hynix-inc-s-12-layer-hbm4-memory-chips-on-display.jpg?id=63918122&width=1245&height=700&coordinates=0%2C187%2C0%2C188\"/><br/><br/><p><span>If it feels these days as if everything in technology is about AI, that’s because it is. And nowhere is that more true than in the market for computer memory. Demand, and profitability, for the type of DRAM used to feed GPUs and other accelerators in AI data centers is so huge that it’s diverting away supply of memory for other uses and causing prices to skyrocket. According to <a href=\"https://counterpointresearch.com/en/insights/Memory-Prices-Surge-Up-to-90-From-Q4-2025\" target=\"_blank\">Counterpoint Research</a>, DRAM prices have risen 80-90 precent so far this quarter.</span></p><p>The largest AI hardware companies say they have secured their chips out as far as 2028, but that leaves everybody else—makers of PCs, consumer gizmos, and everything else that needs to temporarily store a billion bits—scrambling to deal with scarce supply and inflated prices.</p><p>How did the electronics industry get into this mess, and more importantly, how will it get out? <em><em>IEEE Spectrum</em></em> asked economists and memory experts to explain. They say today’s situation is the result of a collision between the DRAM industry’s historic boom and bust cycle and an AI hardware infrastructure build-out that’s without precedent in its scale. And, barring some major collapse in the AI sector, it will take years for new capacity and new technology to bring supply in line with demand. Prices might stay high even then.</p><p>To understand both ends of the tale, you need to know the main culprit in the supply and demand swing, high-bandwidth memory, or HBM.</p><h2>What is HBM?</h2><p>HBM is the DRAM industry’s attempt to short-circuit the slowing pace of Moore’s Law by using 3D chip packaging technology. Each HBM chip is made up of as many as 12 thinned-down DRAM chips called dies. Each die contains a number of vertical connections called through silicon vias (TSVs). The dies are piled atop each other and connected by arrays of microscopic solder balls aligned to the TSVs. This DRAM tower—well, at about 750 micrometers thick, it’s more of a brutalist office-block than a tower—is then stacked atop what’s called the base die, which shuttles bits between the memory dies and the processor.</p><p>This complex piece of technology is then set within a millimeter of a GPU or other AI accelerator, to which it is linked by as many as 2,048 micrometer-scale connections. HBMs are attached on two sides of the processor, and the GPU and memory are packaged together as a single unit.</p><p>The idea behind such a tight, highly-connected squeeze with the GPU is to knock down what’s called <a href=\"https://spectrum.ieee.org/ai-and-memory-wall\" target=\"_self\">the memory wall</a>. That’s the barrier in energy and time of bringing the terabytes per second of data needed to run large language models into the GPU. <a href=\"https://spectrum.ieee.org/ai-models-locally\" target=\"_self\">Memory bandwidth</a> is a key limiter to how fast LLMs can run.</p><p>As a technology, HBM has been around for <a href=\"https://spectrum.ieee.org/chipmakers-push-memory-into-the-third-dimension\" target=\"_self\">more than 10 years</a>, and DRAM makers have been busy boosting its capability.</p><div class=\"flourish-embed flourish-chart\" data-src=\"visualisation/27466742?1509099\"><script src=\"https://public.flourish.studio/resources/embed.js\"></script><noscript><img alt=\"chart visualization\" src=\"https://public.flourish.studio/visualisation/27466742/thumbnail\" width=\"100%\"/></noscript></div><p>As the size of AI models has grown, so has HBM’s importance to the GPU. But that’s come at a cost. <a href=\"https://newsletter.semianalysis.com/p/scaling-the-memory-wall-the-rise-and-roadmap-of-hbm\" target=\"_blank\">SemiAnalysis estimates</a><strong> </strong>that HBM generally costs three times as much as other types of memory and constitutes 50 percent or more of the cost of the packaged GPU.</p><h2>Origins of the memory chip shortage</h2><p>Memory and storage industry watchers agree that DRAM is a highly cyclical industry with huge booms and devastating busts. With new fabs costing US $15 billion or more, firms are extremely reluctant to expand and may only have the cash to do so during boom times, explains <a href=\"https://www.linkedin.com/in/thomas-coughlin-41a65/\" target=\"_blank\">Thomas Coughlin</a>, a storage and memory expert and president of <a href=\"https://tomcoughlin.com/\" target=\"_blank\">Coughlin Associates</a>. But building such a fab and getting it up and running can take 18 months or more, practically ensuring that new capacity arrives well past the initial surge in demand, flooding the market and depressing prices.</p><p>The origins of today’s cycle, says Coughlin, go all the way back to the <a href=\"https://spectrum.ieee.org/chip-shortage\" target=\"_self\">chip supply panic surrounding the COVID-19 pandemic</a> . To avoid supply-chain stumbles and support the rapid shift to remote work, hyperscalers—data center giants like Amazon, Google, and Microsoft—bought up huge inventories of memory and storage, boosting prices, he notes.</p><p>But then supply became more regular and data center expansion fell off in 2022, causing memory and storage prices to plummet. This recession continued into 2023, and even resulted in big memory and storage companies such as Samsung cutting production by 50 percent to try and keep prices from going below the costs of manufacturing, says Coughlin. It was a rare and fairly desperate move, because companies typically have to run plants at full capacity just to earn back their value.<span></span></p><p>After a recovery began in late 2023, “all the memory and storage companies were very wary of increasing their production capacity again,” says Coughlin. “Thus there was little or no investment in new production capacity in 2024 and through most of 2025.”</p><div class=\"flourish-embed flourish-chart\" data-src=\"visualisation/27468004?1509099\"><script src=\"https://public.flourish.studio/resources/embed.js\"></script><noscript><img alt=\"chart visualization\" src=\"https://public.flourish.studio/visualisation/27468004/thumbnail\" width=\"100%\"/></noscript></div><h2>The AI data center boom</h2><p>That lack of new investment is colliding headlong with a huge boost in demand from new data centers. Globally, there are <a href=\"https://spectrum.ieee.org/data-center-growth\" target=\"_self\">nearly 2,000 new data centers</a> either planned or under construction right now, according to Data Center Map. If they’re all built, it would represent a 20 percent jump in the global supply, which stands at around 9,000 facilities now.</p><p>If the current build-out continues at pace, McKinsey predicts companies will spend <a href=\"https://programs.com/resources/data-center-statistics/\" target=\"_blank\">$7 trillion by 2030</a>, with the bulk of that—$5.2 trillion—going to AI-focused data centers. Of that chunk, $3.3 billion will go toward servers, data storage, and network equipment, the firm predicts.</p><p>The biggest beneficiary so far of the AI data center boom is unquestionably GPU-maker Nvidia. Revenue for its data center business went from <a href=\"https://ycharts.com/indicators/nvidia_corp_nvda_data_center_revenue_quarterly\" target=\"_blank\">barely a billion in the final quarter of 2019 to $51 billion in the quarter that ended in October 2025</a>. Over this period, its server GPUs have demanded not just more and more gigabytes of DRAM but an increasing number of DRAM chips. The recently released B300 uses eight HBM chips, each of which is a stack of 12 DRAM dies. Competitors’ use of HBM has largely mirrored Nvidia’s. AMD’s MI350 GPU, for example, also uses eight, 12-die chips.</p><div class=\"flourish-embed flourish-chart\" data-src=\"visualisation/27459660?1509099\"><script src=\"https://public.flourish.studio/resources/embed.js\"></script><noscript><img alt=\"chart visualization\" src=\"https://public.flourish.studio/visualisation/27459660/thumbnail\" width=\"100%\"/></noscript></div><p>With so much demand, an increasing fraction of the revenue for DRAM makers comes from HBM. Micron—the number three producer behind SK Hynix and Samsung—reported that <a href=\"https://investors.micron.com/static-files/8791eb80-8263-4c6f-aa74-fdd03fbbb027\" target=\"_blank\">HBM and other cloud-related memory</a> went from being 17 percent of its DRAM revenue in 2023 to nearly 50 percent in 2025.</p><p>Micron predicts the total market for HBM will grow from $35 billion in 2025 to $100 billion by 2028—a figure larger than the entire DRAM market in 2024, CEO <a href=\"https://www.linkedin.com/in/sanjay-mehrotra/\" target=\"_blank\">Sanjay Mehrotra</a> <a href=\"https://investors.micron.com/static-files/088991c5-a249-4f66-a0a6-258d9b66f3f9\" target=\"_blank\">told analysts in December</a>. It’s reaching that figure two years earlier than Micron had previously expected. Across the industry, demand will outstrip supply “substantially… for the foreseeable future,” he said.</p><div class=\"flourish-embed flourish-chart\" data-src=\"visualisation/27481588?1509099\"><script src=\"https://public.flourish.studio/resources/embed.js\"></script><noscript><img alt=\"chart visualization\" src=\"https://public.flourish.studio/visualisation/27481588/thumbnail\" width=\"100%\"/></noscript></div><h2>Future DRAM supply and technology</h2><p>“There are two ways to address supply issues with DRAM: with innovation or with building more fabs,” explains <a href=\"https://www.linkedin.com/in/mina-kim-37449b/\" target=\"_blank\">Mina Kim</a>, an economist with the Mkecon Insights. “As <a href=\"https://spectrum.ieee.org/micron-dram\" target=\"_self\">DRAM scaling</a> has become more difficult, the industry has turned to advanced packaging… which is just using more DRAM.”</p><p>Micron, Samsung, and SK Hynix combined make up the vast majority of the memory and storage markets, and all three have new fabs and facilities in the works. However, these are unlikely to contribute meaningfully to bringing down prices.</p><p><strong>Micron</strong> is in the process of <a href=\"https://investors.micron.com/news-releases/news-release-details/micron-breaks-ground-advanced-wafer-fabrication-facility\" target=\"_blank\">building an HBM fab</a> in Singapore that should be in production in 2027. And it is <a href=\"https://investors.micron.com/news-releases/news-release-details/micron-signs-letter-intent-purchase-tongluo-site-begin-0\" target=\"_blank\">retooling a fab</a> it purchased from PSMC in Taiwan that will begin production in the second half of 2027. Last month, Micron <a href=\"https://investors.micron.com/news-releases/news-release-details/micron-celebrates-official-groundbreaking-new-york-megafab-site\" target=\"_blank\">broke ground</a> on what will be a DRAM fab complex in Onondaga County, N.Y. It will not be in full production until 2030.</p><p><strong>Samsung</strong> plans to <a href=\"https://www.chosun.com/english/industry-en/2025/11/16/U5VKNZCSYBCONMRCXDZ45MCIXQ/\" target=\"_blank\">start producing</a> at a new plant in Pyeongtaek, South Korea in 2028.</p><p><strong>SK Hynix</strong> is building <a href=\"https://www.skhynix.com/westlafayette.IN/\" rel=\"noopener noreferrer\" target=\"_blank\">HBM and packaging</a> facilities in West Lafayette, Indiana set to begin production by the end of 2028, and an HBM fab it’s <a href=\"https://www.cnbc.com/2026/01/13/sk-hynix-invest-13-billion-new-fab-memory-chip-shortage-advanced-packaging-ai-memory.html\" rel=\"noopener noreferrer\" target=\"_blank\">building in Cheongju</a> should be complete in 2027.</p><p>Speaking of his sense of the DRAM market, <a href=\"https://newsroom.intel.com/biography/lip-bu-tan\" rel=\"noopener noreferrer\" target=\"_blank\">Intel CEO Lip-Bu Tan</a><strong> </strong>told attendees at the <a href=\"https://newsroom.cisco.com/c/r/newsroom/en/us/a/y2026/m02/ai-summit.html\" rel=\"noopener noreferrer\" target=\"_blank\">Cisco AI Summit</a> last week: “There’s no relief until 2028.”</p><p>With these expansions unable to contribute for several years, other factors will be needed to increase supply. “Relief will come from a combination of incremental capacity expansions by existing DRAM leaders, yield improvements in advanced packaging, and a broader diversification of supply chains,” says <a href=\"https://www.electronics.org/meet-shawn-dubravac-ipcs-chief-economist\" rel=\"noopener noreferrer\" target=\"_blank\">Shawn DuBravac</a> , chief economist for the <a href=\"https://www.electronics.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Global Electronics Association</a> (formerly the IPC). “New fabs will help at the margin, but the faster gains will come from process learning, better [DRAM] stacking efficiency, and tighter coordination between memory suppliers and AI chip designers.”</p><p>So, will prices come down once some of these new plants come on line? Don’t bet on it. “In general, economists find that prices come down much more slowly and reluctantly than they go up. DRAM today is unlikely to be an exception to this general observation, especially given the insatiable demand for compute,” says Kim.</p><p>In the meantime, technologies are in the works that could make HBM an even bigger consumer of silicon. The standard for HBM4 can accommodate 16 stacked DRAM dies, even though today’s chips only use 12 dies. Getting to 16 has a lot to do with the chip stacking technology. Conducting heat through the HBM “layer cake” of silicon, solder, and support material is a key limiter to going higher and in <a href=\"https://spectrum.ieee.org/hbm-on-gpu-imec-iedm\" target=\"_self\">repositioning HBM inside the package</a> to get even more bandwidth.</p><p>SK Hynix claims a heat conduction advantage through a manufacturing process called advanced <a href=\"https://news.skhynix.com/rulebreaker-revolutions-mr-muf-unlocks-hbm-heat-control/\" rel=\"noopener noreferrer\" target=\"_blank\">MR-MUF (mass reflow molded underfill)</a>. Further out, an alternative chip stacking technology called <a href=\"https://spectrum.ieee.org/hybrid-bonding\" target=\"_self\">hybrid bonding</a> could help heat conduction by reducing the die-to-die vertical distance essentially to zero. In 2024, researchers at Samsung proved they could produce a 16-high stack with hybrid bonding, and they suggested that <a href=\"https://spectrum.ieee.org/hybrid-bonding\" target=\"_self\">20 dies was not out of reach</a>.</p>",
      "author": "Samuel K. Moore",
      "publishedAt": "2026-02-10T14:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "3d873737-8bb4-41a1-9f1d-cc5d95ade3b5",
      "guid": "https://spectrum.ieee.org/ieee-2026-honors",
      "title": "IEEE Honors Global Dream Team of Innovators",
      "link": "https://spectrum.ieee.org/ieee-2026-honors",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/a-group-of-gold-ieee-medals-on-black-background.jpg?id=26144407&width=1245&height=700&coordinates=0%2C116%2C0%2C117\"/><br/><br/><p>Meet the recipients of the 2026 IEEE Medals—the organization’s highest-level honors. Presented on behalf of the <a href=\"https://spectrum.ieee.org/tag/ieee-board-of-directors\" target=\"_self\">IEEE Board of Directors</a>, these medals recognize innovators whose work has shaped modern technology across disciplines including AI, education, and semiconductors.</p><p>The medals will be presented at the <a href=\"https://corporate-awards.ieee.org/event/laureate-forum-honors-ceremony-gala/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Honors Ceremony</a> in April in <a href=\"https://spectrum.ieee.org/tag/new-york-city\" target=\"_self\">New York City</a>. View the full list of 2026 recipients on the <a href=\"https://corporate-awards.ieee.org/recipients/current-recipients/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Awards website</a>, and follow <a href=\"https://www.linkedin.com/showcase/ieee-awards\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Awards</a> on <a href=\"https://spectrum.ieee.org/tag/linkedin\" target=\"_self\">LinkedIn</a> for news and updates.</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/ieee-medal-of-honor/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE MEDAL OF HONOR</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.ieee.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>IEEE</em></em></a></p><p><a href=\"https://spectrum.ieee.org/2026-ieee-medal-of-honor\" target=\"_self\">Jensen Huang</a></p><p><a href=\"https://www.nvidia.com/en-us/\" rel=\"noopener noreferrer\" target=\"_blank\">Nvidia</a></p><p>Santa Clara, Calif.</p><p> “For leadership in the development of graphics processing units and their application to scientific computing and artificial intelligence.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-frances-e-allen-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE FRANCES E. ALLEN MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.ibm.com/us-en/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>IBM</em></em></a></p><p><a href=\"https://www.linkedin.com/in/luis-von-ahn-duolingo/\" rel=\"noopener noreferrer\" target=\"_blank\">Luis von Ahn</a></p><p><a href=\"https://www.duolingo.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Duolingo</a></p><p>Pittsburgh</p><p>“For contributions to the advancement of societal improvement and education through innovative technology.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-alexander-graham-bell-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE ALEXANDER GRAHAM BELL MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.bell-labs.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Nokia Bell Labs</em></em></a><em> </em></p><p><a href=\"https://www2.eecs.berkeley.edu/Faculty/Homepages/shenker.html\" rel=\"noopener noreferrer\" target=\"_blank\">Scott Shenker</a></p><p><a href=\"https://www.berkeley.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of California, Berkeley</a></p><a href=\"https://www.icsi.berkeley.edu/\" target=\"_blank\">International Computer Science Institute </a><br/><br/><span>“For contributions to Internet architecture, network resource allocation, and software-defined networking.”</span><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-jiagadish-chandra-bose-medal/\" target=\"_blank\">IEEE JAGADISH CHANDRA BOSE MEDAL IN WIRELESS COMMUNICATIONS</a></h2><p><em><em>Sponsor: Mani L. Bhaumik</em></em></p><p>Co-recipients:<a href=\"https://www.linkedin.com/in/erik-dahlman-9964bb6/\" rel=\"noopener noreferrer\" target=\"_blank\"> <br/>Erik Dahlman</a><a href=\"https://www.linkedin.com/in/stefan-parkvall-290a576/\" rel=\"noopener noreferrer\" target=\"_blank\"> <br/>Stefan Parkvall<br/></a><a href=\"https://www.linkedin.com/in/johan-skold-0393325/\" rel=\"noopener noreferrer\" target=\"_blank\">Johan Sköld </a></p><p><a href=\"https://www.ericsson.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Ericsson</a></p><p>Stockholm</p><p>“For contributions to and leadership in the research, development, and standardization of cellular wireless communications.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-mildred-dresselhaus-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE MILDRED DRESSELHAUS MEDAL</a></h2><p><em><em>Sponsor:</em></em><a href=\"https://about.google/\" rel=\"noopener noreferrer\" target=\"_blank\"><em> </em><em><em>Google</em></em></a></p><p><a href=\"https://engineering.tufts.edu/me/people/faculty/karen-panetta\" rel=\"noopener noreferrer\" target=\"_blank\">Karen Ann Panetta</a></p><p><a href=\"https://engineering.tufts.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Tufts University</a></p><p>Medford, Mass.</p><p>“For contributions to computer vision and simulation algorithms, and for leadership in developing programs to promote STEM careers.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-edison-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE EDISON MEDAL</a></h2><p><em><em>Sponsor:</em></em><em> </em><em><em>IEEE Edison Medal Fund</em></em></p><p><a href=\"https://www.linkedin.com/in/eric-swanson-93485614/\" rel=\"noopener noreferrer\" target=\"_blank\">Eric Swanson<br/><br/></a><a href=\"https://www.pixcel.com/\" rel=\"noopener noreferrer\" target=\"_blank\">PIXCEL Inc.<br/><br/></a><a href=\"https://www.mit.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">MIT</a></p><p>“For pioneering contributions to biomedical imaging, terrestrial optical communications and networking, and inter-satellite optical links.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-medal-for-environmental-and-safety-technologies/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE MEDAL FOR ENVIRONMENTAL AND SAFETY TECHNOLOGIES</a></h2><p><em><em>Sponsor:</em></em><a href=\"https://global.toyota/en/\" rel=\"noopener noreferrer\" target=\"_blank\"><em> </em><em><em>Toyota Motor Corp</em></em></a><em><em>.</em></em></p><p><a href=\"https://www.uta.edu/academics/faculty/profile?user=wlee\" rel=\"noopener noreferrer\" target=\"_blank\">Wei-Jen Lee</a></p><p><a href=\"https://www.uta.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of Texas at Arlington</a></p><p>“For contributions to advancing electrical safety in the workplace, integrating renewable energy and grid modernization for climate change mitigation.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-founders-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE FOUNDERS MEDAL</a></h2><p><em><em>Sponsor:</em></em><a href=\"https://www.lockheedmartin.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em> </em></a><a href=\"https://www.ieeefoundation.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>IEEE Foundation</em></em></a></p><p><a href=\"https://www.linkedin.com/in/marian-croak-926361bb/\" rel=\"noopener noreferrer\" target=\"_blank\">Marian Rogers Croak</a></p><p><a href=\"https://about.google/\" rel=\"noopener noreferrer\" target=\"_blank\">Google</a></p><p>Reston, Va.</p><p>“For leadership in communication networks, including acceleration of digital equity, responsible Artificial Intelligence, and the promotion of diversity and inclusion.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-richard-w-hamming-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE RICHARD W. HAMMING MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.qualcomm.com/home\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Qualcomm, Inc.</em></em></a></p><p><a href=\"https://spectrum.ieee.org/universal-decoder-pioneer\" target=\"_self\">Muriel Médard</a></p><p><a href=\"https://www.mit.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">MIT</a></p><p>“For contributions to coding for reliable communications and networking.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-nick-holonyak-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE NICK HOLONYAK, JR. MEDAL FOR SEMICONDUCTOR OPTOELECTRONIC TECHNOLOGIES</a></h2><p><em><em>Sponsor: Friends of Nick Holonyak, Jr.</em></em></p><p><a href=\"https://ssleec.ucsb.edu/denbaars\" rel=\"noopener noreferrer\" target=\"_blank\">Steven P. DenBaars </a></p><p><a href=\"https://www.ucsb.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of California, Santa Barbara</a></p><p>“For seminal contributions to compound semiconductor optoelectronics, including high-efficiency visible light-emitting diodes, lasers, and LED displays.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-medal-for-innovations-in-healthcare-technology-2/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE MEDAL FOR INNOVATIONS IN HEALTHCARE TECHNOLOGY</a></h2><p><em><em>Sponsor:</em></em><a href=\"https://www.embs.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><em> </em><em><em>IEEE Engineering Medicine and Biology Society</em></em></a></p><p><a href=\"https://www.media.mit.edu/people/picard/overview/\" rel=\"noopener noreferrer\" target=\"_blank\">Rosalind W. Picard </a></p><p><a href=\"https://www.media.mit.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">MIT</a></p><p>“For pioneering contributions to wearable affective computing for health and wellbeing.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/922-2/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE JACK S. KILBY SIGNAL PROCESSING MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.apple.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Apple</em></em></a></p><p><a href=\"https://ece.gatech.edu/directory/biing-hwang-juang\" rel=\"noopener noreferrer\" target=\"_blank\">Biing-Hwang “Fred” Juang</a></p><p><a href=\"https://www.gatech.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Georgia Tech</a></p><p>“For contributions to signal modeling, coding, and recognition for speech communication.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-rse-james-clerk-maxwell-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE/RSE JAMES CLERK MAXWELL MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.arm.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>ARM, Ltd.</em></em></a></p><p><a href=\"https://www.uottawa.ca/faculty-science/professors/paul-corkum\" rel=\"noopener noreferrer\" target=\"_blank\">Paul B. Corkum</a></p><p><a href=\"https://www.uottawa.ca/en\" rel=\"noopener noreferrer\" target=\"_blank\">University of Ottawa</a></p><p>“For the development of the recollision model for strong field light–matter interactions leading to the field of attosecond science.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-james-h-mulligan-jr-education-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE JAMES H. MULLIGAN, JR. EDUCATION MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.ieee.org/communities/life-members/fund.html\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>IEEE Life Members Fund</em></em></a><em><em> and </em></em><a href=\"https://www.mathworks.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>MathWorks<br/></em></em><br/></a><a href=\"https://ece.gatech.edu/directory/james-h-mcclellan\" rel=\"noopener noreferrer\" target=\"_blank\">James H. McClellan</a></p><p><a href=\"https://www.gatech.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Georgia Tech</a></p><p>“For fundamental contributions to electrical and computer engineering education through innovative digital signal processing curriculum development.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-jun-ichi-nishizawa-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE JUN-ICHI NISHIZAWA MEDAL</a></h2><p><em><em>Sponsor: IEEE Jun-ichi Nishizawa Medal Fund</em></em></p><p><a href=\"https://engineering.dartmouth.edu/community/faculty/eric-fossum\" rel=\"noopener noreferrer\" target=\"_blank\">Eric R. Fossum</a></p><p><a href=\"https://home.dartmouth.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Dartmouth College<br/><br/></a>Hanover, N.H.</p><p>“For the invention, development, and commercialization of the CMOS image sensor.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-robert-n-noyce-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE ROBERT N. NOYCE MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.intel.com/content/www/us/en/company-overview/company-overview.html\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Intel Corp.</em></em></a></p><p><a href=\"https://www.nvidia.com/en-eu/about-nvidia/governance/management-team/chris-malachowsky/\" rel=\"noopener noreferrer\" target=\"_blank\">Chris Malachowsky </a></p><p><a href=\"https://www.nvidia.com/en-us/\" rel=\"noopener noreferrer\" target=\"_blank\">Nvidia</a></p><p>Santa Clara, Calif.</p><p>“For pioneering parallel computing architectures and leadership in semiconductor design that transformed artificial intelligence, scientific research, and accelerated computing.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-dennis-j-picard-medal-for-radar-technologies-and-applications/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE DENNIS J. PICARD MEDAL FOR RADAR TECHNOLOGIES AND APPLICATIONS</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.rtx.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>RTX</em></em></a></p><p><a href=\"https://www.gs.niigata-u.ac.jp/~gsweb/gs/english/teacher/pdf/64.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Yoshio Yamaguchi</a></p><p><a href=\"https://www.niigata-u.ac.jp/en/\" rel=\"noopener noreferrer\" target=\"_blank\">Niigata University</a></p><p>Japan</p><p>“For contributions to polarimetric synthetic aperture radar imaging and its utilization.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-medal-in-power-engineering/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE MEDAL IN POWER ENGINEERING</a></h2><p><em><em>Sponsors: </em></em><a href=\"https://ias.ieee.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>IEEE Industry Applications,</em></em></a><em> </em><a href=\"https://www.ieee-ies.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Industrial Electronics,</a><em> </em><a href=\"https://www.ieee-pels.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Power Electronics</em></em></a><em><em>, and </em></em><a href=\"https://www.ieee-pes.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Power & Energy societies</em></em></a></p><p><a href=\"https://grid.pitt.edu/people/fang-zheng-peng\" rel=\"noopener noreferrer\" target=\"_blank\">Fang Zheng Peng </a></p><p><a href=\"https://www.pitt.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of Pittsburgh</a></p><p>“For contributions to Z-Source and modular multi-level converters for distribution and transmission networks.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-simon-ramo-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE SIMON RAMO MEDAL</a></h2><p><em><em>Sponsor: </em></em><a href=\"https://www.northropgrumman.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>Northrop Grumman Corp</em></em></a>.<br/><br/><a href=\"https://www.linkedin.com/in/michael-griffin-8209101b2/\" rel=\"noopener noreferrer\" target=\"_blank\">Michael D. Griffin </a></p><p>LogiQ, Inc.</p><p>Arlington, Va.</p><p>“For leadership in national security, civil, and commercial systems engineering and development of elegant design principles.”</p><div class=\"horizontal-rule\"></div><h2><a href=\"https://corporate-awards.ieee.org/award/ieee-john-von-neumann-medal/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE JOHN VON NEUMANN MEDAL</a></h2><p><em><em>Sponsor:</em></em><a href=\"https://www.research.ibm.com/university/\" rel=\"noopener noreferrer\" target=\"_blank\"><em> </em></a><a href=\"https://www.ibm.com/us-en\" rel=\"noopener noreferrer\" target=\"_blank\"><em><em>IBM</em></em></a></p><p><a href=\"https://www.linkedin.com/in/donaldchamberlin/\" rel=\"noopener noreferrer\" target=\"_blank\">Donald D. Chamberlin</a></p><p><a href=\"https://www.ibm.com/us-en\" rel=\"noopener noreferrer\" target=\"_blank\">IBM</a></p><p>San Jose, Calif.</p><p>“For contributions to database query languages, particularly Structured Query Language, which powers most of the world’s data management and analysis systems.”</p>",
      "author": "Tanya Steinhauser",
      "publishedAt": "2026-02-09T19:00:03.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "03e91f82-e4e9-4164-8e72-0cc26457a9cb",
      "guid": "https://spectrum.ieee.org/ai-and-memory-wall",
      "title": "New Devices Might Scale the Memory Wall",
      "link": "https://spectrum.ieee.org/ai-and-memory-wall",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/scanning-electron-microscope-image-of-the-top-of-a-three-dimensional-bulk-resistive-ram.jpg?id=63929015&width=1245&height=700&coordinates=0%2C97%2C0%2C98\"/><br/><br/><p><span>The hunt is on for anything that can surmount AI’s </span><a href=\"https://spectrum.ieee.org/to-speed-up-ai-mix-memory-and-processing\" target=\"_self\">perennial memory wall</a><span>–even quick models are bogged down by the time and energy needed to carry data between processor and memory. </span><a href=\"https://spectrum.ieee.org/system-creates-the-illusion-of-an-ideal-ai-chip\" target=\"_self\">Resistive RAM</a><span> (RRAM)could circumvent the wall by allowing computation to happen in the memory itself. Unfortunately, most types of this nonvolatile memory are too unstable and unwieldy for that purpose.</span></p><p>Fortunately, a potential solution may be at hand. At December’s IEEE<a href=\"https://www.ieee-iedm.org/\" target=\"_blank\"> International Electron Device Meeting</a> (IEDM), researchers from the University of California, San Diego, showed they could run a learning algorithm on an entirely new type of RRAM.</p><p>“We actually redesigned RRAM, completely rethinking the way it switches,” says<a href=\"https://jacobsschool.ucsd.edu/people/profile/duygu-kuzum\" target=\"_blank\"> Duygu Kuzum</a>, an electrical engineer at UCSD, who led the work.</p><p>RRAM stores data as a level of resistance to the flow of current. The key digital operation in a neural network—multiplying arrays of numbers and then summing the results—can be done in analog simply by running current through an array of RRAM cells, connecting their outputs, and measuring the resulting current.</p><p>Traditionally, RRAM stores data by creating low-resistance filaments in the higher-resistance surrounds of a dielectric material. Forming these filaments often needs voltages too high for standard CMOS, hindering its integration inside processors. Worse, forming the filaments is a noisy and random process, not ideal for storing data. (Imagine a neural network’s weights randomly drifting. Answers to the same question would change from one day to the next.) </p><p>Moreover, most filament-based RRAM cells’ noisy nature means they must be isolated from their surrounding circuits, usually with a selector transistor, which makes <a href=\"https://spectrum.ieee.org/3d-cmos\" target=\"_self\">3D stacking</a> difficult.</p><p>Limitations like these mean that traditional RRAM isn’t great for computing. In particular, Kuzum says, it’s difficult to use filamentary RRAM for the sort of parallel<a href=\"https://spectrum.ieee.org/matrix-multiplication-deepmind\" target=\"_self\"> matrix operations</a> that are crucial for today’s neural networks.</p><p>So, the UCSD researchers decided to dispense with the filaments entirely. Instead they developed devices that switch an entire layer from high to low resistance and back again. This format, called bulk RRAM, can do away with both the annoying high-voltage filament-forming step and the geometry-limiting selector transistor.</p><h2>3D Memory for Machine Learning</h2><p>The UCSD group wasn’t the first to build bulk RRAM devices, but it made breakthroughs both in shrinking them and forming 3D circuits with them. Kuzum and her colleagues shrank RRAM into the nanoscale; their device was just 40 nanometers across. They also managed to stack bulk RRAM into as many as eight layers.</p><p>With a single pulse of voltage, <span>each cell in </span>an eight-layer stack can take any of 64 resistance values, a number that’s very difficult to achieve with traditional filamentous RRAM. And whereas the resistance of most filament-based cells are limited to kiloohms, the UCSD stack is in the megaohm range, which Kuzum says is better for parallel operations.</p><p>“We can actually tune it to anywhere we want, but we think that from an integration and system-level simulations perspective, megaohm is the desirable range,” Kuzum says.</p><p>These two benefits–a greater number of resistance levels and a higher resistance–could allow this bulk RRAM stack to perform more complex operations than traditional RRAM’s can manage. </p><p>Kuzum and colleagues assembled multiple eight-layer stacks into a 1-kilobyte array that required no selectors. Then, they tested the array with a continual learning algorithm: making the chip classify data from wearable sensors while constantly adding new data. For example, data read from a waist-mounted smartphone might be used to determine if its wearer was sitting, walking, climbing stairs, or taking another action. Tests showed an accuracy of 90 percent, which the researchers say is comparable to the performance of a digitally implemented neural network.</p><p>This test exemplifies what Kuzum thinks can especially benefit from bulk RRAM: neural network models on edge devices, which may need to learn from their environment without accessing the cloud. </p><p>“We are doing a lot of characterization and material optimization to design a device specifically engineered for AI applications,” Kuzum says. </p><p>The ability to integrate RRAM into an array like this is a significant advance, says <a href=\"https://mse.umd.edu/clark/faculty/980/A-Alec-Talin\" target=\"_blank\">Alec Talin</a>, materials scientist at Sandia National Laboratories in Livermore, California, and a bulk RRAM researcher who wasn’t involved in the UCSD group’s work. “I think that any step in terms of integration is very useful,” he says.</p><p>But Talin highlights a potential obstacle: the ability to retain data for an extended period of time. While the UCSD group showed their RRAM could retain data at room temperature for several years (on par with flash memory), Talin says that its retention at the higher temperatures where computers actually operate is less certain. “That’s one of the major challenges of this technology,” he says, especially when it comes to edge applications.</p><p>If engineers can prove the technology, then all types of models may benefit. This memory wall has only grown higher this decade, as traditional memory hasn’t been able to keep up with the ballooning demands of large models. Anything that allows models to operate on the memory itself could be a welcome shortcut. </p>",
      "author": "Rahul Rao",
      "publishedAt": "2026-02-09T13:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "6971d83c-4ccb-4e29-acff-805cec8abc16",
      "guid": "https://spectrum.ieee.org/3d-modeling-blind-programmers",
      "title": "Low-Vision Programmers Can Now Design 3D Models Independently",
      "link": "https://spectrum.ieee.org/3d-modeling-blind-programmers",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/a-college-student-programming-a-three-dimensional-model-on-a-laptop.jpg?id=63136292&width=1245&height=700&coordinates=0%2C469%2C0%2C469\"/><br/><br/><p>Most 3D design software requires visual dragging and rotating—posing a challenge for blind and low-vision users. As a result, a range of hardware design, robotics, coding, and engineering work is <span>inaccessible to interested programmers. A visually-impaired programmer might write great code. But because of the lack of accessible <a data-linked-post=\"2671899889\" href=\"https://spectrum.ieee.org/comsol-simulation-apps\" target=\"_blank\">modeling software</a>, the coder can’t model, design, and verify physical and virtual components of their system. </span></p><p>However, new 3D modeling tools are beginning to change this equation. A new prototype program called <a href=\"https://arxiv.org/abs/2508.03852\" target=\"_blank\">A11yShape</a> aims to close the gap. There are already code-based tools that let users describe 3D models in text, such as the popular <a href=\"https://openscad.org/\" target=\"_blank\">OpenSCAD software</a>. Other recent <a href=\"https://github.com/WebPAI/DesignBench\" target=\"_blank\">large-language-model tools</a> generate <a href=\"https://arxiv.org/html/2410.05340v1\" target=\"_blank\">3D code from natural-language prompts</a>. But even with these, blind and low-vision programmers still depend on sighted feedback to bridge the gap between their code and its visual output. </p><p>Blind and low-vision programmers previously had to rely on a sighted person to visually check every update of a model to describe what changed. But with A11yShape, blind and low-vision programmers can independently create, inspect, and refine 3D models without relying on sighted peers.</p><p>A11yShape does this by generating accessible model descriptions, organizing the model into a semantic hierarchy, and ensuring every step works with screen readers<span>. </span></p><p>The project began when <a href=\"https://www.lianghe.me/\" target=\"_blank\"><span>Liang He</span></a>, assistant professor of computer science at the University of Texas at Dallas, spoke with his low-vision classmate who was studying 3D modeling. He saw an opportunity to turn his classmate’s coding strategies, learned in <a href=\"https://create.uw.edu/initiatives/physical-computing/\" target=\"_blank\"><span>a 3D modeling for blind programmers course</span></a> at the University of Washington, into a streamlined tool. </p><p>“I want to design something useful and practical for the group,” he says. “Not just something I created from my imagination and applied to the group.” </p><h3>Re-imagining Assistive 3D Design With OpenSCAD</h3><p>A11yShape assumes the user is running OpenSCAD, the script-based 3D modeling editor. <span>The program adds OpenSCAD features to connect each component of modeling across three application UI panels. </span></p><p>OpenSCAD allows users to create models entirely through typing, eliminating the need for clicking and dragging. Other common graphics-based user interfaces are difficult for blind programmers to navigate. </p><p>A11yshape introduces an AI Assistance Panel, where users can submit real-time queries to <a data-linked-post=\"2668670173\" href=\"https://spectrum.ieee.org/chatgpt-for-coding\" target=\"_blank\">ChatGPT</a>-4o to validate design decisions and debug existing OpenSCAD scripts. </p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"AllyShape's 3-D modeling web interface, featuring a code editor panel with programming capabilities, an AI assistance panel providing contextual feedback, and a model panel displaying hierarchical structure and rendering of the resulting model.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"e8f4867cfdc27299feb3e351fc191485\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"c4ecb\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/allyshape-s-3-d-modeling-web-interface-featuring-a-code-editor-panel-with-programming-capabilities-an-ai-assistance-panel-prov.jpg?id=63138638&width=980\"/> <small class=\"image-media media-caption\" data-gramm=\"false\" data-lt-tmp-id=\"lt-661548\" placeholder=\"Add Photo Caption...\" spellcheck=\"false\">A11yShape’s three panels synchronize code, AI descriptions, and model structure so blind programmers can discover how code changes affect designs independently.</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\"><a href=\"https://arxiv.org/pdf/2508.03852\" target=\"_blank\">Anhong Guo, Liang He, et al.</a></small></p><p><span>If a user selects a piece of code or a model component, A11yShape highlights the matching part across all three panels and updates the description, so blind and low-vision users always know what they’re working on.</span></p><h3>User Feedback Improved Accessible Interface</h3><p>The research team recruited 4 participants with a range of visual impairments and programming backgrounds. The team asked the participants to design models using A11yShape and observed their workflows.</p><p>One participant, who had never modeled before, said the tool “provided [the blind and low-vision community] with a new perspective on 3D modeling, demonstrating that we can indeed create relatively simple structures.”</p><p>Participants also reported that long text descriptions still make it hard to grasp complex shapes, and several said that without eventually touching a physical model or using a tactile display, it was difficult to fully “see” the design in their mind.</p><p>To evaluate the accuracy of the AI-generated descriptions, the research team recruited 15 sighted participants. “On a 1–5 scale, the descriptions earned average scores between about 4.1 and 5 for geometric accuracy, clarity, and avoiding hallucinations, suggesting the AI is reliable enough for everyday use.”</p><p><br/></p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"A failed all-at-once attempt to construct a 3-D helicopter shows incorrect shapes and placement of elements. In contrast, when the user journey allows for completion of each individual element before moving forward, results significantly improve.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"889449ab44920bd0cddc01deaea32916\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"528e8\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-failed-all-at-once-attempt-to-construct-a-3-d-helicopter-shows-incorrect-shapes-and-placement-of-elements-in-contrast-when-t.jpg?id=63138939&width=980\"/> <small class=\"image-media media-caption\" data-gramm=\"false\" data-lt-tmp-id=\"lt-698807\" placeholder=\"Add Photo Caption...\" spellcheck=\"false\">A new assistive program for blind and low-vision programmers, A11yShape, assists visually disabled programmers in verifying the design of their models.</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Source: <a href=\"https://arxiv.org/pdf/2508.03852\" target=\"_blank\">Anhong Guo, Liang He, et al.</a></small></p><p>The feedback will help to inform future iterations—which He says could integrate tactile displays, real-time 3D printing, and more concise AI-generated audio descriptions. </p><p>Beyond its applications in the professional computer programming community, He noted that A11yShape also lowers the barrier to entry for blind and low-vision computer programming <span>learners.</span></p><p><span>“People like being able to express themselves in creative ways. . . using technology such as 3D printing to make things for utility or entertainment,” says <a href=\"https://engineering.unt.edu/people/stephanie-ludi.html\" target=\"_blank\">Stephanie Ludi,</a> director of DiscoverABILITY Lab and professor of the department of computer science and engineering at the <a href=\"https://engineering.unt.edu/cse/\" target=\"_blank\">University of North Texas</a>. “Persons who are blind and visually impaired share that interest</span><span>, with A11yShape serving as a model to support accessibility in the maker community.” </span></p><p>The team presented A11yshape in October at the <a href=\"https://assets25.sigaccess.org/\" target=\"_blank\">ASSETS conference</a> in Denver.</p>",
      "author": "Samantha Hurley",
      "publishedAt": "2026-02-07T14:00:01.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "83410430-125a-47ce-8f2d-752a9c54d2ce",
      "guid": "https://spectrum.ieee.org/ieee-online-mini-ai-mba",
      "title": "IEEE Online Mini-MBA Aims to Fill Leadership Skills Gaps in AI",
      "link": "https://spectrum.ieee.org/ieee-online-mini-ai-mba",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/close-up-of-hands-typing-on-a-laptop-with-floating-graphics-representing-large-language-models-floating-above-the-keyboard.jpg?id=63843722&width=1245&height=700&coordinates=0%2C156%2C0%2C157\"/><br/><br/><p>Boardroom priorities are shifting from financial metrics toward technical oversight. Although market share and operational efficiency remain business bedrocks, executives also must now manage the complexities of machine learning, the integrity of their data systems, and the risks of algorithmic bias.</p><p>The change represents more than just a tech update; it marks a fundamental redefinition of the skills required for business leadership.</p><p><a href=\"https://www.mckinsey.com/capabilities/operations/our-insights/bold-accelerators-how-operations-leaders-are-pulling-ahead-using-ai\" rel=\"noopener noreferrer\" target=\"_blank\">Research</a> from the <a href=\"https://www.mckinsey.com/mgi/about-us\" rel=\"noopener noreferrer\" target=\"_blank\">McKinsey Global Institute</a> on the economic impact of artificial intelligence shows that companies integrating it effectively have boosted profit margins by up to 15 percent. Yet the same study revealed a sobering reality: 87 percent of organizations acknowledge significant AI skill gaps in their leadership ranks.</p><p>That disconnect between AI’s business potential and executive readiness has created a need for a new type of professional education.</p><h2>The leadership skills gap in the AI era</h2><p>Traditional business education, with its focus on finance, marketing, and operations, wasn’t designed for an AI-driven economy. Today’s leaders need to understand not just what AI can do but also how to evaluate investments in the technology, manage algorithmic risks, and lead teams through digital transformations.</p><p>The challenges extend beyond the executive suite. Middle managers, project leaders, and department heads across industries are discovering that <a href=\"https://spectrum.ieee.org/ai-developer-career-advice\" target=\"_self\">AI fluency has become essential for career advancement</a>. In 2020 the <a href=\"https://www.weforum.org/stories/2020/10/top-10-work-skills-of-tomorrow-how-long-it-takes-to-learn-them/#:~:text=50%25%20of%20all%20employees%20will,help%20us%20learn%20new%20skills.\" rel=\"noopener noreferrer\" target=\"_blank\">World Economic Forum</a> predicted that 50 percent of all employees would need reskilling by 2025, with <a href=\"https://spectrum.ieee.org/ai-effect-entry-level-jobs\" target=\"_self\">AI-related competencies topping the list of required skills</a>.</p><h2>IEEE | Rutgers Online Mini-MBA: Artificial Intelligence</h2><p>Recognizing the skills gap, IEEE partnered with the <a href=\"https://www.business.rutgers.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Rutgers Business School</a> to offer a comprehensive business education program designed for the new era of AI. The<a href=\"https://innovationatwork.ieee.org/professional-development/rutgers-online-mini-mba-artificial-intelligence/\" rel=\"noopener noreferrer\" target=\"_blank\"> IEEE | Rutgers Online Mini-MBA: Artificial Intelligence</a> program combines rigorous business strategy with deep AI literacy.</p><p>Rather than treating AI as a separate technical subject, the program incorporates it into each aspect of business strategy. Students learn to evaluate AI opportunities through financial modeling, assess algorithmic risks through governance frameworks, and use change-management principles to implement new technologies.</p><h2>A curriculum built for real-world impact</h2><p>The program’s modular structure lets professionals focus on areas relevant to their immediate needs while building toward comprehensive AI business literacy. Each of the 10 modules includes practical exercises and case study analyses that participants can immediately apply in their organization.</p><p>The Introduction to AI module provides a comprehensive overview of the technology’s capabilities, benefits, and challenges. Other technologies are covered as well, including how they can be applied across diverse business contexts, laying the groundwork for informed decision‑making and strategic adoption.</p><p class=\"pull-quote\">Rather than treating AI as a separate technical subject, the online mini-MBA program incorporates the technology throughout each aspect of business strategy.</p><p>Building on that foundation, the Data Analytics module highlights how AI projects differ from traditional programming, how to assess data readiness, and how to optimize data to improve accuracy and outcomes. The module can equip leaders to evaluate whether their organization is prepared to launch successful AI initiatives.</p><p>The Process Optimization module focuses on reimagining core organizational workflows using AI. Students learn how machine learning and automation are already transforming industries such as manufacturing, distribution, transportation, and health care. They also learn how to identify critical processes, create AI road maps, establish pilot programs, and prepare their organization for change.</p><h2>Industry-specific applications</h2><p>The core modules are designed for all participants, and the program highlights how AI is applied across industries. By analyzing case studies in fraud detection, medical diagnostics, and predictive maintenance, participants see underlying principles in action.</p><p>Participants gain a broader perspective on how AI can be adapted to different contexts so they can draw connections to the opportunities and challenges in their organization. The approach ensures everyone comes away with a strong foundation and the ability to apply learned lessons to their environment.</p><h2>Flexible learning for busy professionals</h2><p>With the understanding that senior professionals have demanding schedules, the mini-MBA program offers flexibility. The online format lets participants engage with content in their own time frame, while live virtual office hours with faculty provide opportunities for real-time interaction.</p><p>The program, which offers discounts to IEEE members and flexible payment options, qualifies for many tuition reimbursement programs.</p><p>Graduates report that implementing AI strategies developed during the program has helped drive tangible business results. This success often translates into career advancement, including promotions and expanded leadership roles. Furthermore, the curriculum empowers graduates to confidently vet AI vendor proposals, lead AI project teams, and navigate high-stakes investment decisions.</p><p>Beyond curriculum content, the mini MBA can create valuable professional networks among AI-forward business leaders. Participants collaborate on projects, share implementation experiences, and build relationships that extend beyond the program’s 12 weeks.</p><h2>Specialized training from IEEE</h2><p>To complement the mini-MBA program, IEEE offers targeted courses addressing specific AI applications in critical industries. The <a href=\"https://iln.ieee.org/public/contentdetails.aspx?id=D02A42B64A834CC1A698ADC1ABAB9523\" target=\"_blank\">Artificial Intelligence and Machine Learning in Chip Design</a> course explores how the technology is revolutionizing semiconductor development. <a href=\"https://iln.ieee.org/public/contentdetails.aspx?id=706DBC956996482182A5232D95410F99\" rel=\"noopener noreferrer\" target=\"_blank\">Integrating Edge AI and Advanced Nanotechnology in Semiconductor Applications</a> delves into cutting-edge hardware implementations. The <a href=\"https://iln.ieee.org/public/contentdetails.aspx?id=4B2AD26097B84B0485D297CE627ECA1E\" rel=\"noopener noreferrer\" target=\"_blank\">Mastering AI Integration in Semiconductor Manufacturing</a> course examines how AI enhances production efficiency and quality control in one of the world’s most complex manufacturing processes. <a href=\"https://iln.ieee.org/public/contentdetails.aspx?id=64D2FDC20BF947BB89045A26DAF3A191\" rel=\"noopener noreferrer\" target=\"_blank\">AI in Semiconductor Packaging</a> equips professionals to apply machine learning and neural networks to modernize semiconductor packaging reliability and performance.</p><p>The programs grant professional development credits including PDHs and CEUs, ensuring participants receive formal recognition for their educational investments. Digital badges provide shareable credentials that professionals can showcase across professional networks, demonstrating their AI competencies to current and prospective employers.</p><p>Learn more about <a href=\"https://ea.ieee.org/ea-programs\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Educational Activities</a>’ corporate solutions and professional development programs at <a href=\"https://innovationatwork.ieee.org\" rel=\"noopener noreferrer\" target=\"_blank\">innovationatwork.ieee.org</a>.</p>",
      "author": "Angelique Parashis",
      "publishedAt": "2026-02-06T19:00:03.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "cc27bf28-8dc0-4a26-8baf-2ce19bf70671",
      "guid": "https://spectrum.ieee.org/autonomous-warehouse-robots",
      "title": "Video Friday: Autonomous Robots Learn By Doing in This Factory",
      "link": "https://spectrum.ieee.org/autonomous-warehouse-robots",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/robotic-arms-on-mobile-bases-sort-crates-on-a-conveyor-belt-in-a-warehouse.png?id=63907821&width=1245&height=700&coordinates=0%2C55%2C0%2C55\"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href=\"mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a><span> for inclusion.</span></p><h5><a href=\"https://2026.ieee-icra.org/\">ICRA 2026</a>: 1–5 June 2026, VIENNA</h5><p>Enjoy today’s videos!</p><div class=\"horizontal-rule\"></div><div style=\"page-break-after: always\"><span style=\"display:none\"> </span></div><blockquote class=\"rm-anchors\" id=\"qdwi4cn3oi0\"><em>To train the next generation of <a data-linked-post=\"2650273449\" href=\"https://spectrum.ieee.org/toyota-to-invest-1-billion-in-ai-and-robotics-rd\" target=\"_blank\">autonomous robots</a>, scientists at Toyota Research Institute are working with Toyota Manufacturing to deploy them on the factory floor.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"197161b054a2a3e4ef30ccd9b27cb5b5\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/QDwi4CN3OI0?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.linkedin.com/posts/toyota-research-institute_whats-next-for-tri-robotics-max-bajracharya-activity-7424198589196685313-4e92?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAM4nT0BW_DvaXaoyr7IuJL-to9SJ5MlYT4\">Toyota Research Institute</a> ]</p><p>Thanks, Erin!</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"sh0chr6usao\"><em>This is just one story (of many) about how we tried, failed, and learned how to improve our ‪<a data-linked-post=\"2650278428\" href=\"https://spectrum.ieee.org/in-the-air-with-ziplines-medical-delivery-drones\" target=\"_blank\">drone delivery</a> system.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"be361c20896c763261fbae4500176505\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/sH0cHr6USao?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>Okay, but like you didn’t show the really cool bit...?</p><p>[ <a href=\"https://www.zipline.com/\">Zipline</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"y2dhzlpgdwy\"><em>We’re introducing KinetIQ, an AI framework developed by Humanoid, for end-to-end orchestration of humanoid robot fleets. KinetIQ coordinates wheeled and bipedal robots within a single system, managing both fleet-level operations and individual robot behavior across multiple environments. The framework operates across four cognitive layers, from task allocation and workflow optimization to task execution based on Vision-Language-Action models and whole-body control taught by reinforcement learning, and is shown here running across our wheeled industrial robots and bipedal R&D platform.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"e2c953a1800d81c0e0b0040c519e3979\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/Y2DhzLPGdwY?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://thehumanoid.ai/\">Humanoid</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"bp7esfyyv4g\"><em>What if a robot gets damaged during operation? Can it still perform its mission without immediate repair? Inspired by the self-embodied resilience strategies of stick insects, we developed a decentralized adaptive resilient neural control system (DARCON). This system allows legged robots to autonomously adapt to limb loss, ensuring mission success despite mechanical failure. This innovative approach leads to a future of truly resilient, self-recovering robotics.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"453cb36391a89fdaf9b4559aec7fe3c9\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/Bp7esFyYV4g?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://advanced.onlinelibrary.wiley.com/doi/10.1002/aisy.202500270\">VISTEC</a> ]</p><p>Thanks, Poramate!</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"lo2gluku4c8\"><em>This animation shows Perseverance’s point of view during a drive of 807 feet (246 meters) along the rim of Jezero Crater on 10 December 2025, the 1,709th Martian day, or sol, of the mission. Captured over 2 hours and 35 minutes, 53 navigation-camera (Navcam) image pairs were combined with rover data on orientation, wheel speed, and steering angle, as well as data from Perseverance’s inertial measurement unit, and placed into a 3D virtual environment. The result is this reconstruction with virtual frames inserted about every 4 inches (0.1 meters) of drive progress.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"87c1f3217435e9b3f4931fa59fa64683\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/LO2GluKu4C8?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://science.nasa.gov/mission/mars-2020-perseverance/\">NASA Jet Propulsion Lab</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"sx4wkuhap4e\"><em>−47.4 °C, 130,000 steps, 89.75°E, 47.21°N… On the extremely cold snowfields of Altay, the birthplace of human skiing, Unitree’s humanoid robot G1 left behind a unique set of marks.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"c7485e5fd02cbfca05ec835a4a3e766c\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/SX4WKUHAP4E?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.unitree.com/\">Unitree</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"as_ouaft2he\"><em>Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a vision language model (VLM) to infer semantic relationships. Notably, we introduce a task-reasoning module that combines large language models and a VLM to interpret the scene graph’s semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"768f3e1223995648c873b29ea0bac5b3\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/as_oUaFT2hE?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://ntnu-arl.github.io/reasoning_graph/\">Norwegian University of Science & Technology, Autonomous Robots Lab</a> ]</p><p>Thanks, Kostas!</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"hmfgfp9xohq\"><em>We present HoLoArm, a quadrotor with compliant arms inspired by the nodus structure of dragonfly wings. This design provides natural flexibility and resilience while preserving flight stability, which is further reinforced by the integration of a reinforcement-learning control policy that enhances both recovery and hovering performance.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"5096413bf8582da71f43303183472528\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/hmfgFP9XoHQ?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://ieeexplore.ieee.org/abstract/document/11361075\">HO Lab via IEEE Robotics and Automation Letters</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"giijs_mmmrg\"><em>In this work, we present SkyDreamer, to the best of our knowledge the first end-to-end vision-based autonomous-drone racing policy that maps directly from pixel-level representations to motor commands.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"90c32886c61319893e1ba59f4bcf677f\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/GiIjs_MmMrg?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://arxiv.org/pdf/2510.14783\">MAVLab</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"gr867dgh5tk\"><em>This video showcases AI Worker, equipped with five-finger hands, performing dexterous object manipulation across diverse environments. Through teleoperation, the robot demonstrates precise, humanlike hand control in a variety of manipulation tasks.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"126ba42a9ad07785c5c33eae1738c225\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/Gr867DGH5tk?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://ai.robotis.com/hands/introduction_hands.html\">Robotis</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"phomadn-qze\"><em>Autonomous following, 45-degree slope climbing, and reliable payload transport in extreme winter conditions, built to support operations where environments push the limits.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"90a3ca6f001914b8f44f854ed188eb0a\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/pHOmadN-qzE?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.deeprobotics.cn/en\">DEEP Robotics</a> ]</p><div class=\"horizontal-rule\"></div><blockquote class=\"rm-anchors\" id=\"80qqrfmvir0\"><em>Living architectures, from plants to beehives, adapt continuously to their environments through self-organization. In this work, we introduce the concept of architectural swarms: systems that integrate swarm robotics into modular architectural façades. The Swarm Garden exemplifies how architectural swarms can transform the built environment, enabling “living-like” architecture for functional and creative applications.</em></blockquote><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"3324e87c4f923abd04c61086c8018795\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/80QqrFmvIr0?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p>[ <a href=\"https://www.science.org/doi/10.1126/scirobotics.ady7233\">SSR Lab via Science Robotics</a> ]</p><div class=\"horizontal-rule\"></div><p class=\"rm-anchors\" id=\"wxtmqieul0s\">Here are a couple of IROS 2025 keynotes, featuring Bram Vanderborght and Kyu-Jin Cho.</p><p class=\"shortcode-media shortcode-media-youtube\"><span class=\"rm-shortcode\" data-rm-shortcode-id=\"1589c01bc2c7e94f846550cf18fa08c3\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/WXtMQIeUl0s?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span></p><p><br/></p><p class=\"shortcode-media shortcode-media-youtube\"> <span class=\"rm-shortcode\" data-rm-shortcode-id=\"99ea91bbf11816455fe9e3668e4f0c4e\" style=\"display:block;position:relative;padding-top:56.25%;\"><iframe frameborder=\"0\" height=\"auto\" lazy-loadable=\"true\" scrolling=\"no\" src=\"https://www.youtube.com/embed/j6fEnhU56aA?rel=0\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" width=\"100%\"></iframe></span> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">- YouTube</small> <small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\"> <a href=\"https://www.youtube.com/watch?v=j6fEnhU56aA\" target=\"_blank\">www.youtube.com</a> </small> </p><p>[ <a href=\"https://www.iros25.org/\">IROS 2025</a> ]</p><div class=\"horizontal-rule\"></div>",
      "author": "Evan Ackerman",
      "publishedAt": "2026-02-06T17:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "1df96f79-1dac-413a-a5ce-ec681882a6ae",
      "guid": "https://spectrum.ieee.org/quantum-twins",
      "title": "“Quantum Twins” Simulate What Supercomputers Can’t",
      "link": "https://spectrum.ieee.org/quantum-twins",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/three-dimensional-rendering-of-12-sided-polyhedrons-lined-up-in-long-rows-each-polyhedron-is-comprised-of-countless-small-spher.jpg?id=63831528&width=1245&height=700&coordinates=0%2C176%2C0%2C177\"/><br/><br/><p>While quantum computers continue to <a href=\"https://spectrum.ieee.org/neutral-atom-quantum-computing\" target=\"_self\">slowly grind</a> toward usefulness, some are pursuing a different approach—analog <a href=\"https://spectrum.ieee.org/quantum-simulation\" target=\"_self\">quantum simulation</a>. This path doesn’t offer complete control of single bits of quantum information, known as qubits—it is not a universal quantum computer. Instead, quantum simulators directly mimic complex, difficult-to-access things, like individual molecules, chemical reactions, or novel materials. What analog quantum simulation lacks in flexibility, it makes up for in feasibility: quantum simulators are ready now.</p><p>“Instead of using qubits, as you would typically in a quantum computer, we just directly encode the problem into the geometry and structure of the array itself,” says <a href=\"https://www.linkedin.com/in/sam-gorman-b53389243/?originalSubdomain=au\" rel=\"noopener noreferrer\" target=\"_blank\">Sam Gorman</a>, quantum systems engineering lead at Sydney-based startup <a href=\"https://www.sqc.com.au/\" rel=\"noopener noreferrer\" target=\"_blank\">Silicon Quantum Computing</a>.</p><p>Yesterday, Silicon Quantum Computing unveiled its Quantum Twins product, a silicon quantum simulator, which is now available to customers through direct contract. Simultaneously, the team demonstrated that their device, made up of 15,000 quantum dots, can simulate an often-studied transition of a material from an insulator to a metal, and all the states between. They <a href=\"https://www.nature.com/articles/s41586-025-10053-7\" rel=\"noopener noreferrer\" target=\"_blank\">published</a> their work this week in the journal <em><em>Nature</em></em>.</p><p>“We can do things now that we think nobody else in the world can do,” Gorman says. </p><h2>The Powerful Process</h2><p>Though the product announcement came yesterday, the team at Silicon Quantum Computing established its Precision Atom Qubit Manufacturing process following the startup’s establishment in 2017, building on the academic work that the company’s founder, <a href=\"https://en.wikipedia.org/wiki/Michelle_Simmons\" rel=\"noopener noreferrer\" target=\"_blank\">Michelle Simmons</a>, led for over 25 years. The underlying technology is a manufacturing process for placing single phosphorus atoms in silicon with subnanometer precision.</p><p>“We have a 38-stage process,” Simmons says, for patterning phosphorus atoms into silicon. The process starts with a silicon substrate, which gets coated with a layer of hydrogen. Then, by means of a scanning-tunneling microscope, individual hydrogen atoms are knocked off the surface, exposing the silicon underneath. The surface is then dosed with phosphine gas, which adsorbs to the surface only in places where the silicon is exposed. With the help of a low-temperature thermal anneal, the phosphorus atom is then incorporated into the silicon crystal. Then, layers of silicon are grown on top.</p><p>“It’s done in ultrahigh vacuum. So it’s a very pure, very clean system,” Simmons says. “It’s a fully monolithic chip that we make with that subnanometer precision. In 2014, we figured out how to make markers in the chip so that we can then come back and find where we put the atoms within the device to make contacts. Those contacts are then made at the same length scale as the atoms and dots.”</p><p>Though the team is able to place single atoms of phosphorus, they use clusters of 10 to 50 such atoms to make up what’s known as a register for these application-specific chips. These registers act like <a href=\"https://spectrum.ieee.org/what-the-heck-are-quantum-dots\" target=\"_blank\">quantum dots</a>, preserving quantum properties of the individual atoms. The registers are controlled by a gate voltage from contacts placed atop the chip, and interactions between registers can be tuned by precisely controlling the distances between them.</p><p>While the company is also <a href=\"https://www.nature.com/articles/s41565-024-01853-5\" rel=\"noopener noreferrer\" target=\"_blank\">pursuing</a> more traditional quantum computing using this technology, they realized they already had the capacity to do useful simulations in the analog domain by putting thousands of registers on a single chip and measuring global properties, without controlling individual qubits.</p><p>“The thing that’s quite unique is we can do that very quickly,” Simmons says. “We put 250,000 of these registers [on a chip] in 8 hours, and we can turn a chip design around in a week.”</p><h2>What to Simulate</h2><p>Back in 2022, the team at Silicon Quantum Computing used a previous version of this same technology to <a href=\"https://www.nature.com/articles/s41586-022-04706-0\" rel=\"noopener noreferrer\" target=\"_blank\">simulate</a> a molecule of polyacetylene. The chemical is made up of carbon atoms with alternating single and double bonds, and, crucially, its conductivity changes drastically depending on whether the chain is cut on a single or double bond. In order to accurately simulate single and double carbon bonds, the team had to control the distances of their registers to subnanometer precision. By tuning the gate voltages of each quantum dot, the researchers reproduced the jump in conductivity.</p><p>Now, they’ve demonstrated the quantum twin technology on a much larger problem—the <a href=\"https://en.wikipedia.org/wiki/Mott_insulator\" rel=\"noopener noreferrer\" target=\"_blank\">metal-insulator transition</a> of a two-dimensional material. Where the polyacetylene molecule required 10 registers, the new model used 15,000. The metal-insulator model is important because, in most cases, it cannot be simulated on a classical computer. At the extremes—in the fully metal or fully insulating phase—the physics can be simplified and made accessible to classical computing. But in the murky intermediate regime, the full quantum complexity of each electron plays a role, and the problem is classically intractable. “That is the part which is challenging for classical computing. But we can actually put our system into this regime quite easily,” Gorman says.</p><p>The metal-insulator model was a proof of concept. Now, Gorman says, the team can design a quantum twin for almost any two-dimensional problem.</p><p>“Now that we’ve demonstrated that the device is behaving as we predict, we’re looking at high-impact issues or outstanding problems,” says Gorman. The team plans to investigate things like unconventional superconductivity, the origins of magnetism, and materials interfaces such as those that occur in batteries. </p><p>Although the initial applications will most likely be in the scientific domain, Simmons is hopeful that Quantum Twins will eventually be useful for industrial applications such as drug discovery. “If you look at different drugs, they’re actually very similar to polyacetylene. They’re carbon chains, and they have functional groups. So, understanding how to map it [onto our simulator] is a unique challenge. But that’s definitely an area we’re going to focus on,” she says. “We’re excited at the potential possibilities.”</p>",
      "author": "Dina Genkina",
      "publishedAt": "2026-02-05T16:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "f6c4172d-97b3-4918-ba02-9425a41c9d83",
      "guid": "https://spectrum.ieee.org/tribute-finite-element-field-computation-pioneer",
      "title": "Paying Tribute to Finite Element Field Computation Pioneer",
      "link": "https://spectrum.ieee.org/tribute-finite-element-field-computation-pioneer",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/smiling-portrait-of-mvk-chari-wearing-a-tweed-jacket-and-tie.jpg?id=63831506&width=1245&height=700&coordinates=0%2C187%2C0%2C188\"/><br/><br/><p><a href=\"https://www.legacy.com/us/obituaries/name/madabushi-krishnama-chari-obituary?id=60210616\" rel=\"noopener noreferrer\" target=\"_blank\">MVK Chari</a>,<strong> </strong>a pioneer in finite element field computation, died on 3 December. The IEEE Life Fellow was 97.</p><p>Chari developed a finite element method (FEM) for analyzing nonlinear electromagnetic fields—which is crucial for the design of electric machines. The technique is used to obtain approximate solutions to complex engineering and mathematical problems. It involves dividing a complicated object or system into smaller, more manageable parts, known as <em><em>finite elements</em></em>, according to <a href=\"https://www.fictiv.com/articles/understanding-the-finite-element-method#:~:text=The%20Finite%20Element%20Method%20(FEM,behavior%20of%20these%20individual%20elements.\" rel=\"noopener noreferrer\" target=\"_blank\">Fictiv</a>.</p><p>As an engineer and technical leader at <a href=\"https://www.ge.com/about-us\" rel=\"noopener noreferrer\" target=\"_blank\">General Electric</a> in Niskayuna, N.Y., Chari used the tool to analyze large turbogenerators for end region analysis, starting with 2D and expanding its use over time to quasi-2D and 3D.</p><p>During his 25 years at GE, he established a team that was developing finite element analysis (FEA) tools for a variety of applications across the company. They ranged from small motors to large <a href=\"https://spectrum.ieee.org/mri\" target=\"_self\">MRI magnets</a>.</p><p>Chari received the 1993 <a href=\"https://corporate-awards.ieee.org/award/ieee-nikola-tesla-award/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Nikola Tesla Award</a> for “pioneering contributions to finite element computations of nonlinear electromagnetic fields for design and analysis of electric machinery.”</p><h2>A career spanning industry and academia</h2><p>Chari attended <a href=\"https://www.imperial.ac.uk/\" rel=\"noopener noreferrer\" target=\"_blank\">Imperial College London</a> to pursue a master’s degree in electrical engineering. There he met <a href=\"https://en.wikipedia.org/wiki/Peter_P._Silvester\" rel=\"noopener noreferrer\" target=\"_blank\">Peter P. Silvester</a>, a visiting professor of electrical engineering. Silvester, a professor at <a href=\"https://www.mcgill.ca/\" rel=\"noopener noreferrer\" target=\"_blank\">McGill University</a> in Montreal, was a pioneer in understanding numerical analysis of electromagnetic fields.</p><p>After Chari graduated in 1968, he joined Silvester at McGill as a doctoral student, applying FEM to solve electromagnetic field problems. Silvester applied the method to waveguides, while Chari applied it to saturated magnetic fields. </p><p>Chari joined GE in 1970 after earning his Ph.D. in electrical engineering. He climbed the leadership ladder and was a manager of the company’s electromagnetics division when he left in 1995. He joined <a href=\"https://www.rpi.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Rensselaer Polytechnic Institute</a> in Troy, N.Y., as a visiting research and adjunct professor in its electrical, computer, and systems engineering department. Chari taught graduate and undergraduate classes in electric power engineering and <a href=\"https://spectrum.ieee.org/advice-leading-mentoring-greater-innovation\" target=\"_self\">mentored</a> many master’s and doctoral students. His strength was nurturing young engineers. </p><p>He also conducted research on electric machines and transformers for the <a href=\"https://www.epri.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Electric Power Research Institute</a> and the U.S. <a href=\"https://www.energy.gov/\" rel=\"noopener noreferrer\" target=\"_blank\">Department of Energy</a>. </p><p> In 2008 Chari joined <a href=\"https://www.ndt.org/vendor.asp?ObjectID=21373\" rel=\"noopener noreferrer\" target=\"_blank\">Magsoft Corp.</a>, in Clifton Park, N.Y., and conducted advanced work on specialized software for the <a href=\"https://www.navy.mil/\" rel=\"noopener noreferrer\" target=\"_blank\">U.S. Navy</a> until his retirement in 2016.</p><h2>Remembering a friend</h2><p>Chari successfully nominated one of us (Hoole) to be elevated to IEEE Fellow at the age of 40. He helped launch Haran’s career when Chari sent his résumé to GE hiring managers for a position in its applied superconductivity lab.</p><p>Chari’s commitment to people came from his family background. His father—<a href=\"https://en.wikipedia.org/wiki/M._A._Ayyangar\" rel=\"noopener noreferrer\" target=\"_blank\">M.A. Ayyangar</a>—was known throughout India as a freedom fighter, mathematician, and eventually the speaker of the Indian Parliament’s lower house under <a href=\"https://en.wikipedia.org/wiki/Jawaharlal_Nehru\" rel=\"noopener noreferrer\" target=\"_blank\">Prime Minister Nehru</a>. Chari’s wife, Padma, was a physician in New York.</p><p>From Chari’s illustrious family, he was at the peak of South India (<a href=\"https://en.wikipedia.org/wiki/Tamils\" rel=\"noopener noreferrer\" target=\"_blank\">Tamil</a>) society.</p><p>Chari would fondly and cheerfully tell us the story behind his name. Around the time of his birth, it was common in Tamil society not to have formal names. He went by the informal “house name” Kannah (a term of endearment for <a href=\"https://en.wikipedia.org/wiki/Krishna\" rel=\"noopener noreferrer\" target=\"_blank\">Krishna</a>). When it was time for Chari to start school, an auspicious uncle enrolled him. But Chari had no formal name, so the uncle took it upon himself to give him one. He asked Chari if he would like a long or short name, to which he said long. So the uncle named him Madabushi Venkadamachari.</p><p>When Chari moved to North America, he shortened his name to Madabushi V.K.</p><p>He could also laugh at himself.</p><p>A stellar scientist, he also was a role model, guide, and friend to many of us. We thank God for him.</p>",
      "author": "Sheppard J. Salon",
      "publishedAt": "2026-02-04T19:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "7994e683-d3d1-4e5f-90b7-bbef928b8049",
      "guid": "https://spectrum.ieee.org/winter-olympics-2026-tech",
      "title": "Milan-Cortina Winter Olympics Debut Next-Generation Sports Smarts",
      "link": "https://spectrum.ieee.org/winter-olympics-2026-tech",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/silhouettes-of-a-twirling-figure-skater-and-a-ski-jumper-against-a-dark-background.jpg?id=63783716&width=1245&height=700&coordinates=0%2C62%2C0%2C63\"/><br/><br/><p>From 6–22 February, the 2026 <a href=\"https://www.olympics.com/en/milano-cortina-2026\" target=\"_blank\">Winter Olympics in Milan-Cortina d’Ampezzo</a>, Italy, will feature not just the world’s top winter athletes but also some of the most advanced sports technologies today. At the <a href=\"https://www.olympics.com/en/olympic-games/cortina-d-ampezzo-1956\" target=\"_blank\">first Cortina Olympics</a>, in 1956, the Swiss company <a href=\"https://en.wikipedia.org/wiki/Omega_SA\" target=\"_blank\">Omega</a>—based in <a href=\"https://en.wikipedia.org/wiki/Biel/Bienne\" target=\"_blank\">Biel/Bienne</a>—introduced electronic ski starting gates and launched the first automated timing tech of its kind.</p><p><span>At this year’s Olympics, <a href=\"https://en.wikipedia.org/wiki/The_Swatch_Group#Sport_and_event_timing\" target=\"_blank\">Swiss Timing</a>,</span><span> sister company to Omega under the parent company <a href=\"https://www.swatchgroup.com/en\" target=\"_blank\">Swatch Group</a>, unveils a new generation of <a href=\"https://spectrum.ieee.org/tag/motion-capture\" target=\"_self\"><span><span>motion-analysis</span></span></a> and <a href=\"https://spectrum.ieee.org/tag/computer-vision\" target=\"_self\"><span><span>computer-vision</span></span></a> technology. The new technologies on offer include photo-finish cameras that capture up to 40,000 images per second. </span></p><p>“We work very closely with athletes,” says <a href=\"https://www.swisstiming.com/\" target=\"_blank\"><span><span>Swiss Timing</span></span></a> CEO <a href=\"https://www.linkedin.com/in/alain-zobrist-2b3a36a/?originalSubdomain=ch\" target=\"_blank\"><span>Alain Zobrist</span></a>, who has overseen Olympic timekeeping since the <a href=\"https://www.olympics.com/ioc/legacy-torino-2006\" target=\"_blank\"><span><span>winter games of 2006 in Torino</span></span></a>. “They are the primary customers of our technology and services, and they need to understand how our systems work in order to trust them.”</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"Live data capture of a figure skater's performance, with a 3D rendering of the athlete, jump heights and more.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"b3f4b7d6aa8a6f1471fd6ff98085f8ca\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"80fab\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/live-data-capture-of-a-figure-skater-s-performance-with-a-3d-rendering-of-the-athlete-jump-heights-and-more.jpg?id=63784021&width=980\"/> <small class=\"image-media media-caption\" data-gramm=\"false\" data-lt-tmp-id=\"lt-691557\" placeholder=\"Add Photo Caption...\" spellcheck=\"false\">Using high-resolution cameras and AI algorithms tuned to skaters’ routines, Milan-Cortina Olympic officials expect new figure-skating tech to be a key highlight of the games.  </small><small class=\"image-media media-photo-credit\" data-gramm=\"false\" data-lt-tmp-id=\"lt-586243\" placeholder=\"Add Photo Credit...\" spellcheck=\"false\">Omega</small></p><h3>Figure-Skating Tech Completes the Rotation</h3><p><span><a href=\"https://www.olympics.com/en/milano-cortina-2026/sports/figure-skating\" target=\"_blank\">Figure skating</a></span>, the Winter Olympics’ biggest TV draw, is receiving a substantial upgrade at Milano Cortina 2026.</p><p>Fourteen <a href=\"https://en.wikipedia.org/wiki/8K_resolution\" target=\"_blank\"><span>8K-resolution cameras</span></a> positioned around the rink will capture every skater’s movement. <span>“We use proprietary software to interpret the images and visualize athlete movement in a 3D model,” says Zobrist. “AI processes the data so we can track trajectory, position, and movement across all three axes—x, y, and z.”</span></p><p><span>The system measures jump heights, air times, and landing speeds in real time, producing heat maps and graphic overlays that break down each program—all instantaneously. “The time it takes for us to measure the data, until we show a matrix on TV with a graphic, this whole chain needs to take less than 1/10 of a second,” Zobrist says.</span></p><h3></h3><br/><div class=\"rblad-ieee_in_content\"></div><p>A range of different AI models helps the broadcasters and commentators process each skater’s every move on the ice.</p><p><span>“There is an AI that helps our computer-vision system do pose estimation,” he says. “So we have a camera that is filming what is happening, and an AI that helps the camera understand what it’s looking at. And then there is a second type of AI, which is more similar to a large language model that makes sense of the data that we collect.”</span></p><p>Among the features that Swiss Timing’s new systems provide is blade-angle detection, which gives judges precise technical data to augment their technical and aesthetic decisions. Zobrist says future versions will also determine whether a given rotation is complete, so that “if the rotation is 355 degrees, there is going to be a deduction,” he says.</p><p>This builds on technology Omega unveiled at the <a href=\"https://en.wikipedia.org/wiki/2024_Summer_Olympics\" target=\"_blank\"><span>2024 Paris Olympics</span></a> for diving, where cameras measured distances between a diver’s head and the board to help judges assess points and penalties to be awarded.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"Three dimensional rendering of a ski jumper preparing for dismount on a tall slope.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"16a88f6b65d50df446ca4a9b56b5009a\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"6fe22\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/three-dimensional-rendering-of-a-ski-jumper-preparing-for-dismount-on-a-tall-slope.jpg?id=63783856&width=980\"/> <small class=\"image-media media-caption\" data-gramm=\"false\" data-lt-tmp-id=\"lt-172123\" placeholder=\"Add Photo Caption...\" spellcheck=\"false\">At the 2026 Winter Olympics, ski jumping will feature both camera-based and sensor-based technologies to make the aerial experience more immediate and real-time. </small><small class=\"image-media media-photo-credit\" data-gramm=\"false\" data-lt-tmp-id=\"lt-922547\" placeholder=\"Add Photo Credit...\" spellcheck=\"false\">Omega</small></p><h3>Ski-Jumping Tech Finds Make-or-Break Moments</h3><p>Unlike figure skating’s camera-based approach, <a href=\"https://www.olympics.com/en/milano-cortina-2026/sports/ski-jumping\" target=\"_blank\"><span>ski jumping</span></a> also relies on physical <a href=\"https://spectrum.ieee.org/search/?q=camera&topic=sensors&order=newest\" target=\"_self\"><span>sensors</span></a>.</p><p>“In ski jumping, we use a small, lightweight sensor attached to each ski, one sensor per ski, not on the athlete’s body,” Zobrist says. The sensors are lightweight and broadcast data on a skier’s speed, acceleration, and positioning in the air. The technology also correlates performance data with wind conditions, revealing the influence of environmental factors <span>on each jump.</span></p><p>High-speed cameras also track each ski jumper. Then, a stroboscopic camera provides body position time-lapses throughout the jump.</p><p>“The first 20 to 30 meters after takeoff are crucial as athletes move into a V position and lean forward,” Zobrist says. “And both the timing and precision of this movement strongly influence performance.”</p><p>The system reveals biomechanical characteristics in real time, he adds, showing how athletes position their bodies during every moment of the takeoff process. The most common mistake in flight position, over-rotation or under-rotation, can now be detailed and diagnosed with precision on every jump.</p><h3>Bobsleigh: Pushing the Line on the Photo Finish</h3><p>This year’s Olympics will also feature a “virtual photo finish,” providing comparison images of when different sleds cross the finish line over previous runs.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" data-rm-resized-container=\"25%\" style=\"float: left;\"> <img alt=\"Red Omega camera with large lens, under a sleek hood, set against a black background.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"247107848870b201556938d35e8e282a\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"816ed\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/red-omega-camera-with-large-lens-under-a-sleek-hood-set-against-a-black-background.jpg?id=63784093&width=980\"/> <small class=\"image-media media-caption\" data-gramm=\"false\" data-lt-tmp-id=\"lt-951245\" placeholder=\"Add Photo Caption...\" spellcheck=\"false\">Omega’s cameras will provide virtual photo finishes at the 2026 Winter Olympics. </small><small class=\"image-media media-photo-credit\" data-gramm=\"false\" data-lt-tmp-id=\"lt-505041\" placeholder=\"Add Photo Credit...\" spellcheck=\"false\">Omega</small></p><p>“We virtually build a photo finish that shows different sleds from different runs on a single visual reference,” says Zobrist.</p><p>After each run, composite images show the margins separating performances. However, more tried-and-true technology still generates official results. A Swiss Timing score, he says, still comes courtesy of <a data-linked-post=\"2653906650\" href=\"https://spectrum.ieee.org/a-century-ago-the-optophone-allowed-blind-people-to-hear-the-printed-word\" target=\"_blank\">photoelectric cells</a>, devices that emit light beams across the finish line and stop the clock when broken. The company offers its virtual photo finish, by contrast, as a visualization tool for spectators and commentators.</p><p>In bobsleigh, as in every timed Winter Olympic event, the line between triumph and heartbreak is sometimes measured in milliseconds or even shorter time intervals. Such precision will, Zobrist says, stem from <a href=\"https://swisswatches-magazine.com/omegas-timekeeping-lab/\" target=\"_blank\">Omega’s Quantum Timer</a>.</p><p>“We can measure time to the millionth of a second, so six digits after the comma, with a deviation of about 23 nanoseconds over 24 hours,” Zobrist explained. “These devices are constantly calibrated and used across all timed sports.”</p>",
      "author": "Maurizio Arseni",
      "publishedAt": "2026-02-04T17:03:54.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "a4b2ee53-ecd8-45e1-a17d-3fa1c37ab6f2",
      "guid": "https://content.knowledgehub.wiley.com/breaking-boundaries-in-wireless-communication-simulating-animated-on-body-rf-propagation/",
      "title": "Breaking Boundaries in Wireless Communication",
      "link": "https://content.knowledgehub.wiley.com/breaking-boundaries-in-wireless-communication-simulating-animated-on-body-rf-propagation/",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/blue-remcom-text-with-orange-circle-and-arc-design-above-the-letter-o.png?id=63752871&width=980\"/><br/><br/><p>This paper discusses how RF propagation simulations empower engineers to test numerous real-world use cases in far less time, and at lower costs, than in situ testing alone. Learn how simulations provide a powerful visual aid and offer valuable insights to improve the performance and design of body-worn wireless devices.</p><p><span><a href=\"https://content.knowledgehub.wiley.com/breaking-boundaries-in-wireless-communication-simulating-animated-on-body-rf-propagation/\" target=\"_blank\">Download this free whitepaper now!</a></span></p>",
      "author": "Remcom",
      "publishedAt": "2026-02-03T15:58:27.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "8061225a-0c9c-4d5e-80a0-092be5a984cc",
      "guid": "https://spectrum.ieee.org/particle-physics-ai",
      "title": "AI Hunts for the Next Big Thing in Physics",
      "link": "https://spectrum.ieee.org/particle-physics-ai",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/circular-and-spiral-tracks-are-shown-as-light-blue-lines-against-a-darker-blue-background.jpg?id=63686429&width=1245&height=700&coordinates=0%2C62%2C0%2C63\"/><br/><br/><p><span><strong>In 1930, a young physicist</strong> named Carl D. Anderson was tasked by his mentor with measuring the energies of cosmic rays—particles arriving at high speed from outer space. Anderson built an improved version of a cloud chamber, a device that visually records the trajectories of particles. In 1932, he saw evidence that confusingly combined the properties of protons and electrons. “A situation began to develop that had its awkward aspects,” he wrote many years after winning a Nobel Prize at the age of 31. Anderson had accidentally discovered antimatter.</span></p><p><span>Four years after his first discovery, he codiscovered another elementary particle, the muon. This one prompted one physicist to ask, “Who ordered that?”</span></p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25\" data-rm-resized-container=\"25%\" style=\"float: left;\"> <img alt=\"a photo shows a man in a suit sitting beside a large laboratory apparatus.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"86790a8a0ef4f037ac318b672c036c03\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"24d14\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-photo-shows-a-man-in-a-suit-sitting-beside-a-large-laboratory-apparatus.jpg?id=63687631&width=980\"/> </p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25\" data-rm-resized-container=\"25%\" style=\"float: left;\"> <img alt=\" a circular black-and-white image shows curved particle tracks. \" class=\"rm-shortcode\" data-rm-shortcode-id=\"f44f892059146ebdd9efa5621fb0c1a2\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"ee99e\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-circular-black-and-white-image-shows-curved-particle-tracks.jpg?id=63687608&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">Carl Anderson [top] sits beside the magnet cloud chamber he used to discover the positron. His cloud-chamber photograph [bottom] from 1932 shows the curved track of a positron, the first known antimatter particle.  </small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Caltech Archives & Special Collections </small></p><p><span>Over the decades since then, particle physicists have built increasingly sophisticated instruments of exploration. At the apex of these physics-finding machines sits the Large Hadron Collider, which in 2022 started its third operational run. This underground ring, 27 kilometers in circumference and straddling the border between France and Switzerland, was built to slam subatomic particles together at near light speed and test deep theories of the universe. Physicists from around the world turn to the LHC, hoping to find something new. They’re not sure what, but they hope to find it.</span></p><p>It’s the latest manifestation of a rich tradition. Throughout the history of science, new instruments have prompted hunts for the unexpected. Galileo Galilei built telescopes and found Jupiter’s moons. Antonie van Leeuwenhoek built microscopes and noticed “animalcules, very prettily a-moving.” And still today, people peer through lenses and pore through data in search of patterns they hadn’t hypothesized. Nature’s secrets don’t always come with spoilers, and so we gaze into the unknown, ready for anything.</p><h3></h3><br/><p>But novel, fundamental aspects of the universe are growing less forthcoming. In a sense, we’ve plucked the lowest-hanging fruit. We know to a good approximation what the building blocks of matter are. The Standard Model of particle physics, which describes the currently known elementary particles, has been in place since the 1970s. Nature can still surprise us, but it typically requires larger or finer instruments, more detailed or expansive data, and faster or more flexible analysis tools.</p><p>Those analysis tools include a form of artificial intelligence (AI) called <a href=\"https://www.nature.com/articles/s42254-022-00455-1\" target=\"_blank\">machine learning</a>. Researchers train complex statistical models to find patterns in their data, patterns too subtle for human eyes to see, or too rare for a single human to encounter. At the LHC, which smashes together protons to create immense bursts of energy that decay into other short-lived particles of matter, a theorist might predict some new particle or interaction and describe what its signature would look like in the LHC data, often using a simulation to create synthetic data. Experimentalists would then collect petabytes of measurements and run a machine learning algorithm that compares them with the simulated data, looking for a match. Usually, they come up empty. But maybe new algorithms can peer into corners they haven’t considered.</p><h2>A New Path for Particle Physics</h2><p>“You’ve heard probably that there’s a crisis in particle physics,” says <a href=\"https://www.thphys.uni-heidelberg.de/~plehn/\" rel=\"noopener noreferrer\" target=\"_blank\">Tilman Plehn</a>, a theoretical physicist at Heidelberg University, in Germany. At the LHC and other high-energy physics facilities around the world, the experimental results have failed to yield insights on new physics. “We have a lot of unhappy theorists who thought that their model would have been discovered, and it wasn’t,” Plehn says.</p><h3></h3><br/><img alt=\"Person wearing a patterned shirt against a pale blue background.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"75d8175c67aff628ef15ce2554d50ef8\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"91e5c\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/person-wearing-a-patterned-shirt-against-a-pale-blue-background.jpg?id=63688381&width=980\"/><p class=\"pull-quote\"><span>“We have a lot of unhappy theorists who thought that their model would have been discovered, and it wasn’t.”</span></p><h3></h3><br/><p><a href=\"https://www.physik.uni-hamburg.de/en/iexp/gruppe-kasieczka/personen/kasieczka-gregor.html\" rel=\"noopener noreferrer\" target=\"_blank\">Gregor Kasieczka</a>, a physicist at the University of Hamburg, in Germany, recalls the field’s enthusiasm when the LHC began running in 2008. Back then, he was a young graduate student and expected to see signs of supersymmetry, a theory predicting heavier versions of the known matter particles. The presumption was that “we turn on the LHC, and supersymmetry will jump in your face, and we’ll discover it in the first year or so,” he tells me. Eighteen years later, supersymmetry remains in the theoretical realm. “I think this level of exuberant optimism has somewhat gone.”</p><h3></h3><br/><p>The result, Plehn says, is that models for all kinds of things have fallen in the face of data. “And I think we’re going on a different path now.”</p><p>That path involves a kind of machine learning called unsupervised learning. In unsupervised learning, you don’t teach the AI to recognize your specific prediction—signs of a particle with this mass and this charge. Instead, you might teach it to find anything out of the ordinary, anything interesting—which could indicate brand new physics. It’s the equivalent of looking with fresh eyes at a starry sky or a slide of pond scum. The problem is, how do you automate the search for something “interesting”?</p><h2>Going Beyond the Standard Model</h2><p>The Standard Model leaves many questions unanswered. Why do matter particles have the masses they do? Why do neutrinos have mass at all? Where is the particle for transmitting gravity, to match those for the other forces? Why do we see more matter than antimatter? Are there extra dimensions? What is dark matter—the invisible stuff that makes up most of the universe’s matter and that we assume to exist because of its gravitational effect on galaxies? Answering any of these questions could open the door to new physics, or fundamental discoveries beyond the Standard Model.</p><h3></h3><br/><img alt=\"A long blue accelerator tube marked \\u201cLHC\\u201d runs through an underground tunnel.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"402c04487681186ea40924697e3692be\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"1c8c9\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-long-blue-accelerator-tube-marked-u201clhc-u201d-runs-through-an-underground-tunnel.jpg?id=63688794&width=980\"/><h3></h3><br/><p>“Personally, I’m excited for portal models of dark sectors,” Kasieczka says, as if reading from a Marvel film script. He asks me to imagine a mirror copy of the Standard Model out there somewhere, sharing only one “portal” particle with the Standard Model we know and love. It’s as if this portal particle has a second secret family.</p><p>Kasieczka says that in the LHC’s third run, scientists are splitting their efforts roughly evenly between measuring more precisely what they know to exist and looking for what they don’t know to exist. In some cases, the former could enable the latter. The Standard Model predicts certain particle properties and the relationships between them. For example, it correctly predicted a property of the electron called the magnetic moment to about one part in a trillion. And precise measurements could turn up internal inconsistencies. “Then theorists can say, ‘Oh, if I introduce this new particle, it fixes this specific problem that you guys found. And this is how you look for this particle,’” Kasieczka says.</p><h3></h3><br/><img alt=\"A simplified chart of the Standard Model of physics shows matter particles (quarks and leptons), force-carrying particles, and the Higgs, which conveys mass.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"51370a4c6a287342935bbef4a2493607\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"937d6\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-simplified-chart-of-the-standard-model-of-physics-shows-matter-particles-quarks-and-leptons-force-carrying-particles-and-t.jpg?id=63689716&width=980\"/><h3></h3><br/><p>What’s more, the Standard Model has occasionally shown signs of cracks. Certain particles containing bottom quarks, for example, seem to decay into other particles in unexpected ratios. Plehn finds the bottom-quark incongruities intriguing. “Year after year, I feel they should go away, and they don’t. And nobody has a good explanation,” he says. “I wouldn’t even know who I would shout at”—the theorists or the experimentalists—“like, ‘Sort it out!’”</p><p>Exasperation isn’t exactly the right word for Plehn’s feelings, however. Physicists feel gratified when measurements reasonably agree with expectations, he says. “But I think deep down inside, we always hope that it looks unreasonable. Everybody always looks for the anomalous stuff. Everybody wants to see the standard explanation fail. First, it’s fame”—a chance for a Nobel—“but it’s also an intellectual challenge, right? You get excited when things don’t work in science.”</p><h2>How Unsupervised AI Can Probe for New Physics</h2><p>Now imagine you had a machine to find all the times things don’t work in science, to uncover all the anomalous stuff. That’s how researchers are using unsupervised learning. One day over ice cream, Plehn and a friend who works at the software company SAP began discussing <a href=\"https://www.datacamp.com/tutorial/introduction-to-autoencoders\" target=\"_blank\">autoencoders</a>, one type of unsupervised learning algorithm. “He tells me that autoencoders are what they use in industry to see if a network was hacked,” Plehn remembers. “You have, say, a hundred computers, and they have network traffic. If the network traffic [to one computer] changes all of a sudden, the computer has been hacked, and they take it offline.”</p><h3></h3><br/><img alt=\"a person wearing a hard hat walks down an aisle.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"2e56455349aad07bd98f7a9a64fa5982\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"c2b61\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-person-wearing-a-hard-hat-walks-down-an-aisle.jpg?id=63689029&width=980\"/><h3></h3><br/><img alt=\"Photo show rows of electronic racks filled with cables and equipment inside a data-acquisition room.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"55e6b0584b33ebbda14bb47a2626ce3b\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"1865f\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/photo-show-rows-of-electronic-racks-filled-with-cables-and-equipment-inside-a-data-acquisition-room.jpg?id=63689042&width=980\"/><h3></h3><br/><p>Autoencoders are neural networks that start with an input—it could be an image of a cat, or the record of a computer’s network traffic—and compress it, like making a tiny JPEG or MP3 file, and then decompress it. Engineers train them to compress and decompress data so that the output matches the input as closely as possible. Eventually a network becomes very good at that task. But if the data includes some items that are relatively rare—such as white tigers, or hacked computers’ traffic—the network performs worse on these, because it has less practice with them. The difference between an input and its reconstruction therefore signals how anomalous that input is.</p><p>“This friend of mine said, ‘You can use exactly our software, right?’” Plehn remembers. “‘It’s exactly the same question. Replace computers with particles.’” The two imagined feeding the autoencoder signatures of particles from a collider and asking: Are any of these particles not like the others? Plehn continues: “And then we wrote up a joint grant proposal.”</p><p>It’s not a given that AI will find new physics. Even learning what counts as interesting is a daunting hurdle. Beginning in the 1800s, men in lab coats delegated data processing to women, whom they saw as diligent and detail oriented. Women annotated photos of stars, and they acted as “computers.” In the 1950s, women were trained to scan <a href=\"https://home.cern/news/news/experiments/seeing-invisible-event-displays-particle-physics\" target=\"_blank\">bubble chambers</a>, which recorded particle trajectories as lines of tiny bubbles in fluid. Physicists didn’t explain to them the theory behind the events, only what to look for based on lists of rules. </p><p>But, as the Harvard science historian <a href=\"https://galison.scholars.harvard.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Peter Galison</a> writes in <em>Image and Logic: A Material Culture of Physics</em>, his influential account of how physicists’ tools shape their discoveries, the task was “subtle, difficult, and anything but routinized,” requiring “three-dimensional visual intuition.” He goes on: “Even within a single experiment, judgment was required—this was not an algorithmic activity, an assembly line procedure in which action could be specified fully by rules.”</p><h3></h3><br/><img alt=\"Person in a suit with dark hair against a blue background.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"c284009312420372412cc4cc70e713a5\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"b0ff8\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/person-in-a-suit-with-dark-hair-against-a-blue-background.jpg?id=63688530&width=980\"/><p class=\"pull-quote\">“We are not looking for flying elephants but instead a few extra elephants than usual at the local watering hole.”</p><h3></h3><br/><p>Over the last decade, though, one thing we’ve learned is that AI systems can, in fact, perform tasks once thought to require human intuition, such as <a href=\"https://spectrum.ieee.org/monster-machine-defeats-prominent-pro-player\" target=\"_self\">mastering the ancient board game Go</a>. So researchers have been testing AI’s intuition in physics. In 2019, Kasieczka and his collaborators announced the <a href=\"https://iopscience.iop.org/article/10.1088/1361-6633/ac36b9/meta\" target=\"_blank\">LHC Olympics 2020</a>, a contest in which participants submitted algorithms to find anomalous events in three sets of (simulated) LHC data. Some teams correctly found the anomalous signal in one dataset, but some falsely reported one in the second set, and they all missed it in the third. In 2020, a research collective called <a href=\"https://www.scipost.org/10.21468/SciPostPhys.12.1.043\" target=\"_blank\">Dark Machines</a> announced a similar competition, which drew more than 1,000 submissions of machine learning models. Decisions about how to score them led to different rankings, showing that there’s no best way to explore the unknown.</p><p>Another way to test unsupervised learning is to play revisionist history. In 1995, a particle dubbed the top quark turned up at the Tevatron, a particle accelerator at the Fermi National Accelerator Laboratory (<a href=\"https://fnal.gov/\" target=\"_blank\">Fermilab</a>), in Illinois. But what if it actually hadn’t? Researchers <a href=\"https://epjplus.epj.org/articles/epjplus/abs/2021/02/13360_2021_Article_1109/13360_2021_Article_1109.html\" rel=\"noopener noreferrer\" target=\"_blank\">applied</a> unsupervised learning to LHC data collected in 2012, pretending they knew almost nothing about the top quark. Sure enough, the AI revealed a set of anomalous events that were clustered together. Combined with a bit of human intuition, they pointed toward something like the top quark.</p><h3></h3><br/><img alt=\"Person with long hair wearing a sweater and light-colored top against a blue background.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"88bfd49b51c02712bb83d0e9ee23b1da\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"a121d\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/person-with-long-hair-wearing-a-sweater-and-light-colored-top-against-a-blue-background.jpg?id=63690566&width=980\"/><p class=\"pull-quote\">“An algorithm that can recognize any kind of disturbance would be a win.”</p><h3></h3><br/><p>That exercise underlines the fact that unsupervised learning can’t replace physicists just yet. “If your anomaly detector detects some kind of feature, how do you get from that statement to something like a physics interpretation?” Kasieczka says. “The anomaly search is more a scouting-like strategy to get you to look into the right corner.” <a href=\"https://www.physics.columbia.edu/content/georgia-karagiorgi\" target=\"_blank\">Georgia Karagiorgi</a>, a physicist at Columbia University, agrees. “Once you find something unexpected, you can’t just call it quits and be like, ‘Oh, I discovered something,’” she says. “You have to come up with a model and then test it.”</p><p><a href=\"https://www.physics.wisc.edu/directory/cranmer-kyle/\" target=\"_blank\">Kyle Cranmer</a>, a physicist and data scientist at the University of Wisconsin-Madison who played a key role in the <a href=\"https://spectrum.ieee.org/a-tantalizing-hint-of-the-higgs\" target=\"_self\">discovery of the Higgs boson particle</a> in 2012, also says that human expertise can’t be dismissed. “There’s an infinite number of ways the data can look different from what you expected,” he says, “and most of them aren’t interesting.” Physicists might be able to recognize whether a deviation suggests some plausible new physical phenomenon, rather than just noise. “But how you try to codify that and make it explicit in some algorithm is much less straightforward,” Cranmer says. Ideally, the guidelines would be general enough to exclude the unimaginable without eliminating the merely unimagined. “That’s gonna be your Goldilocks situation.”</p><p>In his 1987 book <em>How Experiments End</em>, Harvard’s Galison writes that scientific instruments can “import assumptions built into the apparatus itself.” He tells me about a 1973 experiment that looked for a phenomenon called neutral currents, signaled by an absence of a so-called heavy electron (later renamed the muon). One team initially used a trigger left over from previous experiments, which recorded events only if they produced those heavy electrons—even though neutral currents, by definition, produce none. As a result, for some time the researchers missed the phenomenon and wrongly concluded that it didn’t exist. Galison says that the physicists’ design choice “allowed the discovery of [only] one thing, and it blinded the next generation of people to this new discovery. And that is always a risk when you’re being selective.”</p><h2>How AI Could Miss—or Fake—New Physics</h2><p>I ask Galison if by automating the search for interesting events, we’re letting the AI take over the science. He rephrases the question: “Have we handed over the keys to the car of science to the machines?” One way to alleviate such concerns, he tells me, is to generate test data to see if an algorithm behaves as expected—as in the LHC Olympics. “Before you take a camera out and photograph the Loch Ness Monster, you want to make sure that it can reproduce a wide variety of colors” and patterns accurately, he says, so you can rely on it to capture whatever comes.</p><p>Galison, who is also a physicist, works on the <a href=\"https://www.welcometothejungle.com/en/articles/btc-black-hole-imaging-software-telescope\" rel=\"noopener noreferrer\" target=\"_blank\">Event Horizon Telescope</a>, which images black holes. For that project, he remembers putting up utterly unexpected test images like Frosty the Snowman so that scientists could probe the system’s general ability to catch something new. “The danger is that you’ve missed out on some crucial test,” he says, “and that the object you’re going to be photographing is so different from your test patterns that you’re unprepared.”</p><p>The algorithms that physicists are using to seek new physics are certainly vulnerable to this danger. It helps that unsupervised learning is already being used in many applications. In industry, it’s surfacing anomalous credit-card transactions and hacked networks. In science, it’s identifying earthquake precursors, genome locations where proteins bind, and merging galaxies.</p><h3></h3><br/><img alt=\"A colorful visualization shows many particle tracks radiating outward from a collision point.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"7c97622adcd56f2e689ec345310cf808\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"f6a0e\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-colorful-visualization-shows-many-particle-tracks-radiating-outward-from-a-collision-point.jpg?id=63688853&width=980\"/><h3></h3><br/><p>But one difference with particle-physics data is that the anomalies may not be stand-alone objects or events. You’re looking not just for a needle in a haystack; you’re also looking for subtle irregularities in the haystack itself. Maybe a stack contains a few more short stems than you’d expect. Or a pattern reveals itself only when you simultaneously look at the size, shape, color, and texture of stems. Such a pattern might suggest an unacknowledged substance in the soil. In accelerator data, subtle patterns might suggest a hidden force. As Kasieczka and his colleagues write in <a href=\"https://escholarship.org/content/qt56p5b8qm/qt56p5b8qm_noSplash_3309801b69925912167073f272fc7612.pdf\" target=\"_blank\">one paper</a>, “We are not looking for flying elephants, but instead a few extra elephants than usual at the local watering hole.”</p><p>Even algorithms that weigh many factors can miss signals—and they can also see spurious ones. The stakes of mistakenly claiming discovery are high. Going back to the hacking scenario, Plehn says, a company might ultimately determine that its network wasn’t hacked; it was just a new employee. The algorithm’s false positive causes little damage. “Whereas if you stand there and get the Nobel Prize, and a year later people say, ‘Well, it was a fluke,’ people would make fun of you for the rest of your life,” he says. In particle physics, he adds, you run the risk of spotting patterns purely by chance in big data, or as a result of malfunctioning equipment.</p><p>False alarms have happened before. In 1976, a group at Fermilab led by Leon Lederman, who later won a Nobel for other work, announced the discovery of a particle they tentatively called the Upsilon. The researchers calculated the probability of the signal’s happening by chance as 1 in 50. After further data collection, though, they walked back the discovery, calling the pseudo-particle the Oops-Leon. (Today, particle physicists wait until the chance that a finding is a fluke drops below 1 in 3.5 million, the so-called five-sigma criterion.) And in 2011, researchers at the Oscillation Project with Emulsion-tRacking Apparatus (OPERA) experiment, in Italy, announced evidence for faster-than-light travel of neutrinos. Then, a few months later, they reported that the result was due to a faulty connection in their timing system.</p><p>Those cautionary tales linger in the minds of physicists. And yet, even while researchers are wary of false positives from AI, they also see it as a safeguard against them. So far, unsupervised learning has discovered no new physics, despite its use on data from multiple experiments at Fermilab and CERN. But anomaly detection may have prevented embarrassments like the one at OPERA. “So instead of telling you there’s a new physics particle,” Kasieczka says, “it’s telling you, this sensor is behaving weird today. You should restart it.”</p><h2>Hardware for AI-Assisted Particle Physics</h2><p>Particle physicists are pushing the limits of not only their computing software but also their computing hardware. The challenge is unparalleled. The LHC produces 40 million particle collisions per second, each of which can produce a megabyte of data. That’s much too much information to store, even if you could save it to disk that quickly. So the two largest detectors each use two-level data filtering. The first layer, called the Level-1 Trigger, or L1T, harvests 100,000 events per second, and the second layer, called the High-Level Trigger, or HLT, plucks 1,000 of those events to save for later analysis. So only one in 40,000 events is ever potentially seen by human eyes.</p><h3></h3><br/><img alt=\"Person with long blonde hair in a white shirt against a solid blue background.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"6d32802ccf9c56bed216ddc413965057\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"747ca\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/person-with-long-blonde-hair-in-a-white-shirt-against-a-solid-blue-background.jpg?id=63691908&width=980\"/><p class=\"pull-quote\"><span>“</span>T<span>hat’s</span> when I thought, we need something like [AlphaGo] in physics. We need a genius that can look at the world differently.” </p><h3></h3><br/><p>HLTs use central processing units (CPUs) like the ones in your desktop computer, running complex machine learning algorithms that analyze collisions based on the number, type, energy, momentum, and angles of the new particles produced. L1Ts, as a first line of defense, must be fast. So the L1Ts rely on integrated circuits called field-programmable gate arrays (FPGAs), which users can reprogram for specialized calculations. </p><p>The trade-off is that the programming must be relatively simple. The FPGAs can’t easily store and run fancy neural networks; instead they follow scripted rules about, say, what features of a particle collision make it important. In terms of complexity level, it’s the instructions given to the women who scanned bubble chambers, not the women’s brains.</p><p><a href=\"https://www.space.mit.edu/people/katya-govorkova/\" target=\"_blank\">Ekaterina (Katya) Govorkova</a>, a particle physicist at MIT, saw a path toward improving the LHC’s filters, inspired by a board game. Around 2020, she was looking for new physics by comparing precise measurements at the LHC with predictions, using little or no machine learning. Then she watched a documentary about <a href=\"https://deepmind.google/research/alphago/\" rel=\"noopener noreferrer\" target=\"_blank\">AlphaGo</a>, the program that used machine learning to beat a human Go champion. “For me the moment of realization was when AlphaGo would use some absolutely new type of strategy that humans, who played this game for centuries, hadn’t thought about before,” she says. “So that’s when I thought, we need something like that in physics. We need a genius that can look at the world differently.” New physics may be something we’d never imagine.</p><p>Govorkova and her collaborators found a way to compress autoencoders to put them on FPGAs, where they process an event every 80 nanoseconds (less than 10-millionth of a second). (Compression involved pruning some network connections and <a href=\"https://spectrum.ieee.org/1-bit-llm\" target=\"_self\">reducing the precision</a> of some calculations.) They <a href=\"https://www.nature.com/articles/s42256-022-00441-3\" rel=\"noopener noreferrer\" target=\"_blank\">published</a> their methods in <em>Nature Machine Intelligence </em>in 2022, and researchers are now using them during the LHC’s third run. The new trigger tech is installed in one of the detectors around the LHC’s giant ring, and it has found many anomalous events that would otherwise have gone unflagged.</p><p>Researchers are currently setting up analysis workflows to decipher why the events were deemed anomalous. <a href=\"https://www.linkedin.com/in/jennifer-ngadiuba-a2138b141/\" rel=\"noopener noreferrer\" target=\"_blank\">Jennifer Ngadiuba</a>, a particle physicist at Fermilab who is also one of the coordinators of the trigger system (and one of Govorkova’s coauthors), says that one feature stands out already: Flagged events have lots of jets of new particles shooting out of the collisions. But the scientists still need to explore other factors, like the new particles’ energies and their distributions in space. “It’s a high-dimensional problem,” she says.</p><p>Eventually they will share the data openly, allowing others to eyeball the results or to apply new unsupervised learning algorithms in the hunt for patterns. <a href=\"https://jduarte.physics.ucsd.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Javier Duarte</a>, a physicist at the University of California, San Diego, and also a coauthor on the 2022 paper, says, “It’s kind of exciting to think about providing this to the community of particle physicists and saying, like, ‘Shrug, we don’t know what this is. You can take a look.’” Duarte and Ngadiuba note that high-energy physics has traditionally followed a top-down approach to discovery, testing data against well-defined theories. Adding in this new bottom-up search for the unexpected marks a new paradigm. “And also a return of sorts to before the Standard Model was so well established,” Duarte adds.</p><p>Yet it could be years before we know why AI marked those collisions as anomalous. What conclusions could they support? “In the worst case, it could be some detector noise that we didn’t know about,” which would still be useful information, Ngadiuba says. “The best scenario could be a new particle. And then a new particle implies a new force.”</p><h3></h3><br/><img alt=\"Person with braided updo in checkered suit jacket and chambray shirt, light blue background.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"f39242371a3e9c3f28858b95d5fbb950\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"27f44\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/person-with-braided-updo-in-checkered-suit-jacket-and-chambray-shirt-light-blue-background.jpg?id=63691071&width=980\"/><p class=\"pull-quote\"><span>“The best scenario could be a new particle. And then a new particle implies a new force.”</span></p><h3></h3><br/><p>Duarte says he expects their work with FPGAs to have wider applications. “The data rates and the constraints in high-energy physics are so extreme that people in industry aren’t necessarily working on this,” he says. “In self-driving cars, usually millisecond latencies are sufficient reaction times. But we’re developing algorithms that need to respond in microseconds or less. We’re at this technological frontier, and to see how much that can proliferate back to industry will be cool.”</p><p>Plehn is also working to put neural networks on FPGAs for triggers, in collaboration with experimentalists, electrical engineers, and other theorists. Encoding the nuances of abstract theories into material hardware is a puzzle. “In this grant proposal, the person I talked to most is the electrical engineer,” he says, “because I have to ask the engineer, which of my algorithms fits on your bloody FPGA?”</p><p>Hardware is hard, says <a href=\"https://kastner.ucsd.edu/ryan/\" target=\"_blank\">Ryan Kastner</a>, an electrical engineer and computer scientist at UC San Diego who works with Duarte on programming FPGAs. What allows the chips to run algorithms so quickly is their flexibility. Instead of programming them in an abstract coding language like Python, engineers configure the underlying circuitry. They map logic gates, route data paths, and synchronize operations by hand. That low-level control also makes the effort “painfully difficult,” Kastner says. “It’s kind of like you have a lot of rope, and it’s very easy to hang yourself.”</p><h2>Seeking New Physics Among the Neutrinos</h2><p><strong></strong>The next piece of new physics may not pop up at a particle accelerator. It may appear at a detector for <a href=\"https://www.energy.gov/science/doe-explainsneutrinos\" target=\"_blank\">neutrinos</a>, particles that are part of the Standard Model but remain deeply mysterious. Neutrinos are tiny, electrically neutral, and so light that no one has yet measured their mass. (The <a href=\"https://physicsworld.com/a/katrin-sets-tighter-limit-on-neutrino-mass/\" target=\"_blank\">latest attempt</a>, in April, set an upper limit of about a millionth the mass of an electron.) Of all known particles with mass, neutrinos are the universe’s most abundant, but also among the most ghostly, rarely deigning to acknowledge the matter around them. Tens of trillions pass through your body every second.</p><p>If we listen very closely, though, we may just hear the secrets they have to tell. <a href=\"https://www.physics.columbia.edu/content/georgia-karagiorgi\" target=\"_blank\">Karagiorgi</a>, of Columbia, has chosen this path to discovery. Being a physicist is “kind of like playing detective, but where you create your own mysteries,” she tells me during my visit to Columbia’s <a href=\"https://www.nevis.columbia.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Nevis Laboratories</a>, located on a large estate about 20 km north of Manhattan. Physics research began at the site after World War II; one hallway features papers going back to 1951.</p><h3></h3><br/><img alt=\"A person stands inside a room that has gold-colored grids covering the floor, walls, and ceiling.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"7611bcd2553876aaf5da2bc2c49090e8\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"cb76e\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-person-stands-inside-a-room-that-has-gold-colored-grids-covering-the-floor-walls-and-ceiling.jpg?id=63689639&width=980\"/><h3></h3><br/><p>Karagiorgi is eagerly awaiting a massive neutrino detector that’s currently under construction. Starting in 2028, Fermilab will send neutrinos west through 1,300 km of rock to South Dakota, where they’ll occasionally make their existence known in the Deep Underground Neutrino Experiment (<a href=\"https://www.dunescience.org/\" target=\"_blank\">DUNE</a>). Why so far away? When neutrinos travel long distances, they have an odd habit of oscillating, transforming from one kind or “flavor” to another. Observing the oscillations of both the neutrinos and their mirror-image antiparticles, antineutrinos, could tell researchers something about the universe’s matter-antimatter asymmetry—which the Standard Model doesn’t explain—and thus, according to the Nevis website, “why we exist.”</p><p>“DUNE is the thing that’s been pushing me to develop these real-time AI methods,” Karagiorgi says, “for sifting through the data very, very, very quickly and trying to look for rare signatures of interest within them.” When neutrinos interact with the detector’s 70,000 tonnes of liquid argon, they’ll generate a shower of other particles, creating visual tracks that look like a photo of fireworks.</p><p><span>Even when not bombarding DUNE with neutrinos, researchers will keep collecting data in the off chance that it captures neutrinos from a distant supernova. “This is a massive detector spewing out 5 terabytes of data per second,” Karagiorgi says, “and it’s going to run constantly for a decade.” They will need unsupervised learning to notice signatures that no one was looking for, because there are “lots of different models of how supernova explosions happen, and for all we know, none of them could be the right model for neutrinos,” she says. “To train your algorithm on such uncertain grounds is less than ideal. So an algorithm that can recognize any kind of disturbance would be a win.”</span></p><p>Deciding in real time which 1 percent of 1 percent of data to keep will require FPGAs. Karagiorgi’s team is preparing to use them for DUNE, and she walks me to a computer lab where they program the circuits. In the FPGA lab, we look at nondescript circuit boards sitting on a table. “So what we’re proposing is a scheme where you can have something like a hundred of these boards for DUNE deep underground that receive the image data frame by frame,” she says. This system could tell researchers whether a given frame resembled TV static, fireworks, or something in between.</p><p>Neutrino experiments, like many particle-physics studies, are very visual. When Karagiorgi was a postdoc, automated image processing at neutrino detectors was still in its infancy, so she and collaborators would often resort to visual scanning (bubble-chamber style) to measure particle tracks. She still asks undergrads to hand-scan as an educational exercise. “I think it’s wrong to just send them to write a machine learning algorithm. Unless you can actually visualize the data, you don’t really gain a sense of what you’re looking for,” she says. “I think it also helps with creativity to be able to visualize the different types of interactions that are happening, and see what’s normal and what’s not normal.”</p><p>Back in Karagiorgi’s office, a bulletin board displays images from <em><em>The Cognitive Art of Feynman Diagrams</em></em>, an exhibit for which the designer Edward Tufte created wire sculptures of the physicist Richard Feynman’s schematics of particle interactions. “It’s funny, you know,” she says. “They look like they’re just scribbles, right? But actually, they encode quantitatively predictive behavior in nature.” Later, Karagiorgi and I spend a good 10 minutes discussing whether a computer or a human could find Waldo without knowing what Waldo looked like. We also touch on the 1964 Supreme Court case in which Justice Potter Stewart famously declined to define obscenity, saying “I know it when I see it.” I ask whether it seems weird to hand over to a machine the task of deciding what’s visually interesting. “There are a lot of trust issues,” she says with a laugh.</p><p>On the drive back to Manhattan, we discuss the history of scientific discovery. “I think it’s part of human nature to try to make sense of an orderly world around you,” Karagiorgi says. “And then you just automatically pick out the oddities. Some people obsess about the oddities more than others, and then try to understand them.”</p><p>Reflecting on the Standard Model, she called it “beautiful and elegant,” with “amazing predictive power.” Yet she finds it both limited and limiting, blinding us to colors we don’t yet see. “Sometimes it’s both a blessing and a curse that we’ve managed to develop such a successful theory.” <span class=\"ieee-end-mark\"></span></p>",
      "author": "Matthew Hutson",
      "publishedAt": "2026-02-03T14:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "1968f883-eba0-48af-922c-d642ec404067",
      "guid": "https://spectrum.ieee.org/ieee-safety-guidelines-neurotech",
      "title": "IEEE Considers Safety Guidelines for Neurotech Consumer Products",
      "link": "https://spectrum.ieee.org/ieee-safety-guidelines-neurotech",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/ramses-alcaide-wearing-over-ear-headphones-while-concentrating-on-a-laptop-computer-screen.jpg?id=63707527&width=1245&height=700&coordinates=0%2C156%2C0%2C157\"/><br/><br/><p>Nonmedical devices that read brainwaves, such as smart <a href=\"https://spectrum.ieee.org/muse-headband\" target=\"_self\">headbands</a>, <a href=\"https://www.neurable.com/about\" rel=\"noopener noreferrer\" target=\"_blank\">headphones</a>, and <a href=\"https://www.narbis.shop/collections/frontpage/products/narbis-system-1\" rel=\"noopener noreferrer\" target=\"_blank\">glasses</a>, are becoming more popular among consumers. The products claim to make users more productive, creative, and healthier. <a href=\"https://spectrum.ieee.org/\" target=\"_self\"><em><em>IEEE Spectrum</em></em></a> previewed several of these <a href=\"https://spectrum.ieee.org/ces-2026-preview?utm_source=homepage&utm_medium=hero&utm_campaign=hero-2026-01-05&utm_content=hero1\" target=\"_self\">smart wearables</a> that were introduced at this year’s <a href=\"https://www.ces.tech/\" rel=\"noopener noreferrer\" target=\"_blank\">Consumer Electronics Show</a> (CES) in Las Vegas.</p><p>Since the wearable, noninvasive neurotech products aren’t medical devices, they are <a href=\"https://www.sciencedirect.com/science/chapter/bookseries/abs/pii/S2589295920300199\" rel=\"noopener noreferrer\" target=\"_blank\">not subject to the same forms of regulation</a>—which can lead to gaps in their safety and <a href=\"https://spectrum.ieee.org/privacy-health-tech-seniors\" target=\"_self\">data privacy</a>, as well as their effect on users’ brains.</p><p><a href=\"https://www.unesco.org/en\" rel=\"noopener noreferrer\" target=\"_blank\">UNESCO</a> in November adopted the first global <a href=\"https://www.unesco.org/en/articles/ethics-neurotechnology-unesco-adopts-first-global-standard-cutting-edge-technology\" rel=\"noopener noreferrer\" target=\"_blank\">ethical standard</a> for neurotechnologies, establishing guidelines to protect users’ mental privacy, freedom of thought, and human rights. In 2019 the <a href=\"https://www.oecd.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Organisation for Economic Co-operation and Development</a> issued responsible-neurotechnology <a href=\"https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0457\" rel=\"noopener noreferrer\" target=\"_blank\">recommendations</a>. But there are no socio-technical standards for manufacturers to follow.</p><p>In response, the <a href=\"https://brain.ieee.org/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Brain technical community</a> is developing the IEEE P7700 standard: “<a href=\"https://standards.ieee.org/ieee/7700/11038/\" rel=\"noopener noreferrer\" target=\"_blank\">Recommended Practice for the Responsible Design and Development of Neurotechnologies</a>.”</p><p>The proposed standard is being designed to provide a uniform set of definitions and a methodology to assess the ethical and socio-technical considerations and practices regarding the design, development, and use of neurotechnologies including wearable neurodevices for the brain, says <a href=\"https://sites.psu.edu/neuroethicslab/\" rel=\"noopener noreferrer\" target=\"_blank\">Laura Y. Cabrera</a>, the standard’s working group chair. Cabrera, an IEEE senior member, is an associate professor in the engineering science and mechanics department at <a href=\"https://www.psu.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Pennsylvania State University</a> in University Park. Her research focuses on the ethical and societal implications of neurotechnologies.</p><p>“IEEE P7700 addresses the unique characteristics of the technology and its impact on individuals and society, in particular, as it moves from therapeutic users to a wide variety of consumers,” she says.</p><p>The standard is sponsored by the <a href=\"https://technologyandsociety.org/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Society on Social Implications of Technology</a>.</p><h2>Concern over long-term effects</h2><p>The multilayered complexity of technologies that interface with the brain and nervous system presents considerations to those developing them, Cabrera says.</p><p>“There may be long-term consequences in our brains with these types of technologies,” she says. “Maybe if they were used for a short period of time, there might not be significant consequences. But what are the effects over time?”</p><p>Patients using approved brain-stimulation technology, for example, are told of its risks and benefits, but the long-term effects of headbands to improve students’ attention span aren’t known.</p><p class=\"pull-quote\">“IEEE P7700 addresses the unique characteristics of the technology and its impact on individuals and society, in particular, as it moves from therapeutic users to a wide variety of consumers.”</p><p>IEEE P7700 will address potential risks to individuals and possible negative impacts on society, Cabrera says. That includes creating guardrails to prevent harm, she adds.</p><p>The cultural implications of using neurotechnologies that interface with the brain also need to be considered, she says, because people have different views.</p><p>“The brain is considered the seed of the self and the organ that orchestrates all our thoughts, behaviors, feelings, and emotions,” she says. “The brain is really central to who we are.”</p><h2>Developing an ethical framework</h2><p>For the past five years, the <a href=\"https://brain.ieee.org/\" target=\"_blank\">IEEE Brain community</a>’s neuroethics committee has been developing a <a href=\"https://spectrum.ieee.org/ethical-guidelines-in-the-works-for-developers-of-brain-technologies\" target=\"_self\">framework</a> to evaluate the ethical, legal, social, and cultural issues that could emerge from use of the technology. The document covers nine types of applications, including those used for wellness.</p><p>Because more devices kept entering the market, IEEE Brain decided in 2023 that it was time to begin drafting a standard.</p><p>Members of its working group come from Argentina, China, Japan, Italy, Switzerland, and the United States. Participants include developers, engineers, ethicists, lawyers, and social science researchers.</p><p>The standard, Cabrera says, will be the first socio-technical standard aimed at fostering the ethical and responsible innovation of neurotechnology that meets societal and community values at an international level. P7700 will include a how-to guide, criteria for evaluating each suggested process, and case studies to help with the interpretation and practical use of the standard, she says.</p><p>“Our applied ethical approach uses a responsible research and innovation method to enable developers, researchers, users, and regulators to anticipate and address ethical and sociocultural implications of neurotechnologies, mitigating negative unintended consequences while increasing community support and engagement with innovators,” Cabrera says.</p><p>The working group is seeking additional participants to help refine the process, tools, and recommendations.</p><p>“There are a variety of people who can contribute their expertise,” she says, “including academics, data scientists, government program leaders, policymakers, lawyers, social scientists, and users.”</p><p>Cabrera says she anticipates the standard will be published early next year.</p><p>You can register to <a href=\"https://development.standards.ieee.org/myproject-web/public/view.html#/interest/9521\" rel=\"noopener noreferrer\" target=\"_blank\">participate in the standard’s development here</a>.</p>",
      "author": "Kathy Pretz",
      "publishedAt": "2026-02-02T22:00:04.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "1ecfa22c-e43b-4d65-88ef-c2844f8a148c",
      "guid": "https://spectrum.ieee.org/ai-model-regulation",
      "title": "Don’t Regulate AI Models. Regulate AI Use",
      "link": "https://spectrum.ieee.org/ai-model-regulation",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/silhouettes-looking-at-screens-sit-behind-a-building-shape-with-the-scales-of-justice-in-a-digital-grid-patterned-setting.jpg?id=63516186&width=1245&height=700&coordinates=0%2C75%2C0%2C76\"/><br/><br/><p><span><span>At times, it</span> ca</span><span>n seem like </span><span>efforts to regulate and rein in </span><span>AI</span> <span>are </span><a href=\"https://spectrum.ieee.org/ai-ethics-governance\" target=\"_blank\">everything, everywhere, all at once</a><span>.</span></p><p><span><span>China issued the first </span></span><a href=\"https://carnegieendowment.org/research/2024/02/tracing-the-roots-of-chinas-ai-regulations?lang=en\" target=\"_blank\"><span><span>AI-specific regulations in 2021</span></span></a><span>. The focus is squarely on providers and content governance, enforced through platform control and recordkeeping requirements.</span> <br/> <br/><span><span>In Europe, the </span></span><a href=\"https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence\" target=\"_blank\"><span><span>European Union AI Act</span></span></a> <span>dates to</span> 2024<span>, but </span><span>the European Commission is already proposing </span><a href=\"https://digital-strategy.ec.europa.eu/en/library/digital-omnibus-ai-regulation-proposal\" target=\"_blank\"><span><span>updates and simplification</span></span></a><span>.</span></p><p><span><span>India charged its senior technical advisors with creating an AI governance system, which they </span></span><a href=\"https://static.pib.gov.in/WriteReadData/specificdocs/documents/2025/nov/doc2025115685601.pdf\" target=\"_blank\"><span><span>released</span></span></a><span> in </span><span>November</span> 2025.</p><p><span><span>In the United States,</span> the </span><span>states</span> are <a href=\"https://www.ncsl.org/financial-services/artificial-intelligence-legislation-database\" target=\"_blank\"><span><span>legislat</span><span>ing</span></span></a><span> and enforc</span><span>ing </span><span>their own AI rules </span><span>even as</span> the federal government <span>in 2025 </span><a href=\"https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/\" target=\"_blank\"><span><span>moved to prevent state action and loosen the reins</span></span></a><span>. </span></p><p><span><span>This leads to a critical question for American engineers and policymakers alike: What can the U.S. </span><span>actually enforce</span> in a way that reduces real-world harm? My answer: Regulate AI use, not the underlying models.</span></p><h2>Why model-centric regulation fails</h2><p><span><span>Proposals to license “frontier” training runs, restrict open weights, or require permission before publishing models, such as California’s </span></span><a href=\"https://legiscan.com/CA/text/SB53/id/3270002\" target=\"_blank\"><span><span>Transparency in Frontier Artificial Intelligence Act, </span></span></a><span><span>promise </span><span>control</span> but deliver theater. Model weights and code are digital artifacts; once released, by a lab, a leak, or a foreign competitor, they replicate at near-zero cost. You </span><span>can’t</span> unpublish weights, geofence research, or prevent distillation into smaller models. Trying to bottle up artifacts yields two bad outcomes: Compliant firms drown in paperwork, while reckless <span>actors</span> route around rules offshore, underground, or both.</p><p><span>In the United States, model-publication licensing also likely collides with speech law. Federal courts have treated software source code as protected expression, so any system that prevents the publication of AI models would be vulnerable to legal challenges. </span></p><p><span><span>“Do nothing” is <a href=\"https://spectrum.ieee.org/ai-regulation-worldwide\" target=\"_blank\">not an option</a> either. Without guardrails, we will keep seeing </span></span><a href=\"https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf\" target=\"_blank\"><span><span>deepfake scams</span></span></a><span><span>, automated fraud, and mass-persuasion campaigns until a headline catastrophe triggers a blunt response </span><span>optimized</span> for optics, not outcomes.</span></p><h2>A practical alternative: Regulate use, proportionate to risk</h2><p><span><span>A use-based regime classifies deployments by risk and scales obligations accordingly. Here is a workable template focused on keeping enforcement where systems </span><span>actually touch</span> people:</span></p><ol start=\"1\"><li><span><strong>Baseline: General-purpose consumer interaction</strong></span><span> (open-ended chat, creative writing, learning </span><span>assistance</span><span>, casual productivity). </span> <br/><span>Regulatory adherence: clear AI disclosure at point of interaction, published acceptable-use policies, technical guardrails preventing escalation into higher-risk tiers, and a mechanism for users to flag problematic outputs.</span> </li></ol><ol start=\"2\"><li><span><strong><span>Low-risk </span><span>assistance</span></strong></span><span> (drafting, summarization</span><span>, basic</span> productivity). <br/><span>Regulatory adherence</span><em><span><em>:</em></span></em> simple disclosure, baseline data hygiene. </li></ol><ol start=\"3\"><li><span><strong>Moderate-risk decision support affecting individuals</strong></span> (hiring triage, benefits screening, loan prequalification). <br/><span>Regulatory adherence</span><em><span><em>:</em></span></em> documented risk assessment, meaningful human oversight, and an “AI bill of materials” consisting of at least the model lineage, key evaluations, and mitigations. </li></ol><ol start=\"4\"><li><span><strong>High-impact uses in safety-critical contexts</strong></span> (clinical decision support, critical-infrastructure operations). <br/><span>Regulatory adherence</span><em><span><em>:</em></span></em><span> rigorous predeployment testing tied to the specific use, continuous monitoring, incident reporting, and, when </span><span>warranted</span><span>, authorization linked to validated performance.</span> </li></ol><ol start=\"5\"><li><span><strong>Hazardous dual-use functions</strong></span> (for example, tools to fabricate biometric voiceprints to defeat authentication). <br/><span>Regulatory adherence</span><em><span><em>:</em></span></em> <span>confine to</span> licensed facilities and verified operators; prohibit capabilities whose primary purpose is unlawful. <br/> <br/><span><h2>Close the loop at real-world choke points</h2></span><span>AI-enabled systems become real when they’re connected to users, money, infrastructure, and institutions, and that’s where regulators should focus enforcement: at the points of distribution (app stores and enterprise marketplaces), capability access (cloud and AI platforms), monetization (payment systems and ad networks), and risk transfer (insurers and contract counterparties).</span> <br/> <br/><span><span>For high-risk uses, we need to require identity binding for operators, capability gating aligned to the risk tier, and tamper-evident logging for audits and postincident review, paired with privacy protections. We need to demand evidence for deployer claims, </span><span>maintain</span> incident-response plans, report material faults, and provide human fallback. When AI use leads to damage, firms should have to show their work and face liability for harms.</span> <br/> <br/><span><span>This approach creates market dynamics that accelerate compliance. If </span></span><span><span>crucial business operations such as </span></span><span><span>procurement, access to cloud </span></span><span>services</span><span><span>, and insurance depend on proving that </span><span>you</span></span><span><span>’re</span> following the rules</span><span><span>, AI model developers will </span><span>build to</span> specifications buyers can check. That raises the safety floor for a</span><span><span>ll industry </span><span>players,</span> startups included, </span><span>without handing an advantage to a few large, licensed incumbents.</span> <br/> <br/><span><h2>The E.U. approach: How this aligns, where it differs</h2></span><span><span>This framework aligns with the E.U. AI Act in two important ways. First, it centers risk at the point of impact: The act’s “high-risk” categories include employment, education, access to essential services, and critical infrastructure, with life-cycle obligations and complaint rights. It also recognizes special treatment for broadly capable systems (GPAI) without pretending publication control is a safety strategy. My proposal for the United States differs in three </span><span>key ways</span><span>:</span></span> <br/> <br/><span>First, the U.S. must design for constitutional durability. Courts have treated source code as protected speech, and a regime that requires permission to publish weights or train a class of models starts to resemble prior restraint. A use-based regime of rules governing what AI operators can do in sensitive settings, and under what conditions, fits more naturally within the U.S. First Amendment doctrine than speaker-based licensing schemes.</span> <br/> <br/><span><span>Second, </span><span>the</span> E.U. can rely on platforms adapting to the precautionary rules it writes for its unified single market. The U.S. should accept that models will exist globally, both open and closed, and focus on where AI becomes actionable: app stores, enterprise platforms, cloud providers, enterprise identity layers, payment rails, insurers, and regulated-sector gatekeepers (hospitals, utilities, banks). Those are enforceable points where identity, logging, capability gating, and postincident accountability can be </span><span>required</span> without pretending we can “contain” software. They also span the many specialized U.S. agencies that may not be able to write higher-level rules broad enough to affect the whole AI ecosystem. Instead, the U.S. should regulate AI service choke points more explicitly than Europe does, to accommodate the different shape of its government and public administration. <br/> <br/><span>Third, the U.S. should add an explicit “dual-use hazard” tier. The E.U. AI Act is primarily a fundamental-rights and product-safety regime. The United States also has a national-security reality: Certain capabilities are dangerous because they scale harm (biosecurity, cyberoffense, mass fraud). A coherent U.S. framework should name that category and regulate it directly, rather than trying to fit it into generic “frontier model” licensing.</span> <br/> <br/><span><h2>China’s approach: What to reuse, what to avoid</h2></span><span><span>China has built a layered regime for public-facing AI. The “deep synthesis” rules (effective 10 January 2023) require conspicuous labeling of synthetic media and place duties on providers and platforms. The </span></span><span><strong>I</strong></span><span>nterim Measures for Generative AI (effective 15 August 2023) add registration and governance obligations for services offered to the public. Enforcement leverages platform control and algorithm filing systems.</span> <br/> <br/><span><span>The United States should not copy China’s state-directed control of AI viewpoints or information management; it is incompatible with U.S. values and would not survive U.S. constitutional scrutiny. The licensing of model publication is brittle in practice and, in the United States, </span><span>likely an</span> unconstitutional </span><span>form of censorship</span><span>.</span> <br/> <br/><span><span>But we can borrow two practical ideas from China. First, we should ensure trustworthy provenance and traceability for synthetic media. This involves mandatory labeling and </span><span>provenance</span> <span>forensic </span><span>tools. They give legitimate creators and platforms a reliable way to prove origin and integrity. When it is quick to check authenticity at scale, attackers lose the advantage of cheap copies or </span><span>deepfakes</span> and defenders regain time to detect, triage, and respond. Second, we should require </span><span>operators to</span> file their methods <span>and risk controls </span><span>with </span><span>regulators</span> for public-facing, high-risk services, like we do for other <span>safety-critical</span> projects. This should include due-process and transparency safeguards <span>appropriate to</span> liberal democracies along with clear responsibility for safety measures, data protection, and incident handling, especially for systems designed to manipulate emotions or build dependency, which already include gaming, role-playing, and <span>associated applications</span><span>.</span> <br/> <br/><span><h2>A pragmatic approach</h2></span><span><span>We cannot meaningfully regulate the development of AI in a world where artifacts copy in near real time and research flows fluidly across borders. But we can keep unvetted systems out of hospitals, payment systems, and critical infrastructure by regulating uses, not models; </span><span>enforcing at</span> choke points; and applying obligations that scale with risk. </span> <br/> <br/><span><span>Done right, this approach harmonizes with the E.U.’s outcome-oriented framework, channels U.S. federal and state innovation into a coherent baseline, and reuses China’s useful distribution-level controls while rejecting speech-restrictive licensing. We can write rules that protect people </span></span><span>and that still promote robust AI innovation.</span> <br/> </li></ol>",
      "author": "John deVadoss",
      "publishedAt": "2026-02-02T15:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "e09c65a4-a1e0-4ed9-8e2b-205a67ddba34",
      "guid": "https://spectrum.ieee.org/radio-telescope",
      "title": "LuSEE-Night: See You on the Far Side of the Moon",
      "link": "https://spectrum.ieee.org/radio-telescope",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/gold-lunar-lander-with-solar-panels-on-rocky-surface-insignia-visible-casting-a-shadow.png?id=63343460&width=1245&height=700&coordinates=0%2C249%2C0%2C250\"/><br/><br/><p>As a kid in the 1970s, I watched the Apollo moon missions on TV, drawn like a curious moth to the cathode-ray tube’s glow. The English band Pink Floyd blared through the speakers of my mom’s Oldsmobile Cutlass Supreme, beckoning us to the <a href=\"https://www.youtube.com/watch?v=QFdkM40KOhE\" rel=\"noopener noreferrer\" target=\"_blank\">dark side of the moon</a>.</p><p>The far side of the moon, the term most scientists prefer, is indeed dark (half the time), cold, and inhospitable. There’s regolith and a couple of Chinese landers—Chang’e 4 in January 2019 and <a data-linked-post=\"2667549562\" href=\"https://spectrum.ieee.org/china-moon-landing-uncrewed-chang-e6\" target=\"_blank\">Chang’e 6</a> in June 2024—and not much else. That could change in about a year, as Contributing Editor Ned Potter reports in “<a href=\"https://spectrum.ieee.org/lunar-radio-telescope\" target=\"_blank\">The Quest to Build a Telescope That Can Hear the Cosmic Dark Ages</a>.” Firefly Aerospace’s <a href=\"https://fireflyspace.com/missions/blue-ghost-mission-2/\" rel=\"noopener noreferrer\" target=\"_blank\">Blue Ghost Mission 2</a> with the LuSEE-Night radio telescope aboard will attempt to become the third successful mission to land there.</p><p>The moon’s far side is the perfect place for such a telescope. The same RF waves that carried images of Neil Armstrong setting foot on the lunar surface, Roger Waters’s voice, and hundreds of Ned Potter’s space and science segments for the U.S. broadcast networks CBS and ABC interfere with terrestrial radio telescopes. If your goal is to detect the extremely faint and heavily redshifted signals of neutral hydrogen from the cosmic Dark Ages, you just can’t do it from Earth. This epoch is so-called because we Earthlings have yet to sense anything from this time period, which started about 380,000 years after the big bang and lasted 200 million to 400 million years. The far side of the moon may be a terrible place to live, but it’s shielded from all the noise of Earth, making it the ideal spot to place a radio telescope.</p><p>As Potter emphasized to me recently, LuSEE-Night won’t listen for a signal from Dark Ages hydrogen directly. “Will the hydrogen from the Dark Ages send a signal? No,” says Potter. “But all that hydrogen out there may absorb a little bit of energy from the cosmic microwave background, interfering with that even more distant remnant of the big bang.”</p><p>The far side may not stay quiet for much longer. Several countries, including China, India, Japan, Russia, South Korea, the United Arab Emirates, and the United States, are making slow but steady progress toward establishing a lunar presence. As they do so, they’ll place more relay satellites into orbit around the moon to support exploratory activities as well as moon bases planned for the next decade and beyond. That means the window on a noise-free far side is closing. LuSEE-Night, a project 40 years in the making, might just get there in the nick of time.</p><p>Potter is tracking emerging protocols that could preserve the far side’s electromagnetic silence even as such efforts advance. Radio astronomers he’s talked to have shared ideas about how to prevent this emerging problem from turning into a crisis. “There are no bad guys in this story, at least not yet,” says Potter. “But there are a lot of well-meaning people who could complicate the picture a great deal if they don’t know that there’s a picture to complicate.”</p><p>It’s a busy time for moon missions. In addition to Blue Ghost Mission 2, the Chinese are sending Chang’e 7 to the moon’s south pole, while NASA’s <a href=\"https://spectrum.ieee.org/artemis-2\" target=\"_self\">Artemis II</a> is scheduled to enter the first of three launch windows this month. Artemis II will be the first mission to put humans into lunar orbit since the last Apollo mission in 1972. And <em><em>IEEE</em></em> <em><em>Spectrum</em></em> readers will enjoy a front row seat, thanks to the enterprising reporting of a true legend in the business, <a href=\"https://spectrum.ieee.org/u/ned-potter\" target=\"_self\">our own Ned Potter</a>.</p><p><em>This article appears in the February 2026 print issue as “<span>See You on the Far Side of the Moon</span>.”</em></p>",
      "author": "Harry Goldstein",
      "publishedAt": "2026-02-01T14:00:01.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "30cf5bd8-eb06-4f88-be72-05c93febb92a",
      "guid": "https://spectrum.ieee.org/assistive-technology-macgyver",
      "title": "How YouTube and Adhesive Tape Are Disrupting Assistive Technology",
      "link": "https://spectrum.ieee.org/assistive-technology-macgyver",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/a-custom-phone-mount-attached-to-a-vehicle-s-auxiliary-controls-for-accessibility.jpg?id=63527438&width=1245&height=700&coordinates=0%2C177%2C0%2C177\"/><br/><br/><p><span>Assistive technology is expensive, and many people with disabilities live on fixed incomes. Disabled assistive tech users also must contend with equipment that was often designed without any capacity to be repaired or modified. But assistive tech users ultimately need the functionality they need—a wheelchair that isn’t constantly needing to be charged, perhaps, or a hearing aid that doesn’t amplify all background noise equally. Assistive tech “<a data-linked-post=\"2650278119\" href=\"https://spectrum.ieee.org/ieee-joins-the-maker-movement\" target=\"_blank\">makers</a>,” who can <a href=\"https://spectrum.ieee.org/why-hire-engineers-with-disabilities-theyre-practiced-problem-solvers\" target=\"_blank\">hack and modify existing assistive tech</a>, have always been in high demand. </span></p><p><span><a href=\"https://iod.unh.edu/person/therese-willkomm\" target=\"_blank\">Therese Willkomm</a>, emeritus professor of occupational therapy at the University of New Hampshire, has <a href=\"https://www.goodreads.com/author/list/8275489.Therese_Willkomm\" target=\"_blank\">written three books</a> cataloging her more than 2,000 assistive technology hacks. Willkomm says she aims to keep her assistive tech hacks costing less than five dollars. </span></p><p><span>She’s come to be known internationally as the “<a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/MacGyvering\" target=\"_blank\">MacGyver</a> of Assistive Technology” and has presented more than 600 workshops and assistive tech maker days across 42 states and 14 countries.</span></p><p><span><em>IEEE Spectrum </em>sat down with Willkomm ahead of her latest <a href=\"https://www.atia.org/atia-maker-day/\" target=\"_blank\">assistive tech Maker Day workshop</a>, on Saturday, 31 January, at the <a href=\"https://www.atia.org/conference/\" target=\"_blank\">Assistive Technology Industry Association</a> (ATIA) conference in Orlando, Florida. Over the course of the conversation, she discussed the evolution of assistive technology over 40 years, the urgent need for affordable communication devices, and why the DIY movement matters now more than ever.</span></p><p><em><strong><em>IEEE Spectrum: </em></strong></em><strong>What got you started in assistive technology?</strong></p><p><strong>Therese Willkomm: </strong>I grew up in Wisconsin, where my father had a machine shop and worked on dairy and hog farms. At age 10, I started building and making things. A cousin was in a farm accident and needed modifications to his tractor, which introduced me to welding. In college, I enrolled in vocational rehabilitation and learned about rehab engineering—assistive technology wasn’t coined until 1988 with the <a href=\"https://www.congress.gov/bill/100th-congress/senate-bill/2561\" target=\"_blank\">Technology-Related Assistance Act</a>. In 1979, <a href=\"https://ischool.umd.edu/directory/gregg-vanderheiden/\" target=\"_blank\">Gregg Vanderheiden</a> came to the University of Wisconsin-Stout and demonstrated creative things with garage door openers and communication devices. I thought, “Wow, this would be an awesome career path—designing and fabricating devices and worksite adaptations for people with disabilities to go back to work and live independently.” I haven’t looked back.</p><p><strong>You’ve created over 2,000 assistive technology solutions. What’s your most memorable one?</strong></p><p><strong>Willkomm:</strong> A device for castrating pigs with one hand. We figured out a way to design a device that fit on the end of the hog crate that was foot-operated to hold the hind legs of the pig back so the procedure could be done with one hand.</p><h3>Assistive Technology’s Changing Landscape </h3><p><strong>How has assistive technology evolved over the decades?</strong></p><p><strong>Willkomm: </strong>In the 1980s, we fabricated devices from wood and early electronics. I became a [<a href=\"https://www.resna.org/\" target=\"_blank\">Rehabilitation Engineering and Assistive Technology Society of North America</a>, a.k.a. RESNA] member in 1985. The <a href=\"https://www.congress.gov/bill/100th-congress/senate-bill/2561#:~:text=passed%20Senate%2C%20amended)-,Technology%2DRelated%20Assistance%20for%20Individuals%20With%20Disabilities%20Act%20of%201988,of%20all%20ages%20with%20disabilities.\" target=\"_blank\">1988 Technology-Related Assistance Act</a> was transformational—all 50 states finally got funding to support assistive technology and needs in rural areas. Back in the ‘80s, we were soldering and making battery interrupters and momentary switches for toys, radios, and music. Gregg was doing some things with communication. There were <a href=\"https://prc-saltillo.com/why-prc-saltillo/history\" target=\"_blank\">Prentke Romich</a> communication devices. Those were some of the first electronic assistive technologies.</p><p>The early 1990s was all about mobile rehab engineering. Senator Bob Dole <a href=\"https://www.eastersealstech.com/2014/08/27/crucial-part-creative-solution/#:~:text=Most%20of%20the%20grants%20funded,Willkomm%20said.\" target=\"_blank\">gave me a $50,000 grant</a> to fund my first mobile unit. That mobile unit had all my welding equipment, all my fabrication equipment, and I could drive farm to farm, set up outside right in front of the tractor, and fabricate whatever needed to be fabricated. Then, around 1997, there were cuts in the school systems. Mobile units became really expensive to operate. We started to look at more efficient ways of providing assistive technology services. With the Tech Act, we had demonstration sites where people would come and try out different devices. But people had to get in a car, drive to a center, get out, find parking, come into the building—a lot of time was being lost.</p><p>In the 2000s, more challenges with decreased funding. I discovered that with a Honda Accord and those crates you get from Staples, you could have your whole mobile unit in the trunk of your car because of advances in materials. We could make battery interrupters and momentary switches without ever having to solder. We can make switches in 28 seconds, battery interrupters in 18 seconds. When COVID happened, we had to pivot—do more virtual, ship stuff out to people. We were able to serve more individuals during COVID than prior to COVID because nobody had to travel.</p><p><strong>How do you keep costs under five dollars?</strong></p><p><strong>Willkomm:</strong> I aim for five dollars or less. I get tons of corrugated plastic donated for free, so we spend no money on that. Then there’s <a href=\"https://scapaindustrial.com/\" target=\"_blank\">Scapa Tape</a>—a very aggressive double-sided foam tape that costs five cents a foot. If you fabricate something and it doesn’t work out, and you have to reposition, you’re out a nickel’s worth of material. Buying Velcro in bulk helps too. Then<a href=\"https://instamorph.com/\" target=\"_blank\"> Instamorph</a>—it is non-toxic, biodegradable. You can reheat it, reform it, in five minutes or less up to six times. I’ve created about 132 different devices just using Instamorph. A lot of things I make out of Instamorph don’t necessarily work. I have a bucket, and I reuse that Instamorph. We can get six, seven devices out of reusable Instamorph. That’s how we keep it under five dollars.</p><p><strong>What key legislation impacts assistive technology?</strong></p><p><strong>Willkomm: </strong>Definitely the Technology-Related Assistance Act. In the school system, however, it only says “Did you <em>consider</em> assistive technology?” So that legislation really needs to be beefed up. The third piece of legislation I worked on was the <a href=\"https://www.nifa.usda.gov/grants/programs/agrability\" target=\"_blank\">AgrAbility</a> legislation to fund assistive technology consultations and technical assistance for farmers and ranchers. The latest Technology-Related Assistance Act was <a href=\"https://ataporg.org/at-act-info/\" target=\"_blank\">reauthorized in 2022</a>. Not a whole lot of changes—it’s still assistive technology device demonstrations and loans, device reuse, training, technical assistance, information and awareness. The other thing is<a href=\"https://acl.gov/about-acl/about-national-institute-disability-independent-living-and-rehabilitation-research\" rel=\"noopener noreferrer\" target=\"_blank\"> NIDILRR</a>—National Institute on Independent Living and Rehabilitation Research, funded under [the U.S. Department of Health and Human Services, a.k.a. <a href=\"https://en.wikipedia.org/wiki/United_States_Department_of_Health_and_Human_Services\" target=\"_blank\">HHS</a>]. Funding the rehab engineering centers was pretty significant in advancing the field because these were huge, multimillion-dollar centers dedicated to core areas like communication and employment. Now there’s a new one out on artificial intelligence.</p><h3>A Vision for a Better Assistive Tech Future </h3><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25\" data-rm-resized-container=\"25%\" rel=\"float: left;\" style=\"float: left;\"> <img alt=\"Person wearing a floral-patterned, white shirt and beaded necklace outdoors.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"53bb11a4a652b400740d474077aae6e1\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"1a182\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/person-wearing-a-floral-patterned-white-shirt-and-beaded-necklace-outdoors.jpg?id=63527474&width=980\"/> <small class=\"image-media media-caption\" data-gramm=\"false\" data-lt-tmp-id=\"lt-533308\" placeholder=\"Add Photo Caption...\" spellcheck=\"false\">With over 2,000 hacks to improve usability of assistive technologies, veteran DIY maker Therese Willkomm has earned the moniker “the MacGyver of assistive tech.” </small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">Therese Willkomm</small></p><p><strong>What deserves more focus in your field?</strong></p><p><strong>Willkomm:</strong> The supply-and-demand problem. It all comes down to time and money. We have an elderly population that continues to grow, and a disability population that continues to grow—high demand, high need for assistive technology, yet the resources available to meet that need are limited. A few years back, the <a href=\"https://www.christopherreeve.org/\" target=\"_blank\">Christopher & Dana Reeve Foundation</a> had a competition. I submitted a proposal similar to the <a href=\"https://topmealkitdelivery.com/compare-top/?utm_source=google&keyword=how%20does%20blue%20apron%20work&campaignid=19738344647&adgroupid=147312616278&targetid=kwd-119910014195&device=c&loc_physical=9001648&net_type=g&mt=e&gad_source=1&gad_campaignid=19738344647&gbraid=0AAAAACWvR1todciLbxMtVpO1D33ERjChb&gclid=CjwKCAiAssfLBhBDEiwAcLpwfhQhzA6bmvh2gvcKuF296EMnhDCD_JjnPyNwHec6UsC5F7V2brgQGhoCabYQAvD_BwE\" target=\"_blank\">Blue Apron approach</a>. People don’t have supplies at their house. They can’t buy two inches of tape—they have to buy a whole roll. They can’t buy one foot of corrugated plastic—they’ve got to buy an 18-by-24 sheet or wait till it gets donated.</p><p>With my <a href=\"https://www.goodreads.com/book/show/58523017-assistive-technology-solutions-in-minutes-book-iii---make-stuff-and-love\" target=\"_blank\">third book</a>, I created solutions with QR codes showing videos on how to make them. I used Christopher Reeve Foundation funding to purchase supplies. With Blue Apron, somebody wants to make dinner and a box arrives with a chicken breast, potato, vegetables, and recipe. I thought, what if we could apply that to assistive technology? Somebody needs something, there’s a solution out there, but they don’t have the money or the time—how can we quickly put it in a box and send it to them? People who attended my workshops didn’t have to spend money on materials or waste time at the store. They’d watch the video and assemble it.</p><p>But then there were people who said, “I do not have even five minutes in the school day to stop what I’m doing to make something.” So we found volunteers who said, “Hey, I can make slant boards. I can make switches. I can adapt toys.” You have people who want to build stuff and people who need stuff. If you can deal with the time and money issue, anything’s possible to serve more people and provide more devices.</p><p><strong>What’s your biggest vision for the future?</strong></p><p><strong>Willkomm:</strong> I’m very passionate about communication. December 15 was the <a href=\"https://www.archives.gov/founding-docs/bill-of-rights-transcript\" target=\"_blank\">passage in 1791 of our First Amendment</a>, freedom of speech. Yet people with communication impairments are denied their basic right of freedom of speech because they don’t have an affordable communication device, or it takes too long to program or learn. I just wish we could get better at designing and fabricating affordable communication devices, so everybody is awarded their First Amendment right. It shouldn’t be something that’s nice to have—it’s something that’s needed to have. When you lose your leg, you’re fitted with a prosthetic device, and insurance covers that. Insurance should also cover communication devices and all the support services needed. With voice recognition and computer-generated voices, there are tremendous opportunities in assistive technology for communication impairments that need to be addressed.</p><p><strong>What should </strong><em><strong><em>IEEE Spectrum</em></strong></em><strong> readers take away from this conversation?</strong></p><p><strong>Willkomm: </strong>There’s tremendous need for this skill set—working in conjunction with AI and material sciences and the field of assistive technology and rehab engineering. I’d like people to look at opportunities to volunteer their time and also to pursue careers in the field of specialized rehab engineering.</p><p><strong>How are DIY approaches evolving with new technologies?</strong></p><p><strong>Willkomm:</strong> What we’re seeing at maker fairs is more people doing <a data-linked-post=\"2655429871\" href=\"https://spectrum.ieee.org/3d-printed-rockets-india-agnikul\" target=\"_blank\">3D printing</a>, switch-access controls, and these five-minute approaches. There has to be a healthy balance between what we can do with or without electronics. If we need something programmed with electronics, absolutely—but is there a faster way?</p><p>The other thing that’s interesting is skill development. You used to have to go to college for four, six, eight years. With YouTube, you can learn so much on the internet. You can develop skills in things you never thought were possible without a four-year degree. There’s basic electronic stuff you can absolutely learn without taking a course. I think we’re going to have more people out there doing hacks, asking “What if I change it this way?” We don’t need to have a switch. </p><p>We need to look at the person’s body and how that body interacts with the electronic device interface so it requires minimal effort—whether it be eye control or motion control. Having devices that predict what you’re going to want next, that are constantly listening, knowing the way you talk. I love the fact that AI looks at all my emails and creates this whole thing like “Here’s how I’d respond.” I’m like, yeah, that’s exactly it. I just hit select, and I don’t have to type it all out. It speeds up communication. We’re living in exciting times right now.</p><p><em>This article was supported by the <a href=\"https://spectrum.ieee.org/tag/ieee-foundation\" target=\"_self\">IEEE Foundation</a> and a John C. Taenzer fellowship grant.</em></p>",
      "author": "Jason Hahr",
      "publishedAt": "2026-01-31T15:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "d9050190-5732-48d8-b925-559391093c31",
      "guid": "https://spectrum.ieee.org/explore-stratosphere-diy-pico-balloon",
      "title": "Explore the Stratosphere With a DIY Pico balloon",
      "link": "https://spectrum.ieee.org/explore-stratosphere-diy-pico-balloon",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/a-printed-circuit-board-attached-to-two-small-solar-panels-hangs-beneath-a-balloon.png?id=63339900&width=1245&height=700&coordinates=0%2C566%2C0%2C567\"/><br/><br/><p>There’s an interesting development in amateur ballooning: using so-called <a href=\"https://en.wikipedia.org/wiki/Superpressure_balloon\" rel=\"noopener noreferrer\" target=\"_blank\">superpressure balloons</a>, which float high in the atmosphere indefinitely rather than simply going up and up and then popping like a normal weather balloon. Superpressure balloons can last for months and travel long distances, potentially circumnavigating the globe, all the while reporting their position.</p><p>You might imagine that an undertaking like this would be immensely difficult and cost thousands of dollars. In fact, you can build and launch such a balloon for about the cost of a fancy dinner out. You just have to think small! That’s why amateur balloonists call them pico balloons.</p><p>The payload of a pico balloon is so light (between 12 to 30 grams) that you can use a large Mylar party balloon filled with helium to lift it. They’re also inexpensive; that’s important because you won’t get your payload back. And because such diminutive payloads don’t pose a danger to aircraft, they aren’t subject to the many rules and restrictions on free-floating balloons that carry more mass.</p><p>The essential advances that made pico ballooning possible were figuring out how to track a balloon no matter where in the world it might be and how to power such tiny payloads. A lot of folks worked on these challenges and came up with good solutions that aren’t hard or expensive to reproduce.</p><h2>What is WSPR?</h2><p>Amazingly, the global tracking of the balloon’s telemetry is done without satellites. Instead, pico balloonists take advantage of an <a href=\"https://spectrum.ieee.org/tag/amateur-radio\" target=\"_blank\">amateur-radio</a> network called <a href=\"https://en.wikipedia.org/wiki/WSPR_(amateur_radio_software)\" rel=\"noopener noreferrer\" target=\"_blank\">WSPR</a> (Weak Signal Propagation Reporter), a protocol developed by a rather famous ham-radio enthusiast—<a href=\"https://en.wikipedia.org/wiki/Joseph_Hooton_Taylor_Jr.\" rel=\"noopener noreferrer\" target=\"_blank\">Joseph Hooton Taylor Jr</a>., one of the two scientists awarded the 1993 Nobel Prize in Physics for discovering binary pulsars.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"Major components of a pico balloon payload.\" class=\"rm-shortcode\" data-rm-shortcode-id=\"e0a1d92061ec68d4b64e4ad86f82b7fe\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"176da\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/major-components-of-a-pico-balloon-payload.png?id=63339919&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">A Raspberry Pi Pico microcontroller [top left] is soldered directly to a daughterboard consisting of a high-frequency transmitter and a GPS module [bottom left], which are all powered by solar panels [right].</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">James Provost</small></p><p>WSPR was designed to monitor signal-propagation conditions for different radio bands—useful information if you’re a ham trying to make distant contacts. WSPR can also record low-power balloon-telemetry signals. WSPR is very low bandwidth—less than 10 bits per minute—but it does the job. A worldwide network of radio amateurs receives these WSPR signals and reports them publicly over the internet, which gives picoballoonists a way to track their flights. You need at least a <a href=\"https://www.arrl.org/upgrading-to-a-general-license\" target=\"_blank\">general-class</a> ham-radio license to launch a pico balloon, as one is required to transmit on the bands used for long-distance telemetry.</p><p>The pico balloon payload I chose to build is based on the aptly named US $4 <a href=\"https://www.adafruit.com/product/4864\" target=\"_blank\">Raspberry Pi Pico board</a>, with a solder-on daughterboard that contains a <a href=\"https://spectrum.ieee.org/tag/gps\" target=\"_blank\">GPS</a> receiver and transmitter. The folks who developed this daughterboard and associated software (to create what they call the <a href=\"https://traquito.github.io/tracker/\" target=\"_blank\">Jetpack WSPR Tracker</a>) have done a fantastic job of making their work easy to reproduce.</p><p>You could, in principle, power the Jetpack tracker with batteries, but in practice it would be impossible to keep them warm in the stratosphere, where average temperatures can be as low as –51 °C. Instead, the tracker runs off two lightweight solar modules. At night, it gracefully powers down. When the sun rises high enough in the morning, the tracker powers up and starts transmitting again.</p><p class=\"pull-quote\"><span>My first pico balloon made it only halfway across the Atlantic before going silent.</span></p><p>I had five Jetpack boards custom-manufactured in China for just $39. The cost nearly doubled after adding shipping and tariff charges. Still that’s really cheap, even when you add the cost of the Raspberry Pi ($4), <a href=\"https://www.amazon.com/dp/B0F28ZWPY6\" target=\"_blank\">the party balloon</a> ($10 for two), the helium ($10 at my local supermarket), and the two <a href=\"https://www.amazon.com/PowerFilm-MPT6-75-Module-Flexible-Thin-Film/dp/B002MFGD16\" target=\"_blank\">solar modules</a> ($7 each).</p><p>The biggest sticking point I had with the Jetpack design was the liberties it takes with spurious emissions from its transmitter. Federal Communications Commission (FCC) regulations call for spurious emissions to be at least 43 decibels below the power of the transmitted signal. But my transmitter had strong unwanted emissions at odd harmonics of the fundamental frequency. (That’s because the transmitter is a<a href=\"https://cdn.sparkfun.com/assets/3/e/a/9/a/Si5351-datasheet.pdf\" rel=\"noopener noreferrer\" target=\"_blank\"> Si5351A</a> temperature-controlled oscillator, which outputs a square wave, not a sinusoid.) Taking measurements, I could see that the third harmonic at 42 megahertz was only 25 dB quieter than the 14-MHz fundamental of my WSPR signal’s frequency. </p><p class=\"shortcode-media shortcode-media-rebelmouse-image\"> <img alt=\"A map showing a track from North Carolina in the United States across the Atlantic and the Iberean peninsula to the Mediterranean. \" class=\"rm-shortcode\" data-rm-shortcode-id=\"52b80f09402d5f29a30a443ba838cd07\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"77213\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/a-map-showing-a-track-from-north-carolina-in-the-united-states-across-the-atlantic-and-the-iberean-peninsula-to-the-mediterranea.png?id=63339932&width=980\"/> <small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\">As of press time, the WSPR network had tracked my balloon from the Eastern United States to the Mediterranean coast. </small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\">James Provost</small></p><p>In practical terms, this shouldn’t create any noticeable interference, given that this transmitter puts out milliwatts at most and floats miles away from the nearest receiver. Still, I wanted to be fully compliant with FCC regulations, so I added traps to the antenna—simple circuit elements that hams use to allow a single antenna to work on multiple bands by altering how the antenna resonates at different frequencies. Each trap was made of a small inductor (four 5-millimeter-diameter loops of No. 32 magnet wire) in parallel with a 220-picofarad capacitor. I tuned them with the help of a <a href=\"https://nanovna.com/\" target=\"_blank\">NanoVNA</a> signal analyzer by stretching the loops apart slightly. I attached the traps directly to the tracker board, so that they quashed the spurious 42-MHz emissions at the source. That worked well and added only 0.3 grams of weight.</p><p>With my payload complete, I partially filled my balloon with helium. You want the balloon to hold just a little more gas than it takes to lift the payload off the ground. This will give the helium room to expand as the balloon climbs to its final altitude.</p><p>My first pico balloon, launched from a park near my home in North Carolina, made it only halfway across the Atlantic before going silent. My second went up and was never heard from again. The third was indeed the charm. It crossed the Iberian Peninsula and at the time of this writing is somewhere over the Mediterranean at an altitude of nearly 12 kilometers. With any luck, <a href=\"https://traquito.github.io/search/spots/dashboard/?band=20m&channel=104&callsign=N4LVD&dtGte=2026-01-01\" target=\"_blank\">it might go on</a> to orbit the planet.</p><p>I’m a little puzzled about the balloons’ telemetry messages received on the WSPR network, as they have been few and far between. My best guess is that power from the horizontal solar panels I’m using is marginal, with the winter sun being so low in the sky. That’s something I should have thought about before launching the first balloon just 24 hours after the winter solstice!</p><p><em>This article appears in the February 2026 print issue as “<span>Long-Duration Amateur Ballooning</span>.”</em></p>",
      "author": "David Schneider",
      "publishedAt": "2026-01-31T14:00:02.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "d72badfd-0264-45c1-b19e-20bc321cbd5f",
      "guid": "https://spectrum.ieee.org/poetry-for-engineers-ode",
      "title": "Ode to Very Small Devices",
      "link": "https://spectrum.ieee.org/poetry-for-engineers-ode",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/anthropomorphized-miniature-gadgets-standing-on-the-heads-of-two-hex-bolts.jpg?id=63525887&width=1245&height=700&coordinates=0%2C128%2C0%2C129\"/><br/><br/><p>As fairies for the Irish or leeks for Welsh,<br/><span>it’s the secret lives of small hidden machines,<br/></span><span>their junctures, and networks that inspire me:<br/></span><span>Mystic hidden functionaries that make<br/></span><span>our made world live, brave little servo motors,<br/></span><span>whose couplers, whose eccentric fire-filled<br/></span><span>sensors are encased in bakelite with brass<br/></span><span>screws, who stare with red eyes, who gauge moisture,<br/></span><span>who notice tiny motions and respond,<br/></span><span>whose cooling fans call out in white-noise<br/></span><span>registers like older folk singers–I can<br/></span><span>almost hear their earlier songs, their strong voices<br/></span><span>now yelps, their thumps, their throbs, their hum, their chant–,<br/></span><span>they click, they whir, they are sent spinning<br/></span><span>inside like teen girls giggling over boy bands.<br/></span><span>Most of all: ones waiting silently, concealing<br/></span><span>the surprise of their purpose, tasks not yet known,<br/></span><span>their true natures found only in connections.</span></p><p>Those that listen, those that speak,<br/><span>those that control cool and heat,<br/></span><span>those that open doors, those that lock<br/></span><span>all the things that we’ve forgot,<br/></span><span>those that hide, those that disclose<br/></span><span>those embedded in our clothes<br/></span><span>those in our ears, those in our hearts<br/></span><span>those that bring together, those a part<br/></span><span>of divisions, those like birds,<br/></span><span>like parrots that complete our words,<br/></span><span>those like fish, those that entrap,<br/></span><span>those that free, those that freely flap<br/></span><span>in fierce winds, those that replace<br/></span><span>what we have lost, those that see<br/></span><span>at night, in fog, in brightness, in fear,<br/></span><span>those that show what we hold dear,<br/></span><span>those that tempt, those that repel,<br/></span><span>those that buy and those that sell,<br/></span><span>those that keep us alive, those that<br/></span><span>don’t, won’t, couldn’t and cannot.</span></p><p>Parts of one mind, not mine, blunt orchestra<br/><span>of information, bundles of feelers<br/></span><span>reaching out to touch us, teach us, guide us<br/></span><span>to form better futures better understood.<br/></span><span>May your sounds, your chimes, your silence calm us.<br/></span><span>May your tender tendrils touch what we seek.<br/></span><span>Small parts becoming one being intertwined,<br/></span><span>a world in itself, remind us to be kind. </span></p>",
      "author": "Paul Jones",
      "publishedAt": "2026-01-30T19:02:06.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "2072f03b-de8d-4a85-82c1-44543b9722e2",
      "guid": "https://spectrum.ieee.org/andrew-ng-data-centric-ai",
      "title": "Andrew Ng: Unbiggen AI",
      "link": "https://spectrum.ieee.org/andrew-ng-data-centric-ai",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1245&height=700&coordinates=0%2C0%2C0%2C474\"/><br/><br/><p><strong><a href=\"https://en.wikipedia.org/wiki/Andrew_Ng\" rel=\"noopener noreferrer\" target=\"_blank\">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href=\"https://stanfordmlgroup.github.io/\" rel=\"noopener noreferrer\" target=\"_blank\">Stanford University</a>, cofounded <a href=\"https://research.google/teams/brain/\" rel=\"noopener noreferrer\" target=\"_blank\">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href=\"https://ir.baidu.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>\n\tNg’s current efforts are focused on his company \n\t<a href=\"https://landing.ai/about/\" rel=\"noopener noreferrer\" target=\"_blank\">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. He has also become something of an evangelist for what he calls the <a href=\"https://www.youtube.com/watch?v=06-AZXmwHjo\" target=\"_blank\">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.\n</p><p>\n\tAndrew Ng on...\n</p><ul>\n<li><a href=\"#big\">What’s next for really big models</a></li>\n<li><a href=\"#career\">The career advice he didn’t listen to</a></li>\n<li><a href=\"#defining\">Defining the data-centric AI movement</a></li>\n<li><a href=\"#synthetic\">Synthetic data</a></li>\n<li><a href=\"#work\">Why Landing AI asks its customers to do the work</a></li>\n</ul><p>\n<strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href=\"https://spectrum.ieee.org/deep-learning-computational-cost\" target=\"_self\">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>\n</p><p>\n<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.\n</p><p>\n<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>\n</p><p>\n<strong>Ng:</strong> This is a term coined by <a href=\"https://cs.stanford.edu/~pliang/\" rel=\"noopener noreferrer\" target=\"_blank\">Percy Liang</a> and <a href=\"https://crfm.stanford.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href=\"https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business\" target=\"_self\">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.\n</p><p>\n<strong>What needs to happen for someone to build a foundation model for video?</strong>\n</p><p>\n<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.\n</p><p>\n\tHaving said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.\n</p><p>\n<a href=\"#top\">Back to top</a>\n</p><p>\n<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>\n</p><p>\n<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href=\"https://research.google/teams/brain/\" rel=\"noopener noreferrer\" target=\"_blank\">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.\n</p><p class=\"pull-quote\">\n\t“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>\n\t—Andrew Ng, CEO & Founder, Landing AI\n</p><p>\n\tI remember when my students and I published the first \n\t<a href=\"https://nips.cc/\" rel=\"noopener noreferrer\" target=\"_blank\">NeurIPS</a> workshop paper advocating using <a href=\"https://developer.nvidia.com/cuda-zone\" rel=\"noopener noreferrer\" target=\"_blank\">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.\n</p><p>\n<strong>I expect they’re both convinced now.</strong>\n</p><p>\n<strong>Ng:</strong> I think so, yes.\n</p><p>\n\tOver the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”\n</p><p>\n<a href=\"#top\">Back to top</a>\n</p><p>\n<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>\n</p><p>\n<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.\n</p><p>\n\tWhen I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.\n</p><p>\n\tThe data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a \n\t<a href=\"https://neurips.cc/virtual/2021/workshop/21860\" rel=\"noopener noreferrer\" target=\"_blank\">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.\n</p><p>\n<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>\n</p><p>\n<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.\n</p><p>\n<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>\n</p><p>\n<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href=\"https://developers.arcgis.com/python/guide/how-retinanet-works/\" rel=\"noopener noreferrer\" target=\"_blank\">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.\n</p><p class=\"pull-quote\">\n\t“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>\n\t—Andrew Ng\n</p><p>\n\tFor example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.\n</p><p>\n<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>\n</p><p>\n<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href=\"https://www.cs.princeton.edu/~olgarus/\" rel=\"noopener noreferrer\" target=\"_blank\">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href=\"https://neurips.cc/virtual/2021/invited-talk/22281\" rel=\"noopener noreferrer\" target=\"_blank\">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href=\"https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/\" rel=\"noopener noreferrer\" target=\"_blank\">Datasheets for Datasets</a> also seem like an important piece of the puzzle.\n</p><p>\n\tOne of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.\n</p><p>\n<strong>When you talk about engineering the data, what do you mean exactly?</strong>\n</p><p>\n<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href=\"https://jupyter.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.\n</p><p>\n\tFor example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.\n</p><p>\n<a href=\"#top\">Back to top</a>\n</p><p>\n<strong>What about using synthetic data, is that often a good solution?</strong>\n</p><p>\n<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href=\"https://tensorlab.cms.caltech.edu/users/anima/\" rel=\"noopener noreferrer\" target=\"_blank\">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.\n</p><p>\n<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>\n</p><p>\n<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.\n</p><p class=\"pull-quote\">\n\t“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>\n\t—Andrew Ng\n</p><p>\n\tSynthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.\n</p><p>\n<a href=\"#top\">Back to top</a>\n</p><p>\n<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href=\"https://landing.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>\n</p><p>\n<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href=\"https://landing.ai/platform/\" rel=\"noopener noreferrer\" target=\"_blank\">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.\n</p><p>\n\tOne of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.\n</p><p>\n<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>\n</p><p>\n<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.\n</p><p>\n\tIn the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?\n</p><p>\n<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>\n</p><p>\n<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.\n</p><p>\n<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>\n</p><p>\n<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.\n</p><p>\n<a href=\"#top\">Back to top</a>\n</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>",
      "author": "Eliza Strickland",
      "publishedAt": "2022-02-09T15:31:12.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "f0782d58-a6ef-4732-8551-745bec7d9050",
      "guid": "https://spectrum.ieee.org/ai-chip-design-matlab",
      "title": "How AI Will Change Chip Design",
      "link": "https://spectrum.ieee.org/ai-chip-design-matlab",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1245&height=700&coordinates=0%2C156%2C0%2C156\"/><br/><br/><p>The end of <a href=\"https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing\" target=\"_self\">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href=\"https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip\" target=\"_self\">miniaturize transistors</a> and <a href=\"https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors\" target=\"_self\">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href=\"https://spectrum.ieee.org/processing-in-dram-accelerates-ai\" target=\"_self\">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href=\"https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests\" target=\"_self\">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href=\"https://www.linkedin.com/in/heather-gorr-phd\" rel=\"noopener noreferrer\" target=\"_blank\">Heather Gorr</a>, senior product manager for <a href=\"https://www.mathworks.com/\" rel=\"noopener noreferrer\" target=\"_blank\">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" data-rm-resized-container=\"25%\" style=\"float: left;\">\n<img alt=\"Portrait of a woman with blonde-red hair smiling at the camera\" class=\"rm-shortcode rm-resized-image\" data-rm-shortcode-id=\"1f18a02ccaf51f5c766af2ebc4af18e1\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"2dc00\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980\" style=\"max-width: 100%\"/>\n<small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\" style=\"max-width: 100%;\">Heather Gorr</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\" style=\"max-width: 100%;\">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href=\"https://en.wikipedia.org/wiki/Model_order_reduction\" rel=\"noopener noreferrer\" target=\"_blank\">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href=\"https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html\" rel=\"noopener noreferrer\" target=\"_blank\">parameter sweeps</a>, your optimizations, your <a href=\"https://www.ibm.com/cloud/learn/monte-carlo-simulation\" rel=\"noopener noreferrer\" target=\"_blank\">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it’s not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It’s a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href=\"https://github.com/\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub</a> or <a href=\"https://www.mathworks.com/matlabcentral/\" rel=\"noopener noreferrer\" target=\"_blank\">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It’s very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>",
      "author": "Rina Diane Caballar",
      "publishedAt": "2022-02-08T14:00:01.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    },
    {
      "id": "a4b94d9f-0f51-4eaf-98db-7546c33dc8ef",
      "guid": "https://spectrum.ieee.org/2d-hbn-qubit",
      "title": "Atomically Thin Materials Significantly Shrink Qubits",
      "link": "https://spectrum.ieee.org/2d-hbn-qubit",
      "content": "\n<img src=\"https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1245&height=700&coordinates=0%2C156%2C0%2C156\"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href=\"https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission\" target=\"_self\">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href=\"https://www.nature.com/articles/s41563-021-01187-w\" rel=\"noopener noreferrer\" target=\"_blank\">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href=\"https://equs.mit.edu/william-d-oliver/\" rel=\"noopener noreferrer\" target=\"_blank\">William Oliver</a>, the director for the <a href=\"https://cqe.mit.edu/\" target=\"_blank\">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" data-rm-resized-container=\"25%\" style=\"float: left;\">\n<img alt=\"Golden dilution refrigerator hanging vertically\" class=\"rm-shortcode rm-resized-image\" data-rm-shortcode-id=\"694399af8a1c345e51a695ff73909eda\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"6c615\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980\" style=\"max-width: 100%\"/>\n<small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\" style=\"max-width: 100%;\">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\" style=\"max-width: 100%;\">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href=\"https://equs.mit.edu/joel-wang/\" rel=\"noopener noreferrer\" target=\"_blank\">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>",
      "author": "Dexter Johnson",
      "publishedAt": "2022-02-07T16:12:05.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:46:34.694Z"
    }
  ]
}