{
  "feedId": "849d41ff-457d-42b3-a546-f3401bad7843",
  "entries": [
    {
      "id": "1f7065cb-388d-421d-9dda-7316753af362",
      "guid": "https://news.mit.edu/2026/new-j-pal-research-policy-initiative-to-test-scale-ai-innovations-fight-poverty-0212",
      "title": "New J-PAL research and policy initiative to test and scale AI innovations to fight poverty",
      "link": "https://news.mit.edu/2026/new-j-pal-research-policy-initiative-to-test-scale-ai-innovations-fight-poverty-0212",
      "content": "<p>The Abdul Latif Jameel Poverty Action Lab (J-PAL) at MIT has awarded funding to eight new research studies to understand how artificial intelligence innovations can be used in the fight against poverty through its new <a href=\"https://www.povertyactionlab.org/initiative/partnership-ai-evidence-paie\" target=\"_blank\">Project AI Evidence</a>.</p><p>The age of AI has brought wide-ranging optimism and skepticism about its effects on society. To realize AI’s full potential, Project AI Evidence (PAIE) will identify which AI solutions work and for whom, and scale only the most effective, inclusive, and responsible solutions — while scaling down those that may potentially cause harm.</p><p>PAIE will generate evidence on what works by connecting governments, tech companies, and nonprofits with world-class economists at MIT and across J-PAL’s global network to evaluate and improve AI solutions to entrenched social challenges.</p><p>The new initiative is prioritizing questions policymakers are already asking: Do AI-assisted teaching tools help all children learn? How can early-warning flood systems help people affected by natural disasters? Can machine learning algorithms help reduce deforestation in the Amazon? Can AI-powered chatbots help improve people’s health? In the coming years, PAIE will run a series of funding competitions to invite proposals for evaluations of AI tools that address questions like these, and many more.</p><p>PAIE is financially supported by a grant from Google.org, philanthropic support from Community Jameel, a grant from Canada’s International Development Research Centre and UK International Development, and a collaboration agreement with Amazon Web Services. Through a grant from Eric and Wendy Schmidt, awarded by recommendation of Schmidt Sciences, the initiative will also study generative AI in the workplace, particularly in low- and middle-income countries.</p><p>Alex Diaz, head of AI for social good at Google.org, says, “we’re thrilled to collaborate with MIT and J-PAL, already leaders in this space, on Project AI Evidence. AI has great potential to benefit all people, but we urgently need to study what works, what doesn’t, and why, if we are to realize this potential.”</p><p>“Artificial intelligence holds extraordinary potential, but only if the tools, knowledge, and power to shape it are accessible to all — that includes contextually grounded research and evidence on what works and what does not,” adds Maggie Gorman-Velez, vice president of strategy, regions, and policies at IDRC. “That is why IDRC is proud to be supporting this new evaluation work as part of our ongoing commitment to the responsible scaling of proven safe, inclusive, and locally relevant AI innovations.”</p><p>J-PAL is uniquely positioned to help understand AI’s effects on society: Since its inception in 2003, J-PAL’s network of researchers has led over 2,500 rigorous evaluations of social policies and programs around the world. Through PAIE, J-PAL will bring together leading experts in AI technology, research, and social policy, in alignment with MIT president Sally Kornbluth’s focus on generative AI as a <a href=\"https://president.mit.edu/writing-speeches/launching-mit-generative-ai-impact-consortium\" target=\"_blank\">strategic priority</a>.</p><p>PAIE is chaired by Professor <a href=\"https://www.povertyactionlab.org/person/blumenstock\" target=\"_blank\">Joshua Blumenstock</a> of the University of California at Berkeley; J-PAL Global Executive Director <a href=\"https://www.povertyactionlab.org/person/dhaliwal\" target=\"_blank\">Iqbal Dhaliwal</a>; and Professor <a href=\"https://www.povertyactionlab.org/person/yanagizawa-drott\" target=\"_blank\">David Yanagizawa-Drott</a> of the University of Zurich.</p><p><strong>New evaluations of urgent policy questions</strong></p><p>The studies funded in PAIE’s first round of competition explore urgent questions in key sectors like education, health, climate, and economic opportunity.</p><p><strong>How can AI be most effective in classrooms, helping both students and teachers?</strong></p><p>Existing <a href=\"https://www.povertyactionlab.org/policy-insight/tailoring-instruction-students-learning-levels-increase-learning\" target=\"_blank\">research</a> shows that personalized learning is important for students, but challenging to implement with limited resources. In Kenya, education social enterprise EIDU has developed an AI tool that helps teachers identify learning gaps and adapt their daily lesson plans. In India, the nongovernmental organization (NGO) Pratham is developing an AI tool to increase the impact and scale of the evidence-informed <a href=\"https://www.povertyactionlab.org/case-study/teaching-right-level-improve-learning\" target=\"_blank\">Teaching at the Right Level</a> approach. J-PAL researchers Daron Acemoglu, Iqbal Dhaliwal, and Francisco Gallego will work with both organizations to study the effects and potential of these different use cases on <a href=\"https://www.povertyactionlab.org/initiative-project/ai-powered-structured-pedagogy-programs-impact-student-learning-and-teacher\" target=\"_blank\">teachers’ productivity and students’ learning</a>.</p><p><strong>Can AI tools reduce gender bias in schools?</strong></p><p>Researchers are collaborating with Italy’s Ministry of Education to evaluate whether AI tools can help <a href=\"https://www.povertyactionlab.org/initiative-project/ai-powered-structured-pedagogy-programs-impact-student-learning-and-teacher\" target=\"_blank\">close gender gaps in students’ performance</a> by addressing teachers’ unconscious biases. J-PAL affiliates Michela Carlana and Will Dobbie, along with Francesca Miserocchi and Eleonora Patacchini, will study the impacts of two AI tools, one that helps teachers predict performance and a second that gives real-time feedback on the diversity of their decisions.</p><p><strong>Can AI help career counselors uncover more job opportunities?</strong></p><p>In Kenya, researchers are evaluating if an AI tool can <a href=\"https://www.povertyactionlab.org/initiative-project/learning-recommend-ai-human-counselors-and-job-matching-kenya\" target=\"_blank\">identify overlooked skills and unlock employment opportunities</a>, particularly for youth, women, and those without formal education. In collaboration with NGOs Swahilipot and Tabiya, Jasmin Baier and J-PAL researcher Christian Meyer will evaluate how the tool changes people’s job search strategies and employment. This study will shed light on AI as a complement, rather than a substitute, for human expertise in career guidance.</p><p><strong>Looking forward</strong></p><p>As use of AI in the social sector evolves, these evaluations are a first step in discovering effective, responsible solutions that will go the furthest in alleviating poverty and inequality.</p><p>J-PAL’s Dhaliwal notes, “J-PAL has a long history of evaluating innovative technology and its ability to improve people’s lives. While AI has incredible potential, we need to maximize its benefits and minimize possible harms. We’re grateful to our donors, sponsors, and collaborators for their catalytic support in launching PAIE, which will help us do exactly that by continuing to expand evidence on the impacts of AI innovations.”</p><p>J-PAL is also seeking new collaborators who share its vision of discovering and scaling up real-world AI solutions. It aims to support more governments and social sector organizations that want to adopt AI responsibly, and will continue to expand funding for new evaluations and provide policy guidance based on the latest research.</p><p>To learn more about Project AI Evidence, <a href=\"https://povertyactionlab.org/subscribe\" target=\"_blank\">subscribe</a> to J-PAL's newsletter or contact <a href=\"mailto:paie@povertyactionlab.org\" target=\"_blank\">paie@povertyactionlab.org</a>.</p>",
      "author": "Abdul Latif Jameel Poverty Action Lab (J-PAL)",
      "publishedAt": "2026-02-12T23:50:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "1690bd02-bf4a-4d35-97fb-6a8ef35812a8",
      "guid": "https://news.mit.edu/2026/accelerating-science-ai-and-simulations-rafael-gomez-bombarelli-0212",
      "title": "Accelerating science with AI and simulations",
      "link": "https://news.mit.edu/2026/accelerating-science-ai-and-simulations-rafael-gomez-bombarelli-0212",
      "content": "<p>For more than a decade, MIT Associate Professor Rafael Gómez-Bombarelli has used artificial intelligence to create new materials. As the technology has expanded, so have his ambitions.</p><p>Now, the newly tenured professor in materials science and engineering believes AI is poised to transform science in ways never before possible. His work at MIT and beyond is devoted to accelerating that future.</p><p>“We’re at a second inflection point,”<strong>&nbsp;</strong>Gómez-Bombarelli says. “The first one was around 2015 with the first wave of representation learning, generative AI, and high-throughput data in some areas of science. Those are some of the techniques I first brought into my lab at MIT. Now I think we’re at a second inflection point, mixing language and merging multiple modalities into general scientific intelligence. We’re going to have all the model classes and scaling laws needed to reason about language, reason over material structures, and reason over synthesis recipes.”</p><p>Gómez Bombarelli’s research combines physics-based simulations with approaches like machine learning and generative AI to discover new materials with promising real-world applications. His work has led to new materials for batteries, catalysts, plastics, and organic light-emitting diodes (OLEDs). He has also co-founded multiple companies and served on scientific advisory boards for startups applying AI to drug discovery, robotics, and more. His latest company, Lila Sciences, is working to build a scientific superintelligence platform for the life sciences, chemical, and materials science industries.</p><p>All of that work is designed to ensure the future of scientific research is more seamless and productive than research today.</p><p>“AI for science is one of the most exciting and aspirational uses of AI,” Gómez-Bombarelli says. “Other applications for AI have more downsides and ambiguity. AI for science is about bringing a better future forward in time.”</p><p><strong>From experiments to simulations</strong></p><p>Gómez-Bombarelli grew up in Spain and gravitated toward the physical sciences from an early age. In 2001, he won a Chemistry Olympics competition, setting him on an academic track in chemistry, which he studied as an undergraduate at his hometown college, the University of Salamanca. Gómez-Bombarelli stuck around for his PhD, where he investigated the function of DNA-damaging chemicals.</p><p>“My PhD started out experimental, and then I got bitten by the bug of simulation and computer science about halfway through,” he says. “I started simulating the same chemical reactions I was measuring in the lab. I like the way programming organizes your brain; it felt like a natural way to organize one’s thinking. Programming is also a lot less limited by what you can do with your hands or with scientific instruments.”</p><p>Next, Gómez-Bombarelli went to Scotland for a postdoctoral position, where he studied quantum effects in biology. Through that work, he connected with Alán Aspuru-Guzik, a chemistry professor at Harvard University, whom he joined for his next postdoc in 2014.</p><p>“I was one of the first people to use generative AI for chemistry in 2016, and I was on the first team to use neural networks to understand molecules in 2015,” Gómez-Bombarelli says. “It was the early, early days of deep learning for science.”</p><p>Gómez-Bombarelli also began working to eliminate manual parts of molecular simulations to run more high-throughput experiments. He and his collaborators ended up running hundreds of thousands of calculations across materials, discovering hundreds of promising materials for testing.</p><p>After two years in the lab, Gómez-Bombarelli and Aspuru-Guzik started a general-purpose materials computation company, which eventually pivoted to focus on producing organic light-emitting diodes. Gómez-Bombarelli joined the company full-time and calls it the hardest thing he’s ever done in his career.</p><p>“It was amazing to make something tangible,” he says. “Also, after seeing Aspuru-Guzik run a lab, I didn’t want to become a professor. My dad was a professor in linguistics, and I thought it was a mellow job. Then I saw Aspuru-Guzik with a 40-person group, and he was on the road 120 days a year. It was insane. I didn’t think I had that type of energy and creativity in me.”</p><p>In 2018, Aspuru-Guzik suggested Gómez-Bombarelli apply for a new position in MIT’s Department of Materials Science and Engineering. But, with his trepidation about a faculty job, Gómez-Bombarelli let the deadline pass. Aspuru-Guzik confronted him in his office, slammed his hands on the table, and told him, “You need to apply for this.” It was enough to get Gómez-Bombarelli to put together a formal application.</p><p>Fortunately at his startup, Gómez-Bombarelli had spent a lot of time thinking about how to create value from computational materials discovery. During the interview process, he says, he was attracted to the energy and collaborative spirit at MIT. He also began to appreciate the research possibilities.</p><p>“Everything I had been doing as a postdoc and at the company was going to be a subset of what I could do at MIT,” he says. “I was making products, and I still get to do that. Suddenly, my universe of work was a subset of this new universe of things I could explore and do.”</p><p>It’s been nine years since Gómez Bombarelli joined MIT. Today his lab focuses on how the composition, structure, and reactivity of atoms impact material performance. He has also used high-throughput simulations to create new materials and helped develop tools for merging deep learning with physics-based modeling.</p><p>“Physics-based simulations make data and AI algorithms get better the more data you give them,” Gómez Bombarelli’s says. “There are all sorts of virtuous cycles between AI and simulations.”</p><p>The research group he has built is solely computational — they don’t run physical experiments.</p><p>“It’s a blessing because we can have a huge amount of breadth and do lots of things at once,” he says. “We love working with experimentalists and try to be good partners with them. We also love to create computational tools that help experimentalists triage the ideas coming from AI .”</p><p>Gómez-Bombarelli is also still focused on the real-world applications of the materials he invents. His lab works closely with companies and organizations like MIT’s Industrial Liaison Program to understand the material needs of the private sector and the practical hurdles of commercial development.</p><p><strong>Accelerating science</strong></p><p>As excitement around artificial intelligence has exploded, Gómez-Bombarelli has seen the field mature. Companies like Meta, Microsoft, and Google’s DeepMind now regularly conduct physics-based simulations reminiscent of what he was working on back in 2016. In November, the U.S. Department of Energy launched the Genesis Mission to accelerate scientific discovery, national security, and energy dominance using AI.</p><p>“AI for simulations has gone from something that maybe could work to a consensus scientific view,” Gómez-Bombarelli says. “We’re at an inflection point. Humans think in natural language, we write papers in natural language, and it turns out these large language models that have mastered natural language have opened up the ability to accelerate science. We’ve seen that scaling works for simulations. We’ve seen that scaling works for language. Now we’re going to see how scaling works for science.”</p><p>When he first came to MIT,<strong>&nbsp;</strong>Gómez-Bombarelli says he was blown away by how non-competitive things were between researchers. He tries to bring that same positive-sum thinking to his research group, which is made up of about 25 graduate students and postdocs.</p><p>“We’ve naturally grown into a really diverse group, with a diverse set of mentalities,” Gomez-Bombarelli says. “Everyone has their own career aspirations and strengths and weaknesses. Figuring out how to help people be the best versions of themselves is fun. Now I’ve become the one insisting that people apply to faculty positions after the deadline. I guess I’ve passed that baton.”</p>",
      "author": "Zach Winn | MIT News",
      "publishedAt": "2026-02-12T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "8368ecb1-81cb-4d9a-ba18-0a5e237c4d5f",
      "guid": "https://news.mit.edu/2026/using-synthetic-biology-ai-address-global-antimicrobial-resistance-0211",
      "title": "Using synthetic biology and AI to address global antimicrobial resistance threat",
      "link": "https://news.mit.edu/2026/using-synthetic-biology-ai-address-global-antimicrobial-resistance-0211",
      "content": "<p>James J. Collins, the Termeer Professor of Medical Engineering and Science at MIT and faculty co-lead of the Abdul Latif Jameel Clinic for Machine Learning in Health, is embarking on a multidisciplinary research project that applies synthetic biology and generative artificial intelligence to the growing global threat of antimicrobial resistance (AMR).</p><p>The research project is sponsored by Jameel Research, part of the Abdul Latif Jameel International network. The initial three-year, $3 million research project in MIT’s Department of Biological Engineering and Institute of Medical Engineering and Science focuses on developing and validating programmable antibacterials against key pathogens.</p><p>AMR — driven by the overuse and misuse of antibiotics — has accelerated the rise of drug-resistant infections, while the development of new antibacterial tools has slowed. The impact is felt worldwide, especially in low- and middle-income countries, where limited diagnostic infrastructure causes delays or ineffective treatment.</p><p>The project centers on developing a new generation of targeted antibacterials using AI to design small proteins to disable specific bacterial functions. These designer molecules would be produced and delivered by engineered microbes, providing a more precise and adaptable approach than traditional antibiotics.</p><p>“This project reflects my belief that tackling AMR requires both bold scientific ideas and a pathway to real-world impact,” Collins says. “Jameel Research is keen to address this crisis by supporting innovative, translatable research at MIT.”</p><p>Mohammed Abdul Latif Jameel ’78, chair of Abdul Latif Jameel, says, “antimicrobial resistance is one of the most urgent challenges we face today, and addressing it will require ambitious science and sustained collaboration. We are pleased to support this new research, building on our long-standing relationship with MIT and our commitment to advancing research across the world, to strengthen global health and contribute to a more resilient future.”</p>",
      "author": "Daniel J. Darling | Department of Biological Engineering",
      "publishedAt": "2026-02-11T13:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "746c0f39-e624-4d86-9df8-9450e06b23b6",
      "guid": "https://news.mit.edu/2026/new-window-on-brainstem-ai-algorithm-enables-tracking-white-matter-pathways-0210",
      "title": "AI algorithm enables tracking of vital white matter pathways",
      "link": "https://news.mit.edu/2026/new-window-on-brainstem-ai-algorithm-enables-tracking-white-matter-pathways-0210",
      "content": "<p>The signals that drive many of the brain and body’s most essential functions — consciousness, sleep, breathing, heart rate, and motion — course through bundles of “white matter” fibers in the brainstem, but imaging systems so far have been unable to finely resolve these crucial neural cables. That has left researchers and doctors with little capability to assess how they are affected by trauma or neurodegeneration.&nbsp;</p><p>In a new study, a team of MIT, Harvard University, and Massachusetts General Hospital researchers unveil AI-powered software capable of automatically segmenting eight distinct bundles in any diffusion MRI sequence.</p><p>In the open-access study, <a href=\"https://www.pnas.org/doi/10.1073/pnas.2509321123\">published Feb. 6 in the <em>Proceedings of the National Academy Sciences</em></a><em>,&nbsp;</em>the research team led by MIT graduate student Mark Olchanyi reports that their BrainStem Bundle Tool (BSBT), which they’ve made <a href=\"https://github.com/markolchanyi/BSBT\">publicly available</a>, revealed distinct patterns of structural changes in patients with Parkinson’s disease, multiple sclerosis, and traumatic brain injury, and shed light on Alzheimer’s disease as well. Moreover, the study shows, BSBT retrospectively enabled tracking of bundle healing in a coma patient that reflected the patient’s seven-month road to recovery.</p><p>“The brainstem is a region of the brain that is essentially not explored because it is tough to image,” says Olchanyi, a doctoral candidate in MIT’s Medical Engineering and Medical Physics Program. “People don't really understand its makeup from an imaging perspective. We need to understand what the organization of the white matter is in humans and how this organization breaks down in certain disorders.”</p><p>Adds Professor <a href=\"https://picower.mit.edu/emery-n-brown\">Emery N. Brown</a>, Olchanyi’s thesis supervisor and co-senior author of the study, “the brainstem is one of the body’s most important control centers. Mark’s algorithms are a significant contribution to imaging research and to our ability to the understand regulation of fundamental physiology. By enhancing our capacity&nbsp;to image the brainstem, he offers us new access to vital physiological functions such as control of the respiratory and cardiovascular systems, temperature regulation, how we stay awake during the day and how sleep at night.”</p><p>Brown is the Edward Hood Taplin Professor of Computational Neuroscience and Medical Engineering in The Picower Institute for Learning and Memory, the Institute for Medical Engineering and Science, and the Department of Brain and Cognitive Sciences at MIT. He is also an anesthesiologist at MGH and a professor at Harvard Medical School.</p><p><strong>Building the algorithm</strong></p><p>Diffusion MRI helps trace the long branches, or “axons,” that neurons extend to communicate with each other. Axons are typically clad in a sheath of fat called myelin, and water diffuses along the axons within the myelin, which is also called the brain’s “white matter.” Diffusion MRI can highlight this very directed displacement of water. But segmenting the distinct bundles of axons in the brainstem has proved challenging, because they are small and masked by flows of brain fluids and the motions produced by breathing and heart beats.</p><p>As part of his thesis work to better understand the neural mechanisms that underpin consciousness, Olchanyi wanted to <a href=\"https://news.mit.edu/2023/takeda-fellows-leveraging-ai-positively-impact-human-health-0112\">develop an AI algorithm</a> to overcome these obstacles. BSBT works by tracing fiber bundles that plunge into the brainstem from neighboring areas higher in the brain, such as the thalamus and the cerebellum, to produce a “probabilistic fiber map.” An artificial intelligence module called a “convolutional neural network” then combines the map with several channels of imaging information from within the brainstem to distinguish eight individual bundles.</p><p>To train the neural network to segment the bundles, Olchanyi “showed” it 30 live diffusion MRI scans from volunteers in the Human Connectome Project (HCP). The scans were manually annotated to teach the neural network how to identify the bundles. Then he validated BSBT by testing its output against “ground truth” dissections of post-mortem human brains where the bundles were well delineated via microscopic inspection or very slow but ultra-high-resolution imaging. After training, BSBT became proficient in automatically identifying the eight distinct fiber bundles in new scans.</p><p>In an experiment to test its consistency and reliability, Olchanyi tasked BSBT with finding the bundles in 40 volunteers who underwent separate scans two months apart. In each case, the tool was able to find the same bundles in the same patients in each of their two scans. Olchanyi also tested BSBT with multiple datasets (not just the HCP), and even inspected how each component of the neural network contributed to BSBT’s analysis by hobbling them one by one.</p><p>“We put the neural network through the wringer,” Olchanyi says. “We wanted to make sure that it’s actually doing these plausible segmentations and it is leveraging each of its individual components in a way that improves the accuracy.”</p><p><strong>Potential novel biomarkers</strong></p><p>Once the algorithm was properly trained and validated, the research team moved on to testing whether the ability to segment distinct fiber bundles in diffusion MRI scans could enable tracking of how each bundle’s volume and structure varied with disease or injury, creating a novel kind of biomarker. Although the brainstem has been difficult to examine in detail, many studies show that neurodegenerative diseases affect the brainstem, often early on in their progression.</p><p>Olchanyi, Brown and their co-authors applied BSBT to scores of datasets of diffusion MRI scans from patients with Alzheimer’s, Parkinson’s, MS, and traumatic brain injury (TBI). Patients were compared to controls and sometimes to themselves over time. In the scans, the tool measured bundle volume and “fractional anisotropy,” (FA) which tracks how much water is flowing along the myelinated axons versus how much is diffusing in other directions, a proxy for white matter structural integrity.</p><p>In each condition, the tool found consistent patterns of changes in the bundles. While only one bundle showed significant decline in Alzheimer’s, in Parkinson’s the tool revealed a reduction in FA in three of the eight bundles. It also revealed volume loss in another bundle in patients between a baseline scan and a two-year follow-up. Patients with MS showed their greatest FA reductions in four bundles and volume loss in three. Meanwhile, TBI patients didn’t show significant volume loss in any bundles, but FA reductions were apparent in the majority of bundles.</p><p>Testing in the study showed that BSBT proved more accurate than other classifier methods in discriminating between patients with health conditions versus controls.</p><p>BSBT, therefore, can be “a key adjunct that aids current diagnostic imaging methods by providing a fine-grained assessment of brainstem white matter structure and, in some cases, longitudinal information,” the authors wrote.</p><p>Finally, in the case of a 29-year-old man who suffered a severe TBI, Olchanyi applied BSBT to a scans taken during the man’s seven-month coma. The tool showed that the man’s brainstem bundles had been displaced, but not cut, and showed that over his coma, the lesions on the nerve bundles decreased by a factor of three in volume. As they healed, the bundles moved back into place as well.</p><p>The authors wrote that BSBT “has substantial prognostic potential by identifying preserved brainstem bundles that can facilitate coma recovery.”</p><p>The study’s other senior authors are Juan Eugenio Iglesias and Brian Edlow. Other co-authors are David Schreier, Jian Li, Chiara Maffei, Annabel Sorby-Adams, Hannah Kinney, Brian Healy, Holly Freeman, Jared Shless, Christophe Destrieux, and Hendry Tregidgo.</p><p>Funding for the study came from the National Institutes of Health, U.S. Department of Defense, James S. McDonnell Foundation, Rappaport Foundation, American SidS Institute, American Brain Foundation, American Academy of Neurology, Center for Integration of Medicine and Innovative Technology, Blueprint for Neuroscience Research, and Massachusetts Life Sciences Center.</p>",
      "author": "David Orenstein | The Picower Institute for Learning and Memory",
      "publishedAt": "2026-02-10T22:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "15d34fb6-2ff1-4b7d-9701-7a6075256e70",
      "guid": "https://news.mit.edu/2026/3-questions-using-ai-help-olympic-skaters-land-quint-0210",
      "title": "3 Questions: Using AI to help Olympic skaters land a quint",
      "link": "https://news.mit.edu/2026/3-questions-using-ai-help-olympic-skaters-land-quint-0210",
      "content": "<p><em>Olympic figure skating looks effortless. Athletes sail across the ice, then soar into the air, spinning like a top, before landing on a single blade just 4-5 millimeters wide. To help figure skaters land quadruple axels, Salchows, Lutzes, and maybe even the elusive quintuple without looking the least bit stressed, Jerry Lu MFin ’24 developed an optical tracking system called </em><a href=\"https://oofskate.com/\" target=\"_blank\"><em>OOFSkate</em></a><em> that uses artificial intelligence to analyze video of a figure skater’s jump and make recommendations on how to improve. Lu, a former researcher at the&nbsp;</em><a href=\"https://sportslab.mit.edu/\" target=\"_blank\"><em>MIT Sports Lab</em></a><em>, has been aiding elite skaters on Team USA with their technical performance and will be working with NBC Sports during the 2026 Winter Olympics to help commentators and TV viewers make better sense of the complex scoring system in figure skating, snowboarding, and skiing. He’ll be applying AI technologies to explain nuanced judging decisions and demonstrate just how technically challenging these sports can be.</em></p><p><em>Meanwhile, Professor Anette “Peko” Hosoi, co-founder and faculty director of the MIT Sports Lab, is embarking on new research aimed at understanding how AI systems evaluate aesthetic performance in figure skating. Hosoi and Lu recently chatted with&nbsp;</em>MIT News<em> about applying AI to sports, whether AI systems could ever be used to judge Olympic figure skating, and when we might see a skater land a quint.</em></p><p><strong>Q:</strong> Why apply AI to figure skating?</p><p><strong>Lu:</strong> Skaters can always keep pushing, higher, faster, stronger. OOFSkate is all about helping skaters figure out a way to rotate a little bit faster in their jumps or jump a little bit higher. The system helps skaters catch things that perhaps could pass an eye test, but that might allow them to target some high-value areas of opportunity. The artistic side of skating is much harder to evaluate than the technical elements because it’s subjective.</p><p>To use mobile training app, you just need to take a video of an athlete’s jump, and it will spit out the physical metrics that drive how many rotations you can do. It tracks those metrics and builds in all of the other current elite and former elite athletes. You can see your data and then see, “This is how an Olympic champion did this element, perhaps I should try that.” You get the comparison and the automated classifier, which shows you if you did this trick at World Championships and it were judged by an international panel, this is approximately the grade of execution score they would give you.</p><p><strong>Hosoi:</strong> There are a lot of AI tools that are coming online, especially things like pose estimators, where you can approximate skeletal configurations from video. The challenge with these pose estimators is that if you only have one camera angle, they do very well in the plane of the camera, but they do very poorly with depth. For example, if you’re trying to critique somebody’s form in fencing, and they’re moving toward the camera, you get very bad data. But with figure skating, Jerry has found one of the few areas where depth challenges don’t really matter. In figure skating, you need to understand: How high did this person jump, how many times did they go around, and how well did they land? None of those rely on depth. He’s found an application that pose estimators do really well, and that doesn’t pay a penalty for the things they do badly.</p><p><strong>Q:</strong> Could you ever see a world in which AI is used to evaluate the artistic side of figure skating?</p><p><strong>Hosoi:</strong> When it comes to AI and aesthetic evaluation, we have new work underway thanks to a MIT Human Insight Collaborative (<a href=\"https://mithic.mit.edu/\" target=\"_blank\">MITHIC</a>) grant. This work is in collaboration with Professor <a href=\"https://lit.mit.edu/abahr/\" target=\"_blank\">Arthur Bahr</a> and IDSS graduate student Eric Liu. When you ask an AI platform for an aesthetic evaluation such as “What do you think of this painting?” it will respond with something that sounds like it came from a human. What we want to understand is, to get to that assessment, are the AIs going through the same sort of reasoning pathways or using the same intuitive concepts that humans go through to arrive at, “I like that painting,” or “I don’t like that painting”? Or are they just parrots? Are they just mimicking what they heard a person say? Or is there some concept map of aesthetic appeal? Figure skating is a perfect place to look for this map because skating is aesthetically judged. And there are numbers. You can’t go around a museum and find scores, “This painting is a 35.” But in skating, you’ve got the data.</p><p>That brings up another even more interesting question, which is the difference between novices and experts. It’s known that expert humans and novice humans will react differently to seeing the same thing. Somebody who is an expert judge may have a different opinion of a skating performance than a member of the general population. We’re trying to understand differences between reactions from experts, novices, and AI. Do these reactions have some common ground in where they are coming from, or is the AI coming from a different place than both the expert and the novice?</p><p><strong>Lu:</strong> Figure skating is interesting because everybody working in the field of AI is trying to figure out AGI or artificial general intelligence and trying to build this extremely sound AI that replicates human beings. Working on applying AI to sports like figure skating helps us understand how humans think and approach judging. This has down-the-line impacts for AI research and companies that are developing AI models. By gaining a deeper understanding of how current state-of-the-art AI models work with these sports, and how you need to do training and fine-tuning of these models to make them work for specific sports, it helps you understand how AI needs to advance.</p><p><strong>Q:</strong> What will you be watching for in the Milan Cortina Olympics figure skating competitions, now that you’ve been studying and working in this area? Do you think someone will land a quint?</p><p><strong>Lu:</strong> For the winter games, I am working with NBC for the figure skating, ski, and snowboarding competitions to help them tell a data-driven story for the American people. The goal is to make these sports more relatable. Skating looks slow on television, but it’s not. Everything is supposed to look effortless. If it looks hard, you are probably going to get penalized. Skaters need to learn how to spin very fast, jump extremely high, float in the air, and land beautifully on one foot. The data we are gathering can help showcase how hard skating actually is, even though it is supposed to look easy.</p><p>I’m glad we are working in the Olympics sports realm because the world watches once every four years, and it is traditionally coaching-intensive and talent-driven sports, unlike a sport like baseball, where if you don’t have an elite-level optical tracking system you are not maximizing the value that you currently have. I’m glad we get to work with these Olympic sports and athletes and make an impact here.</p><p><strong>Hosoi:</strong> I have always watched Olympic figure skating competitions, ever since I could turn on the TV. They’re always incredible. One of the things that I’m going to be practicing is identifying the jumps, which is very hard to do if you’re an amateur “judge.”</p><p>I have also done some back-of-the-envelope calculations to see if a quint is possible. I am now totally convinced it’s possible. We will see one in our lifetime, if not relatively soon. Not in this Olympics, but soon. When I saw we were so close on the quint, I thought, what about six? Can we do six rotations? Probably not. That’s where we start to come up against the limits of human physical capability. But five, I think, is in reach.</p>",
      "author": "Abby Abazorius | MIT News",
      "publishedAt": "2026-02-10T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "892eae9f-cef0-4bfa-b587-33850ee640ce",
      "guid": "https://news.mit.edu/2026/study-platforms-rank-latest-llms-can-be-unreliable-0209",
      "title": "Study: Platforms that rank the latest LLMs can be unreliable",
      "link": "https://news.mit.edu/2026/study-platforms-rank-latest-llms-can-be-unreliable-0209",
      "content": "<p>A firm that wants to use a large language model (LLM) to summarize sales reports or triage customer inquiries can choose between hundreds of unique LLMs with dozens of model variations, each with slightly different performance.</p><p>To narrow down the choice, companies often rely on LLM ranking platforms, which gather user feedback on model interactions to rank the latest LLMs based on how they perform on certain tasks.</p><p>But MIT researchers found that a handful of user interactions can skew the results, leading someone to mistakenly believe one LLM is the ideal choice for a particular use case. Their study reveals that removing a tiny fraction of crowdsourced data can change which models are top-ranked.</p><p>They developed a fast method to test ranking platforms and determine whether they are susceptible to this problem. The evaluation technique identifies the individual votes most responsible for skewing the results so users can inspect these influential votes.</p><p>The researchers say this work underscores the need for more rigorous strategies to evaluate model rankings. While they didn’t focus on mitigation in this study, they provide suggestions that may improve the robustness of these platforms, such as gathering more detailed feedback to create the rankings.</p><p>The study also offers a word of warning to users who may rely on rankings when making decisions about LLMs that could have far-reaching and costly impacts on a business or organization.</p><p>“We were surprised that these ranking platforms were so sensitive to this problem. If it turns out the top-ranked LLM depends on only two or three pieces of user feedback out of tens of thousands, then one can’t assume the top-ranked LLM is going to be consistently outperforming all the other LLMs when it is deployed,” says Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS); a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society; an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author of this study.</p><p>She is joined on the <a href=\"https://arxiv.org/pdf/2508.11847\" target=\"_blank\">paper</a> by lead authors and EECS graduate students Jenny Huang and Yunyi Shen as well as Dennis Wei, a senior research scientist at IBM Research. The study will be presented at the International Conference on Learning Representations.</p><p><strong>Dropping data</strong></p><p>While there are many types of LLM ranking platforms, the most popular variations ask users to submit a query to two models and pick which LLM provides the better response.</p><p>The platforms aggregate the results of these matchups to produce rankings that show which LLM performed best on certain tasks, such as coding or visual understanding.</p><p>By choosing a top-performing LLM, a user likely expects that model’s top ranking to generalize, meaning it should outperform other models on their similar, but not identical, application with a set of new data.</p><p>The MIT researchers previously studied generalization in areas like statistics and economics. That work revealed certain cases where dropping a small percentage of data can change a model’s results, indicating that those studies’ conclusions might not hold beyond their narrow setting.</p><p>The researchers wanted to see if the same analysis could be applied to LLM ranking platforms.</p><p>“At the end of the day, a user wants to know whether they are choosing the best LLM. If only a few prompts are driving this ranking, that suggests the ranking might not be the end-all-be-all,” Broderick says.</p><p>But it would be impossible to test the data-dropping phenomenon manually. For instance, one ranking they evaluated had more than 57,000 votes. Testing a data drop of 0.1 percent means removing each subset of 57 votes out of the 57,000, (there are more than 10<sup>194&nbsp;</sup>subsets), and then recalculating the ranking.</p><p>Instead, the researchers developed an efficient approximation method, based on their prior work, and adapted it to fit LLM ranking systems.</p><p>“While we have theory to prove the approximation works under certain assumptions, the user doesn’t need to trust that. Our method tells the user the problematic data points at the end, so they can just drop those data points, re-run the analysis, and check to see if they get a change in the rankings,” she says.</p><p><strong>Surprisingly sensitive</strong></p><p>When the researchers applied their technique to popular ranking platforms, they were surprised to see how few data points they needed to drop to cause significant changes in the top LLMs. In one instance, removing just two votes out of more than 57,000, which is 0.0035 percent, changed which model is top-ranked.</p><p>A different ranking platform, which uses expert annotators and higher quality prompts, was more robust. Here, removing 83 out of 2,575 evaluations (about 3 percent) flipped the top models.</p><p>Their examination revealed that many influential votes may have been a result of user error. In some cases, it appeared there was a clear answer as to which LLM performed better, but the user chose the other model instead, Broderick says.</p><p>“We can never know what was in the user’s mind at that time, but maybe they mis-clicked or weren’t paying attention, or they honestly didn’t know which one was better. The big takeaway here is that you don’t want noise, user error, or some outlier determining which is the top-ranked LLM,” she adds.</p><p>The researchers suggest that gathering additional feedback from users, such as confidence levels in each vote, would provide richer information that could help mitigate this problem. Ranking platforms could also use human mediators to assess crowdsourced responses.</p><p>For the researchers’ part, they want to continue exploring generalization in other contexts while also developing better approximation methods that can capture more examples of non-robustness.</p><p>“Broderick and her students’ work shows how you can get valid estimates of the influence of specific data on downstream processes, despite the intractability of exhaustive calculations given the size of modern machine-learning models and datasets,” says Jessica Hullman, the Ginni Rometty Professor of Computer Science at Northwestern University, who was not involved with this work. &nbsp;“The recent work provides a glimpse into the strong data dependencies in routinely applied — but also very fragile — methods for aggregating human preferences and using them to update a model. Seeing how few preferences could really change the behavior of a fine-tuned model could inspire more thoughtful methods for collecting these data.”</p><p>This research is funded, in part, by the Office of Naval Research, the MIT-IBM Watson AI Lab, the National Science Foundation, Amazon, and a CSAIL seed award.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2026-02-09T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "3173393a-ea83-4964-9298-037b7a92b3b2",
      "guid": "https://news.mit.edu/2026/science-mit-president-talks-about-importance-americas-research-enterprise-gbhs-boston-public",
      "title": "“This is science!” – MIT president talks about the importance of America’s research enterprise on GBH’s Boston Public Radio",
      "link": "https://news.mit.edu/2026/science-mit-president-talks-about-importance-americas-research-enterprise-gbhs-boston-public",
      "content": "<p>In a wide-ranging live conversation, MIT President Sally Kornbluth joined Jim Braude and Margery Eagan live in studio for GBH’s <em>Boston Public Radio&nbsp;</em>on Thursday, February 5. They talked about MIT, the pressures facing America’s research enterprise, the importance of science, that Congressional hearing on antisemitism in 2023, and more – including Sally’s experience as a Type 1 diabetic.</p><p>Reflecting on how research and innovation in the treatment of diabetes has advanced over decades of work, leading to markedly better patient care, Kornbluth exclaims: “This is science!”</p><p>With new financial pressures facing universities, increased competition for talented students and scholars from outside the U.S., as well as unprecedented pressures on university leaders and campuses, co-host Eagan asks Kornbluth what she thinks will happen in years to come.</p><p>“For us, one of the hardest things now is the endowment tax,” remarks Kornbluth. “That is $240 million a year. Think about how much science you can get for $240 million a year. Are we managing it? Yes. Are we still forging ahead on all of our exciting initiatives? Yes. But we’ve had to reconfigure things. We’ve had to merge things. And it’s not the way we should be spending our time and money.”&nbsp; &nbsp;</p><p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LFVXCGgRgMg?si=W-qeFnN-qYh3jBMP&amp;start=4020\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"></iframe></p><p><strong>Watch and listen to the&nbsp;</strong><a href=\"https://www.youtube.com/watch?v=LFVXCGgRgMg\"><strong>full episode on YouTube</strong></a><strong>. </strong>President Kornbluth appears one hour and seven minutes into the broadcast.</p><p>Following Kornbluth’s appearance, MIT Assistant Professor John Urschel – also a former offensive lineman for the Baltimore Ravens –&nbsp; &nbsp;joined Edgar B. Herwick III, host of GBH’s newest show, <em>The Curiosity Desk,&nbsp;</em>to talk about his love of his family, linear algebra, and football.</p><p>On how he eventually chose math over football, Urschel quips: “Well, I hate to break it to you, I like math better… let me tell you, when I started my PhD at MIT, I just fell in love with the place. I fell in love with this idea of being in this environment [where] everyone loves math, everyone wants to learn. I was just constantly excited every day showing up.”</p><p>Prof. Urschel appears about 2 hours and 40 minutes into the <a href=\"file:///Users/s_mcd/Desktop/Watch%20and%20listen%20on%20YouTube.\">webcast on YouTube</a>.</p><p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LFVXCGgRgMg?si=0BU7eW0CsUCRE0g9&amp;start=9634\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\"></iframe></p><p><strong>Coming up on Curiosity Desk later this month…</strong></p><p>Airing weekday afternoons from 1-2 p.m., <em>The Curiosity Desk</em> will welcome additional MIT guests in the coming weeks. On Thursday, Feb. 12, Professors Sangeeta Bhatia and Angela Belcher talk with Herwick about their research to improve diagnostics for ovarian cancer. We learn that about 80% of the time ovarian cancer starts in the fallopian tubes and how this points the way to a whole new approach to diagnosing and treating the disease.&nbsp;</p><p>Then, on Tuesday, Feb. 17 Anette “Peko” Hosoi, Pappalardo Professor of Mechanical Engineering, and Jerry Lu MFin ’24, a former researcher at the MIT Sports Lab, visit <em>The Curiosity Desk&nbsp;</em>to discuss their work using AI to help Olympic figure skaters improve their jumps.</p><p><iframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/soundcloud%253Atracks%253A2261481722&amp;color=%23040404&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=false\"></iframe></p><div style=\"color:#cccccc;font-family:Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-size:10px;font-weight:100;line-break:anywhere;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-break:normal;\"><a style=\"color:#cccccc;text-decoration:none;\" href=\"https://soundcloud.com/mitnewsoffice\" target=\"_blank\" title=\"MIT News\">MIT News</a> · <a style=\"color:#cccccc;text-decoration:none;\" href=\"https://soundcloud.com/mitnewsoffice/curiosity-desk-preview\" target=\"_blank\" title=\"Curiosity Desk Preview\">Curiosity Desk Preview</a><br><strong>Source: GBH&nbsp;</strong></div>",
      "author": null,
      "publishedAt": "2026-02-06T17:38:22.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "3e662e03-7084-46cc-8af8-e46f94f71c42",
      "guid": "https://news.mit.edu/2026/helping-ai-agents-search-to-get-best-results-from-llms-0205",
      "title": "Helping AI agents search to get the best results out of large language models ",
      "link": "https://news.mit.edu/2026/helping-ai-agents-search-to-get-best-results-from-llms-0205",
      "content": "<p dir=\"ltr\" id=\"docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c\">Whether you’re a scientist brainstorming research ideas or a CEO hoping to automate a task in human resources or finance, you’ll find that artificial intelligence tools are becoming the assistants you didn’t know you needed. In particular,&nbsp;many professionals are <a href=\"https://www.globenewswire.com/news-release/2025/05/20/3084932/0/en/72-of-professionals-report-using-AI-at-work-compared-to-just-48-in-2024.html\" target=\"_blank\">tapping into the talents</a> of semi-autonomous software systems called AI agents, which can call on AI at specific points to solve problems and complete tasks.<br><br>AI agents are particularly effective when they use large language models (LLMs) because those systems are powerful, efficient, and adaptable. One way to program such technology is by describing in code what you want your system to do (the “workflow”), including when it should use an LLM. If you were a software company trying to revamp your old codebase to use a more modern programming language for better optimizations and safety, you might build a system that uses an LLM to translate the codebase one file at a time, testing each file as you go.<br><br>But what happens when LLMs make mistakes? You’ll want the agent to backtrack to make another attempt, incorporating lessons it learned from previous mistakes. Coding this up can take as much effort as implementing the original agent; if your system for translating a codebase contained thousands of lines of code, then you’d be making thousands of lines of code changes or additions to support the logic for backtracking when LLMs make mistakes.&nbsp;</p><p dir=\"ltr\" id=\"docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c\">To save programmers time and effort, researchers with MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Asari AI have <a href=\"https://arxiv.org/pdf/2512.03571\" target=\"_blank\">developed a framework called “EnCompass.”</a>&nbsp;</p><p dir=\"ltr\">With EnCompass, you no longer have to make these changes yourself. Instead, when EnCompass runs your program, it automatically backtracks if LLMs make mistakes. EnCompass can also make clones of the program runtime to make multiple attempts in parallel in search of the best solution. In full generality, EnCompass searches over the different possible paths your agent could take as a result of the different possible outputs of all the LLM calls, looking for the path where the LLM finds the best solution.<br><br>Then, all you have to do is to annotate the locations where you may want to backtrack or clone the program runtime, as well as record any information that may be useful to the strategy used to search over the different possible execution paths of your agent (the search strategy). You can then separately specify the search strategy — you could either use one that EnCompass provides out of the box or, if desired, implement your own custom search strategy.</p><p dir=\"ltr\">“With EnCompass, we’ve separated the search strategy from the underlying workflow of an AI agent,” says lead author Zhening Li ’25, MEng ’25, who is an MIT electrical engineering and computer science (EECS) PhD student, CSAIL researcher, and research consultant at Asari AI. “Our framework lets programmers easily experiment with different search strategies to find the one that makes the AI agent perform the best.”&nbsp;<br><br>EnCompass was used for agents implemented as Python programs that call LLMs, where it demonstrated noticeable code savings. EnCompass reduced coding effort for implementing search by up to 80 percent across agents, such as an agent for translating code repositories and for discovering transformation rules of digital grids. In the future, EnCompass could enable agents to tackle large-scale tasks, including managing massive code libraries, designing and carrying out science experiments, and creating blueprints for rockets and other hardware.</p><p dir=\"ltr\"><strong>Branching out</strong></p><p dir=\"ltr\">When programming your agent, you mark particular operations — such as calls to an LLM — where results may vary. These annotations are called “branchpoints.” If you imagine your agent program as generating a single plot line of a story, then adding branchpoints turns the story into a choose-your-own-adventure story game, where branchpoints are locations where the plot branches into multiple future plot lines.&nbsp;</p><p dir=\"ltr\">You can then specify the strategy that EnCompass uses to navigate that story game, in search of the best possible ending to the story. This can include launching parallel threads of execution or backtracking to a previous branchpoint when you get stuck in a dead end.<br><br>Users can also plug-and-play a few common search strategies provided by EnCompass out of the box, or define their own custom strategy. For example, you could opt for Monte Carlo tree search, which builds a search tree by balancing exploration and exploitation, or beam search, which keeps the best few outputs from every step. EnCompass makes it easy to experiment with different approaches to find the best strategy to maximize the likelihood of successfully completing your task.</p><p dir=\"ltr\"><strong>The coding efficiency of EnCompass</strong></p><p dir=\"ltr\">So just how code-efficient is EnCompass for adding search to agent programs? According to researchers’ findings, the framework drastically cut down how much programmers needed to add to their agent programs to add search, helping them experiment with different strategies to find the one that performs the best.<br><br>For example, the researchers applied EnCompass to an agent that translates a repository of code from the Java programming language, which is commonly used to program apps and enterprise software, to Python. They found that implementing search with EnCompass — mainly involving adding branchpoint annotations and annotations that record how well each step did — required 348 fewer lines of code (about 82 percent) than implementing it by hand. They also demonstrated how EnCompass enabled them to easily try out different search strategies, identifying the best strategy to be a two-level beam search algorithm, achieving an accuracy boost of 15 to 40 percent across five different repositories at a search budget of 16 times the LLM calls made by the agent without search.</p><p dir=\"ltr\">“As LLMs become a more integral part of everyday software, it becomes more important to understand how to efficiently build software that leverages their strengths and works around their limitations,” says co-author Armando Solar-Lezama, who is an MIT professor of EECS and CSAIL principal investigator. “EnCompass is an important step in that direction.”</p><p dir=\"ltr\">The researchers add that EnCompass targets agents where a program specifies the steps of the high-level workflow; the current iteration of their framework is less applicable to agents that are entirely controlled by an LLM. “In those agents, instead of having a program that specifies the steps and then using an LLM to carry out those steps, the LLM itself decides everything,” says Li. “There is no underlying programmatic workflow, so you can execute inference-time search on whatever the LLM invents on the fly. In this case, there’s less need for a tool like EnCompass that modifies how a program executes with search and backtracking.”</p><p dir=\"ltr\">Li and his colleagues plan to extend EnCompass to more general search frameworks for AI agents. They also plan to test their system on more complex tasks to refine it for real-world uses, including at companies. What’s more, they’re evaluating how well EnCompass helps agents work with humans on tasks like brainstorming hardware designs or translating much larger code libraries. For now, EnCompass is a powerful building block that enables humans to tinker with AI agents more easily, improving their performance.</p><p dir=\"ltr\">“EnCompass arrives at a timely moment, as AI-driven agents and search-based techniques are beginning to reshape workflows in software engineering,” says Carnegie Mellon University Professor Yiming Yang, who wasn’t involved in the research. “By cleanly separating an agent’s programming logic from its inference-time search strategy, the framework offers a principled way to explore how structured search can enhance code generation, translation, and analysis. This abstraction provides a solid foundation for more systematic and reliable search-driven approaches to software development.”&nbsp;&nbsp;</p><p>Li and Solar-Lezama wrote the paper with two Asari AI researchers: Caltech Professor Yisong Yue, an advisor at the company; and senior author Stephan Zheng, who is the founder and CEO. Their work was supported by Asari AI.<br><br>The team’s work was presented at the Conference on Neural Information Processing Systems (NeurIPS) in December.</p>",
      "author": "Alex Shipps | MIT CSAIL",
      "publishedAt": "2026-02-05T21:30:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "1f91e612-a922-4343-97fc-86cb58a9e861",
      "guid": "https://news.mit.edu/2026/brian-hedden-named-co-associate-dean-serc-0204",
      "title": "Brian Hedden named co-associate dean of Social and Ethical Responsibilities of Computing",
      "link": "https://news.mit.edu/2026/brian-hedden-named-co-associate-dean-serc-0204",
      "content": "<p><a href=\"https://www.eecs.mit.edu/people/brian-hedden/\">Brian Hedden</a> PhD ’12 has been appointed co-associate dean of the Social and Ethical Responsibilities of Computing (SERC) at MIT, a cross-cutting initiative in the MIT Schwarzman College of Computing, effective Jan. 16.</p><p>Hedden is a professor in the Department of Linguistics and Philosophy, holding an MIT Schwarzman College of Computing shared position with the Department of Electrical Engineering and Computer Science (EECS). He joined the MIT faculty last fall from the Australian National University and the University of Sydney, where he previously served as a faculty member. He earned his BA from Princeton University and his PhD from MIT, both in philosophy.</p><p>“Brian is a natural and compelling choice for SERC, as a philosopher whose work speaks directly to the intellectual challenges facing education and research today, particularly in computing and AI. His expertise in epistemology, decision theory, and ethics addresses questions that have become increasingly urgent in an era defined by information abundance and artificial intelligence. His scholarship exemplifies the kind of interdisciplinary inquiry that SERC exists to advance,” says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science.</p><p>Hedden’s research focuses on how we ought to form beliefs and make decisions, and it explores how philosophical thinking about rationality can yield insights into contemporary ethical issues, including ethics of AI. He is the author of “Reasons without Persons: Rationality, Identity, and Time” (Oxford University Press, 2015) and articles on topics such as collective action problems, legal standards of proof, algorithmic fairness, and political polarization.</p><p>Joining co-associate dean Nikos Trichakis, the J.C. Penney Professor of Management at the MIT Sloan School of Management, Hedden will help lead SERC and advance the initiative’s ongoing research, teaching, and engagement efforts. He succeeds professor of philosophy <a href=\"https://computing.mit.edu/news/caspar-hare-georgia-perakis-named-associate-deans-of-social-and-ethical-responsibilities-of-computing/\">Caspar Hare</a>, who stepped down at the conclusion of his three-year term on Sept. 1, 2025.</p><p>Since its inception in 2020, SERC has launched a range of programs and activities designed to cultivate responsible “habits of mind and action” among those who create and deploy computing technologies, while fostering the development of technologies in the public interest.</p><p>The <a href=\"https://computing.mit.edu/cross-cutting/social-and-ethical-responsibilities-of-computing/serc-scholars-program/\">SERC Scholars Program</a> invites undergraduate and graduate students to work alongside postdoctoral mentors to explore interdisciplinary ethical challenges in computing. The initiative also hosts an <a href=\"https://computing.mit.edu/cross-cutting/social-and-ethical-responsibilities-of-computing/envisioning-the-future-of-computing-prize-home/\">annual prize competition</a> that challenges MIT students to envision the future of computing, publishes a twice-yearly series of <a href=\"https://mit-serc.pubpub.org/\">case studies</a>, and collaborates on coordinated <a href=\"https://computing.mit.edu/news/learning-to-think-critically-about-machine-learning/\">curricular materials</a>, including active-learning projects, homework assignments, and in-class demonstrations. In 2024, SERC introduced a <a href=\"https://computing.mit.edu/news/inaugural-serc-seed-grants-aim-to-advance-ethical-and-responsible-computing-practices/\">new seed grant program</a> to support MIT researchers investigating ethical technology development; to date, <a href=\"https://computing.mit.edu/news/second-round-of-serc-seed-grants-award-to-mit-scholars/\">two rounds</a> of grants have been awarded to 24 projects.</p>",
      "author": "Amanda Diehl | MIT Schwarzman College of Computing",
      "publishedAt": "2026-02-04T18:25:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "a75ed97a-e320-478b-bf57-786758f9536d",
      "guid": "https://news.mit.edu/2026/antonio-torralba-three-mit-alumni-named-acm-fellows-0204",
      "title": "Antonio Torralba, three MIT alumni named 2025 ACM fellows",
      "link": "https://news.mit.edu/2026/antonio-torralba-three-mit-alumni-named-acm-fellows-0204",
      "content": "<p dir=\"ltr\">Antonio Torralba, Delta Electronics Professor of Electrical Engineering and Computer Science and faculty head of artificial intelligence and decision-making at MIT, has been named to the 2025 cohort of Association for Computing Machinery (ACM) Fellows. He shares the honor of an ACM Fellowship with three MIT alumni: Eytan Adar ’97, MEng ’98; George Candea ’97, MEng ’98; and Gookwon Edward Suh SM ’01, PhD ’05.</p><p dir=\"ltr\">A principal investigator within the Computer Science and Artificial Intelligence Laboratory, Torralba received his BS in telecommunications engineering from the Universitat Politècnica de Catalunya, in Spain, in 1994, and a PhD in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, in France, in 2000. At different points in his MIT career, he has been director of both the MIT Quest for Intelligence (now the MIT Siegel Family Quest for Intelligence) and the MIT-IBM Watson AI Lab.&nbsp;</p><p dir=\"ltr\">Torralba’s research focuses on computer vision, machine learning, and human visual perception; as he puts it, “I am interested in building systems that can perceive the world like humans do.” Alongside Phillip Isola and William Freeman, he recently co-authored&nbsp;“Foundations of Computer Vision,” an 800-plus page textbook exploring the foundations and core principles of the field.&nbsp;</p><p dir=\"ltr\">Among other awards and recognitions, he is the recipient of the 2008 National Science Foundation Career award; the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition; the 2017 Frank Quick Faculty Research Innovation Fellowship; the Louis D. Smullin (’39) Award for Teaching Excellence; and the 2020 PAMI Mark Everingham Prize. In 2021, he was awarded the inaugural Thomas Huang Memorial Prize by the Pattern Analysis and Machine Intelligence Technical Committee and was named a fellow of the Association for the Advancement of Artificial Intelligence. In 2022, he received an honorary doctoral degree from the Universitat Politècnica de Catalunya.&nbsp;</p><p>ACM fellows, the highest honor bestowed by the professional organization, are registered members of the society selected by their peers for&nbsp;outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.</p>",
      "author": "Jane Halpern | Department of Electrical Engineering and Computer Science",
      "publishedAt": "2026-02-04T18:15:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "50713e9b-7b53-4b5a-9591-1b4faa248abc",
      "guid": "https://news.mit.edu/2026/3-questions-using-ai-to-accelerate-discovery-design-therapeutic-drugs-0204",
      "title": "3 Questions: Using AI to accelerate the discovery and design of therapeutic drugs",
      "link": "https://news.mit.edu/2026/3-questions-using-ai-to-accelerate-discovery-design-therapeutic-drugs-0204",
      "content": "<p><em>In the pursuit of solutions to complex global challenges including disease, energy demands, and climate change, scientific researchers, including at MIT, have turned to artificial intelligence, and to quantitative analysis and modeling, to design and construct engineered cells with novel properties. The engineered cells can be programmed to become new therapeutics — battling, and perhaps eradicating, diseases.</em></p><p><a href=\"https://imes.mit.edu/people/collins-james\"><em>James J. Collins</em></a><em> is one of the founders of the field of synthetic biology, and is also a leading researcher in systems biology, the interdisciplinary approach that uses mathematical analysis and modeling of complex systems to better understand biological systems. His research has led to the development of new classes of diagnostics and therapeutics, including in the detection and treatment of pathogens like Ebola, Zika, SARS-CoV-2, and antibiotic-resistant bacteria. Collins, the Termeer Professor of Medical Engineering and Science and professor of biological engineering at MIT, is a core faculty member of the Institute for Medical Engineering and Science (IMES), the director of the MIT Abdul Latif Jameel Clinic for Machine Learning in Health, as well as an institute member of the Broad Institute of MIT and Harvard, and core founding faculty at the Wyss Institute for Biologically Inspired Engineering, Harvard.</em></p><p><em>In this Q&amp;A, Collins speaks about his latest work and goals for this research.</em></p><p><strong>Q.&nbsp;</strong><em>&nbsp;</em>You’re known for collaborating with colleagues across MIT, and at other institutions. How have these collaborations and affiliations helped you with your research?&nbsp;</p><p><strong>A: </strong>Collaboration has been central to the work in my <a href=\"https://www.collinslab.mit.edu/\">lab</a>. At the <a href=\"https://jclinic.mit.edu/\">MIT Jameel Clinic for Machine Learning in Health</a>, I formed a collaboration with <a href=\"https://imes.mit.edu/people/barzilay-regina\">Regina Barzilay</a> [the Delta Electronics Professor in the MIT Department of Electrical Engineering and Computer Science and affiliate faculty member at IMES] and <a href=\"https://people.csail.mit.edu/tommi/\">Tommi Jaakkola</a> [the Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society] to use deep learning to discover new antibiotics. This effort combined our expertise in artificial intelligence, network biology, and systems microbiology, leading to the discovery of halicin, a potent new antibiotic effective against a broad range of multidrug-resistant bacterial pathogens. Our results were published in <em>Cell</em> in 2020 and showcased the power of bringing together complementary skill sets to tackle a global health challenge.</p><p>At the <a href=\"https://wyss.harvard.edu/\">Wyss Institute,</a> I’ve worked closely with <a href=\"https://wyss.harvard.edu/team/core-faculty/donald-ingber/\">Donald Ingber</a> [the Judah Folkman Professor of Vascular Biology at Harvard Medical School and the Vascular Biology Program at Boston Children’s Hospital, and Hansjörg Wyss Professor of Biologically Inspired Engineering at Harvard], leveraging his organs-on-chips technology to test the efficacy of AI-discovered and AI-generated antibiotics. These platforms allow us to study how drugs behave in human tissue-like environments, complementing traditional animal experiments and providing a more nuanced view of their therapeutic potential.</p><p>The common thread across our many collaborations is the ability to combine computational predictions with cutting-edge experimental platforms, accelerating the path from ideas to validated new therapies.</p><p><strong>Q.&nbsp;</strong>Your research has led to many advances in designing novel antibiotics, using generative AI and deep learning. Can you talk about some of the advances you’ve been a part of in the development of drugs that can battle multi-drug-resistant pathogens, and what you see on the horizon for breakthroughs in this arena?</p><p><strong>A: </strong>In 2025, <a href=\"https://www.collinslab.mit.edu/\">our lab</a> published a study in&nbsp;<a href=\"https://static1.squarespace.com/static/5c264953620b850c9fb03732/t/68fc262685146f4e0f25ef77/1761355336069/cell_krishnan_small.pdf\"><em>Cell</em> demonstrating how generative AI</a> can be used to design completely new antibiotics from scratch. We used genetic algorithms and variational autoencoders to generate millions of candidate molecules, exploring both fragment-based designs and entirely unconstrained chemical space. After computational filtering, retrosynthetic modeling, and medicinal chemistry review, we synthesized 24 compounds and tested them experimentally. Seven showed selective antibacterial activity. One lead, NG1, was highly narrow-spectrum, eradicating multi-drug-resistant&nbsp;<em>Neisseria gonorrhoeae</em>, including strains resistant to first-line therapies, while sparing commensal species. Another, DN1, targeted methicillin-resistant <em>Staphylococcus aureus</em> (MRSA) and cleared infections in mice through broad membrane disruption. Both were non-toxic and showed low rates of resistance.</p><p>Looking ahead, we are using deep learning to design antibiotics with drug-like properties that make them stronger candidates for clinical development. By integrating AI with high-throughput biological testing, we aim to accelerate the discovery and design of antibiotics that are novel, safe, and effective, ready for real-world therapeutic use. This approach could transform how we respond to drug-resistant bacterial pathogens, moving from a reactive to a proactive strategy in antibiotic development.</p><p><strong>Q.&nbsp;</strong>You’re a co-founder of <a href=\"https://www.pharebio.org/\">Phare Bio,</a> a nonprofit organization that uses AI to discover new antibiotics, and the Collins Lab has helped to launch the Antibiotics-AI Project in collaboration with Phare Bio. Can you tell us more about what you hope to accomplish with these collaborations, and how they tie back to your research goals?</p><p><strong>A: </strong>We founded Phare Bio as a nonprofit to take the most promising antibiotic candidates emerging from the Antibiotics-AI Project at MIT and advance them toward the clinic. The idea is to bridge the gap between discovery and development by collaborating with biotech companies, pharmaceutical partners, AI companies, philanthropies, other nonprofits, and even nation states. Akhila Kosaraju has been doing a brilliant job leading Phare Bio, coordinating these efforts and moving candidates forward efficiently.</p><p>Recently, we received a grant from ARPA-H to use generative AI to design 15 new antibiotics and develop them as pre-clinical candidates. This project builds directly on our lab’s research, combining computational design with experimental testing to create novel antibiotics that are ready for further development. By integrating generative AI, biology, and translational partnerships, we hope to create a pipeline that can respond more rapidly to the global threat of antibiotic resistance, ultimately delivering new therapies to patients who need them most.</p>",
      "author": "Mindy Blodgett | Institute for Medical Engineering and Science",
      "publishedAt": "2026-02-04T18:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "68038af5-ce70-43f7-96ca-cda6be14aab5",
      "guid": "https://news.mit.edu/2026/katie-spivakovsky-wins-churchill-scholarship-0203",
      "title": "Katie Spivakovsky wins 2026 Churchill Scholarship",
      "link": "https://news.mit.edu/2026/katie-spivakovsky-wins-churchill-scholarship-0203",
      "content": "<p>MIT senior Katie Spivakovsky has been selected as a 2026-27 Churchill Scholar and will undertake an MPhil in biological sciences at the Wellcome Sanger Institute at Cambridge University in the U.K. this fall.</p><p>Spivakovsky, who is double-majoring in biological engineering and artificial intelligence, with minors in mathematics and biology, aims to integrate computation and bioengineering in an academic research career focused on developing robust, scalable solutions that promote equitable health outcomes.</p><p>At MIT’s Bathe BioNanoLab, Spivakovsky investigates therapeutic applications of DNA origami, DNA-scaffolded nanoparticles for gene and mRNA delivery, and co-authored a manuscript in press at&nbsp;<em>Science</em>. She leads the development of an immune therapy for cancer cachexia with a team supported by MIT’s BioMakerSpace; this work earned a silver medal at the international synthetic biology competition iGEM and was published in the&nbsp;<em>MIT Undergraduate Research Journal</em>. Previously, she worked on Merck’s Modeling&nbsp;&amp;&nbsp;Informatics team, characterizing a cancer-associated protein mutation, and at the New York Structural Biology Center, where she improved cryogenic electron microscopy particle detection models.</p><p>On campus, Spivakovsky serves as director of the Undergraduate Initiative in the MIT Biotech Group. She is deeply committed to teaching and mentoring, and has served as a lecturer and co-director for&nbsp;class 6.S095 (Probability Problem Solving), a teaching assistant for&nbsp;classes 20.309 (Bioinstrumentation) and&nbsp;20.A06 (Hands-on Making in Biological Engineering), a lab assistant for&nbsp;6.300 (Signal Processing), and as an associate advisor.</p><p>“Katie is a brilliant researcher who has a keen intellectual curiosity that will make her a leader in biological engineering in the future. We are proud that she will be representing MIT at Cambridge University,” says Kim Benard, associate dean of distinguished fellowships.</p><p>The Churchill Scholarship is a highly competitive fellowship that annually offers 16 American students the opportunity to pursue a funded graduate degree in science, mathematics, or engineering at Churchill College within Cambridge University. The scholarship, established in 1963, honors former British Prime Minister Winston Churchill’s vision for U.S.-U.K. scientific exchange.&nbsp;Since 2017, two Kanders Churchill Scholarships have also been awarded each year for studies in science policy.</p><p>MIT students interested in learning more about the Churchill Scholarship should contact Kim Benard in MIT Career Advising and Professional Development.</p>",
      "author": "Julia Mongo | Office of Distinguished Fellowships",
      "publishedAt": "2026-02-03T22:25:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "781079cd-0775-4560-b1a1-c75221796de4",
      "guid": "https://news.mit.edu/2026/counter-intelligence-kitchen-cosmo-0203",
      "title": "Counter intelligence",
      "link": "https://news.mit.edu/2026/counter-intelligence-kitchen-cosmo-0203",
      "content": "<p>How can artificial intelligence step out of a screen and become something we can physically touch and interact with?</p><p>That question formed the foundation of class 4.043/4.044 (Interaction Intelligence), an MIT course focused on designing a new category of AI-driven interactive objects. Known as large language objects (LLOs), these physical interfaces extend large language models into the real world. Their behaviors can be deliberately generated for specific people or applications, and their interactions can evolve from simple to increasingly sophisticated — providing meaningful support for both novice and expert users.</p><p>“I came to the realization&nbsp;that, while powerful, these new forms of intelligence&nbsp;still remain largely ignorant of the world outside of language,” says Marcelo Coelho, associate professor of the practice in the MIT Department of Architecture, who has been teaching the design studio for several years and directs the&nbsp;<a href=\"https://designintelligence.mit.edu/\" target=\"_blank\">Design Intelligence Lab</a>. “They lack real-time, contextual understanding of our physical surroundings, bodily experiences, and social relationships to be truly intelligent. In contrast, LLOs are physically situated and interact in real time with their physical environment. The course is an attempt to both address this gap and develop a new kind of design discipline for the age of AI.”</p><p>Given the assignment to design an interactive device that they would want in their lives,&nbsp;students Jacob Payne and Ayah Mahmoud focused on the kitchen. While they each enjoy cooking and baking, their design inspiration came from the first home computer: the&nbsp;<a href=\"https://www.computerhistory.org/revolution/minicomputers/11/362\" target=\"_blank\">Honeywell 316 Kitchen Computer</a>, marketed by Neiman Marcus in 1969. Priced at $10,000, there is no record of one ever being sold.</p><p>“It was an ambitious but impractical early attempt at a home kitchen computer,” says Payne, an architecture graduate student. “It made an intriguing historical reference for the project.”</p><p>“As somebody who likes learning to cook — especially now, in college as an undergrad — the thought of designing something that makes cooking easy for those who might not have a cooking background and just wants a nice meal that satisfies their cravings was a great starting point for me,” says&nbsp;Mahmoud, a senior design major.</p><p>“We thought about the leftover ingredients you have in the refrigerator or pantry, and how AI could help you find new creative uses for things that you may otherwise throw away,” says Payne.</p><p><strong>Generative cuisine</strong></p><p>The students designed their device — named&nbsp;<a href=\"https://designintelligence.mit.edu/work/kitchen-cosmo\" target=\"_blank\">Kitchen Cosmo</a> — with instructions to function as a “recipe generator.” One challenge was prompting the LLM to consistently acknowledge real-world cooking parameters, such as heating, timing, or temperature. One issue they worked out was having the LLM recognize flavor profiles and spices accurate to regional and cultural dishes around the world to support a wider range of cuisines. Troubleshooting included taste-testing recipes Kitchen Cosmo generated. Not every early recipe produced a winning dish.</p><p>“There were lots of small things that AI wasn't great at conceptually understanding,” says Mahmoud. “An LLM needs to fundamentally understand human taste to make a great meal.”</p><p>They fine-tuned their device to allow for the myriad ways people approach preparing a meal. Is this breakfast, lunch, dinner, or a snack? How advanced of a cook are you? How much meal prep time do you have? How many servings will you make? Dietary preferences were also programmed, as well as the type of mood or vibe you want to achieve. Are you feeling nostalgic, or are you in a celebratory mood? There’s a dial for that.</p><p>“These selections were the focal point of the device because we were curious to see how the LLM would interpret subjective adjectives as inputs and use them to transform the type of recipe outputs we would get,” says Payne.</p><p>Unlike most AI interactions that tend to be invisible, Payne and Mahmoud wanted their device to be more of a “partner” in the kitchen. The tactile interface was intentionally designed to structure the interaction, giving users a physical control over how the AI responded.</p><p>“While I’ve worked with electronics and hardware before, this project pushed me to integrate the components with a level of precision and refinement that felt much closer to a product-ready device,” says Payne of the course work.</p><p><strong>Retro and red</strong></p><p>After their electronic work was completed, the students designed a series of models using cardboard until settling on the final look, which Payne describes as “retro.” The body was designed in a 3D modeling software and printed. In a nod to the original Honeywell computer, they painted it red.</p><p>A thin, rectangular device about 18 inches in height, Kitchen Cosmo has a webcam that hinges open to scan ingredients set on a counter. It translates these into a recipe that takes into consideration general spices and condiments common in most households. An integrated thermal printer delivers a printed recipe that is torn off. Recipes can be stored in a plastic receptacle on its base.</p><p>While Kitchen Cosmo made a modest splash in design magazines, both students have ideas where they will take future iterations.</p><p>Payne would like to see it “take advantage of a lot of the data we have in the kitchen and use AI as a mediator, offering tips for how to improve on what you’re cooking at that moment.”</p><p>Mahmoud is looking at how to optimize Kitchen Cosmo for her thesis. Classmates have given feedback to upgrade its abilities. One suggestion is to provide multi-person instructions that give several people tasks needed to complete a recipe. Another idea is to create a “learning mode” in which a kitchen tool — for example, a paring knife — is set in front of Kitchen Cosmo, and it delivers instructions on how to use the tool. Mahmoud has been researching food science history as well.</p><p>“I’d like to get a better handle on how to train AI to fully understand food so it can tailor recipes to a user’s liking,” she says.</p><p>Having begun her MIT education as a geologist, Mahmoud’s pivot to design has been a revelation, she says. Each design class has been inspiring. Coelho’s course was her first class to include designing with AI. Referencing the often-mentioned analogy of “drinking from a fireho<del cite=\"mailto:Adelaide%20Zollinger\" datetime=\"2026-01-15T15:11\"><s>u</s></del>se” while a student at MIT, Mahmoud says the course helped define a path for her in product design.</p><p>“For the first time, in that class, I felt like I was finally drinking as much as I could and not feeling overwhelmed. I see myself doing design long-term, which is something I didn’t think I would have said previously about technology.”&nbsp;</p>",
      "author": "Maria Iacobo | School of Architecture and Planning",
      "publishedAt": "2026-02-03T22:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "91540a4d-f769-4cee-af12-c2808eb399ab",
      "guid": "https://news.mit.edu/2026/smart-launches-wearable-imaging-transforming-elderly-care-research-group-0203",
      "title": "SMART launches new Wearable Imaging for Transforming Elderly Care research group",
      "link": "https://news.mit.edu/2026/smart-launches-wearable-imaging-transforming-elderly-care-research-group-0203",
      "content": "<p>What if ultrasound imaging is no longer confined to hospitals? Patients with chronic conditions, such as hypertension and heart failure, could be monitored continuously in real-time at home or on the move, giving health care practitioners ongoing clinical insights instead of the occasional snapshots — a scan here and a check-up there. This shift from reactive, hospital-based care to preventative, community and home-based care could enable earlier detection and timely intervention, and truly personalized care.</p><p>Bringing this vision to reality, the&nbsp;<a href=\"https://smart.mit.edu/\">Singapore-MIT Alliance for Research and Technology</a> (SMART), MIT’s research enterprise in Singapore, has launched a new collaborative research project:&nbsp;<a href=\"https://smart.mit.edu/research/witec/about-witec\">Wearable Imaging for Transforming Elderly Care</a> (WITEC).&nbsp;</p><p>WITEC marks a pioneering effort in wearable technology, medical imaging, research, and materials science. It will be dedicated to foundational research and development of the world’s first wearable ultrasound imaging system capable of 48-hour intermittent cardiovascular imaging for continuous and real-time monitoring and diagnosis of chronic conditions such as hypertension and heart failure.&nbsp;</p><p>This multi-million dollar, multi-year research program, supported by the National Research Foundation (NRF) Singapore under its Campus for Research Excellence and Technological Enterprise program, brings together top researchers and expertise from MIT, Nanyang Technological University (NTU Singapore), and the National University of Singapore (NUS). Tan Tock Seng Hospital (TTSH) is WITEC’s clinical collaborator and will conduct patient trials to validate long-term heart imaging for chronic cardiovascular disease management.</p><p>“Addressing society’s most pressing challenges requires innovative, interdisciplinary thinking. Building on SMART’s long legacy in Singapore as a hub for research and innovation, WITEC will harness interdisciplinary expertise — from MIT and leading institutions in Singapore — to advance transformative research that creates real-world impact and benefits Singapore, the U.S., and societies all over. This is the kind of collaborative research that not only pushes the boundaries of knowledge, but also redefines what is possible for the future of health care,” says Bruce Tidor, chief executive officer and interim director of SMART, who is also an MIT professor of biological engineering and electrical engineering and computer science.</p><p><strong>Industry-leading precision equipment and capabilities</strong></p><p>To support this work, WITEC’s laboratory is equipped with advanced tools, including Southeast Asia’s first sub-micrometer 3D printer and the latest Verasonics Vantage NXT 256 ultrasonic imaging system, which is the first unit of its kind in Singapore.</p><p>Unlike conventional 3D printers that operate at millimeter or micrometer scales, WITEC’s 3D printer can achieve sub‑micrometer resolution, allowing components to be fabricated at the level of single cells or tissue structures. With this capability, WITEC researchers can prototype bioadhesive materials and device interfaces with unprecedented accuracy — essential to ensuring skin‑safe adhesion and stable, long‑term imaging quality.</p><p>Complementing this is the latest Verasonics ultrasonic imaging system. Equipped with a new transducer adapter and supporting a significantly larger number of probe control channels than existing systems, it gives researchers the freedom to test highly customized imaging methods. This allows more complex beamforming, higher‑resolution image capture, and integration with AI‑based diagnostic models — opening the door to long‑duration, real‑time cardiovascular imaging not possible with standard hospital equipment.</p><p>Together, these technologies allow WITEC to accelerate the design, prototyping, and testing of its wearable ultrasound imaging system, and to demonstrate imaging quality on phantoms and healthy subjects.</p><p><strong>Transforming chronic disease care through wearable innovation&nbsp;</strong></p><p>Chronic diseases<strong> </strong><a href=\"https://www.nature.com/articles/s43587-025-00915-0\">are rising rapidly in Singapore</a> and globally, especially among the aging population and individuals with multiple long-term conditions. This trend highlights the urgent need for effective home-based care and easy-to-use monitoring tools that go beyond basic wellness tracking.</p><p>Current consumer wearables, such as smartwatches and fitness bands, offer limited physiological data like heart rate or step count. While useful for general health, they lack the depth needed to support chronic disease management. Traditional ultrasound systems, although clinically powerful, are bulky, operator-dependent, can only be deployed episodically within the hospitals, and are limited to snapshots in time, making them unsuitable for long-term, everyday use.</p><p>WITEC aims to bridge this gap with its wearable ultrasound imaging system that uses bioadhesive technology to enable up to 48 hours of uninterrupted imaging. Combined with AI-enhanced diagnostics, the innovation is aimed at supporting early detection, home-based pre-diagnosis, and continuous monitoring of chronic diseases.</p><p>Beyond improving patient outcomes, this innovation could help ease labor shortages by freeing up ultrasound operators, nurses, and doctors to focus on more complex care, while reducing demand for hospital beds and resources. By shifting monitoring to homes and communities, WITEC’s technology will enable patient self-management and timely intervention, potentially lowering health-care costs and alleviating the increasing financial and manpower pressures of an aging population.</p><p><strong>Driving innovation through interdisciplinary collaboration</strong></p><p>WITEC is led by the following co-lead principal investigators: Xuanhe Zhao, professor of mechanical engineering and professor of civil and environmental engineering at MIT; Joseph Sung, senior vice president of health and life sciences at NTU Singapore and dean of the Lee Kong Chian School of Medicine (LKCMedicine); Cher Heng Tan, assistant dean of clinical research at LKCMedicine; Chwee Teck Lim, NUS Society Professor of Biomedical Engineering at NUS and director of the Institute for Health Innovation and Technology at NUS; and Xiaodong Chen, distinguished university professor at the School of Materials Science and Engineering within NTU.&nbsp;</p><p>“We’re extremely proud to bring together an exceptional team of researchers from Singapore and the U.S. to pioneer core technologies that will make wearable ultrasound imaging a reality. This endeavor combines deep expertise in materials science, data science, AI diagnostics, biomedical engineering, and clinical medicine. Our phased approach will accelerate translation into a fully wearable platform that reshapes how chronic diseases are monitored, diagnosed and managed,” says Zhao, who serves as a co-lead PI of WITEC.</p><p><strong>Research roadmap with broad impact across health care, science, industry, and economy</strong></p><p>Bringing together leading experts across interdisciplinary fields, WITEC will advance foundational work in soft materials, transducers, microelectronics, data science and AI diagnostics, clinical medicine, and biomedical engineering. As a deep-tech R&amp;D group, its breakthroughs will have the potential to drive innovation in health-care technology and manufacturing, diagnostics, wearable ultrasonic imaging, metamaterials, diagnostics, and AI-powered health analytics. WITEC’s work is also expected to accelerate growth in high-value jobs across research, engineering, clinical validation, and health-care services, and attract strategic investments that foster biomedical innovation and industry partnerships in Singapore, the United States, and beyond.</p><p>“Chronic diseases present significant challenges for patients, families, and health-care systems, and with aging populations such as Singapore, those challenges will only grow without new solutions. Our research into a wearable ultrasound imaging system aims to transform daily care for those living with cardiovascular and other chronic conditions — providing clinicians with richer, continuous insights to guide treatment, while giving patients greater confidence and control over their own health. WITEC’s pioneering work marks an important step toward shifting care from episodic, hospital-based interventions to more proactive, everyday management in the community,” says Sung, who serves as co‑lead PI of WITEC.</p><p>Led by Violet Hoon, senior consultant at TTSH, clinical trials are expected to commence this year to validate long-term heart monitoring in the management of chronic cardiovascular disease. Over the next three years, WITEC aims to develop a fully integrated platform capable of 48-hour intermittent imaging through innovations in bioadhesive couplants, nanostructured metamaterials, and ultrasonic transducers.</p><p>As MIT’s research enterprise in Singapore, SMART is committed to advancing breakthrough technologies that address pressing global challenges. WITEC adds to SMART’s existing research endeavors that foster a rich exchange of ideas through collaboration with leading researchers and academics from the United States, Singapore, and around the world in key areas such as antimicrobial resistance, cell therapy development, precision agriculture, AI, and 3D-sensing technologies.</p>",
      "author": "Singapore-MIT Alliance for Research and Technology",
      "publishedAt": "2026-02-03T15:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "d9ab2a24-24d3-49ea-bf19-87df1dfbc9e7",
      "guid": "https://news.mit.edu/2026/how-generative-ai-can-help-scientists-synthesize-complex-materials-0202",
      "title": "How generative AI can help scientists synthesize complex materials",
      "link": "https://news.mit.edu/2026/how-generative-ai-can-help-scientists-synthesize-complex-materials-0202",
      "content": "<p>Generative artificial intelligence models have been used to create enormous libraries of theoretical materials that could help solve all kinds of problems. Now, scientists just have to figure out how to make them.</p><p>In many cases, materials synthesis is not as simple as following a recipe in the kitchen. Factors like the temperature and length of processing can yield huge changes in a material’s properties that make or break its performance. That has limited researchers’ ability to test millions of promising model-generated materials.</p><p>Now, MIT researchers have created an AI model that guides scientists through the process of making materials by suggesting promising synthesis routes. In a new paper, they showed the model delivers state-of-the-art accuracy in predicting effective synthesis pathways for a class of materials called zeolites, which could be used to improve catalysis, absorption, and ion exchange processes. Following its suggestions, the team synthesized a new zeolite material that showed improved thermal stability.</p><p>The researchers believe their new model could break the biggest bottleneck in the materials discovery process.</p><p>“To use an analogy, we know what kind of cake we want to make, but right now we don’t know how to bake the cake,” says lead author Elton Pan, a PhD candidate in MIT’s Department of Materials Science and Engineering (DMSE). “Materials synthesis is currently done through domain expertise and trial and error.”</p><p>The paper describing the work <a href=\"https://www.nature.com/articles/s43588-025-00949-9\" target=\"_blank\">appears today in <em>Nature Computational Science</em></a>. Joining Pan on the paper are Soonhyoung Kwon ’20, PhD ’24; DMSE postdoc Sulin Liu; chemical engineering PhD student Mingrou Xie; DMSE postdoc Alexander J. Hoffman; Research Assistant Yifei Duan SM ’25; DMSE visiting student Thorben Prein; DMSE PhD candidate Killian Sheriff; MIT Robert T. Haslam Professor in Chemical Engineering Yuriy Roman-Leshkov; Valencia Polytechnic University Professor Manuel Moliner; MIT Paul M. Cook Career Development Professor Rafael Gómez-Bombarelli; and MIT Jerry McAfee Professor in Engineering Elsa Olivetti.</p><p><strong>Learning to bake</strong></p><p>Massive investments in generative AI have led companies like Google and Meta to create huge databases filled with material recipes that, at least theoretically, have properties like high thermal stability and selective absorption of gases. But making those materials can require weeks or months of careful experiments that test specific reaction temperatures, times, precursor ratios, and other factors.</p><p>“People rely on their chemical intuition to guide the process,” Pan says. “Humans are linear. If there are five parameters, we might keep four of them constant and vary one of them linearly. But machines are much better at reasoning in a high-dimensional space.”</p><p>The synthesis process of materials discovery now often takes the most time in a material’s journey from hypothesis to use.</p><p>To help scientists navigate that process, the MIT researchers trained a generative AI model on over 23,000 material synthesis recipes described over 50 years of scientific papers. The researchers iteratively added random “noise” to the recipes during training, and the model learned to de-noise and sample from the random noise to find promising synthesis routes.</p><p>The result is DiffSyn, which uses an approach in AI known as diffusion.</p><p>“Diffusion models are basically a generative AI model like ChatGPT, but more like the DALL-E image generation model,” Pan says. “During inference, it converts noise into meaningful structure by subtracting a little bit of noise at each step. In this case, the ‘structure’ is the synthesis route for a desired material.”</p><p>When a scientist using DiffSyn enters a desired material structure, the model offers some promising combinations of reaction temperatures, reaction times, precursor ratios, and more.</p><p>“It basically tells you how to bake your cake,” Pan says. “You have a cake in mind, you feed it into the model, the model spits out the synthesis recipes. The scientist can pick whichever synthesis path they want, and there are simple ways to quantify the most promising synthesis path from what we provide, which we show in our paper.”</p><p>To test their system, the researchers used DiffSyn to suggest novel synthesis paths for a zeolite, a material class that is complex and takes time to form into a testable material.</p><p>“Zeolites have a very high-dimensional synthesis space,” Pan says. “Zeolites also tend to take days or weeks to crystallize, so the impact [of finding the best synthesis pathway faster] is much higher than other materials that crystallize in hours.”</p><p>The researchers were able to make the new zeolite material using synthesis pathways suggested by DiffSyn. Subsequent testing revealed the material had a promising morphology for catalytic applications.</p><p>“Scientists have been trying out different synthesis recipes one by one,” Pan says. “That makes them very time-consuming. This model can sample 1,000 of them in under a minute. It gives you a very good initial guess on synthesis recipes for completely new materials.”</p><p><strong>Accounting for complexity</strong></p><p>Previously, researchers have built machine-learning models that mapped a material to a single recipe. Those approaches do not take into account that there are different ways to make the same material.</p><p>DiffSyn is trained to map material structures to many different possible synthesis paths. Pan says that is better aligned with experimental reality.</p><p>“This is a paradigm shift away from one-to-one mapping between structure and synthesis to one-to-many mapping,” Pan says. “That’s a big reason why we achieved strong gains on the benchmarks.”</p><p>Moving forward, the researchers believe the approach should work to train other models that guide the synthesis of materials outside of zeolites, including metal-organic frameworks, inorganic solids, and other materials that have more than one possible synthesis pathway.</p><p>“This approach could be extended to other materials,” Pan says. “Now, the bottleneck is finding high-quality data for different material classes. But zeolites are complicated, so I can imagine they are close to the upper-bound of difficulty. Eventually, the goal would be interfacing these intelligent systems with autonomous real-world experiments, and agentic reasoning on experimental feedback to dramatically accelerate the process of materials design.”</p><p>The work was supported by MIT International Science and Technology Initiatives (MISTI), the National Science Foundation, Generalitat Vaslenciana, the Office of Naval Research, ExxonMobil, and the Agency for Science, Technology and Research in Singapore.</p>",
      "author": "Zach Winn | MIT News",
      "publishedAt": "2026-02-02T10:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "e79ede9a-cada-4142-92ce-b5ce2fa52066",
      "guid": "https://news.mit.edu/2026/philosophical-puzzle-rational-artificial-intelligence-0130",
      "title": "The philosophical puzzle of rational artificial intelligence",
      "link": "https://news.mit.edu/2026/philosophical-puzzle-rational-artificial-intelligence-0130",
      "content": "<p>To what extent can an artificial system be rational?</p><p>A new MIT course, <a href=\"https://computing.mit.edu/cross-cutting/common-ground-for-computing-education/common-ground-subjects/ai-and-rationality/\">6.S044/24.S00</a> (AI and Rationality), doesn’t seek to answer this question. Instead, it challenges students to explore this and other philosophical problems through the lens of AI research. For the next generation of scholars, concepts of rationality and agency could prove integral in AI decision-making, especially when influenced by how humans understand their own cognitive limits and their constrained, subjective views of what is or isn’t rational.</p><p>This inquiry is rooted in a deep relationship between computer science and philosophy, which have long collaborated in formalizing what it is to form rational beliefs, learn from experience, and make rational decisions in pursuit of one's goals.</p><p>“You’d imagine computer science and philosophy are pretty far apart, but they’ve always intersected. The technical parts of philosophy really overlap with AI, especially early AI,” says course instructor Leslie Kaelbling, the Panasonic Professor of Computer Science and Engineering at MIT, calling to mind Alan Turing, who was both a computer scientist and a philosopher. Kaelbling herself holds an undergraduate degree in philosophy from Stanford University, noting that computer science wasn’t available as a major at the time.</p><p>Brian Hedden, a professor in the Department of Linguistics and Philosophy, holding an MIT Schwarzman College of Computing shared position with the Department of Electrical Engineering and Computer Science (EECS), who teaches the class with Kaelbling, notes that the two disciplines are more aligned than people might imagine, adding that the “differences are in emphasis and perspective.”</p><p><strong>Tools for further theoretical thinkin</strong>g</p><p>Offered for the first time in fall 2025, Kaelbling and Hedden created AI and Rationality as part of the <a href=\"https://computing.mit.edu/cross-cutting/common-ground-for-computing-education/\">Common Ground for Computing Education,</a> a cross-cutting initiative of the MIT Schwarzman College of Computing that brings multiple departments together to develop and teach new courses and launch new programs that blend computing with other disciplines.</p><p>With over two dozen students registered, AI and Rationality is one of two Common Ground classes with a foundation in philosophy, the other being <a href=\"https://news.mit.edu/2025/bridging-philosophy-and-ai-to-explore-computing-ethics-0211\">6.C40/24.C40 (Ethics of Computing)</a>.</p><p>While Ethics of Computing explores concerns about the societal impacts of rapidly advancing technology, AI and Rationality examines the disputed definition of rationality by considering several components: the nature of rational agency, the concept of a fully autonomous and intelligent agent, and the ascription of beliefs and desires onto these systems.</p><p>Because AI is extremely broad in its implementation and each use case raises different issues, Kaelbling and Hedden brainstormed topics that could provide fruitful discussion and engagement between the two perspectives of computer science and philosophy.</p><p>“It's important when I work with students studying machine learning or robotics that they step back a bit and examine the assumptions they’re making,” Kaelbling says. “Thinking about things from a philosophical perspective helps people back up and understand better how to situate their work in actual context.”</p><p>Both instructors stress that this isn’t a course that provides concrete answers to questions on what it means to engineer a rational agent.</p><p>Hedden says, “I see the course as building their foundations. We’re not giving them a body of doctrine to learn and memorize and then apply. We’re equipping them with tools to think about things in a critical way as they go out into their chosen careers, whether they’re in research or industry or government.”</p><p>The rapid progress of AI also presents a new set of challenges in academia. Predicting what students may need to know five years from now is something Kaelbling sees as an impossible task. “What we need to do is give them the tools at a higher level — the habits of mind, the ways of thinking — that will help them approach the stuff that we really can’t anticipate right now,” she says.</p><p><strong>Blending disciplines and questioning assumptions</strong></p><p>So far, the class has drawn students from a wide range of disciplines — from those firmly grounded in computing to others interested in exploring how AI intersects with their own fields of study.</p><p>Throughout the semester’s reading and discussions, students grappled with different definitions of rationality and how they pushed back against assumptions in their fields.</p><p>On what surprised her about the course, Amanda Paredes Rioboo, a senior in EECS, says, “We’re kind of taught that math and logic are this golden standard or truth. This class showed us a variety of examples that humans act inconsistently with these mathematical and logical frameworks. We opened up this whole can of worms as to whether, is it humans that are irrational? Is it the machine learning systems that we designed that are irrational? Is it math and logic itself?”</p><p>Junior Okoroafor, a PhD student in the Department of Brain and Cognitive Sciences, was appreciative of the class’s challenges and the ways in which the definition of a rational agent could change depending on the discipline. “Representing what each field means by rationality in a formal framework, makes it clear exactly which assumptions are to be shared, and which were different, across fields.”</p><p>The co-teaching, collaborative structure of the course, as with all Common Ground endeavors, gave students and the instructors opportunities to hear different perspectives in real-time.</p><p>For Paredes Rioboo, this is her third Common Ground course. She says, “I really like the interdisciplinary aspect. They’ve always felt like a nice mix of theoretical and applied from the fact that they need to cut across fields.”</p><p>According to Okoroafor, Kaelbling and Hedden demonstrated an obvious synergy between fields, saying that it felt as if they were engaging and learning along with the class. How computer science and philosophy can be used to inform each other allowed him to understand their commonality and invaluable perspectives on intersecting issues.</p><p>He adds, “philosophy also has a way of surprising you.”</p>",
      "author": "Amanda Diehl | MIT Schwarzman College of Computing",
      "publishedAt": "2026-01-30T21:50:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "c63eb979-b727-45ec-9a0f-ee59182235fa",
      "guid": "https://news.mit.edu/2026/why-its-critical-to-move-beyond-overly-aggregated-machine-learning-metrics-0120",
      "title": "Why it’s critical to move beyond overly aggregated machine-learning metrics",
      "link": "https://news.mit.edu/2026/why-its-critical-to-move-beyond-overly-aggregated-machine-learning-metrics-0120",
      "content": "<p>MIT researchers have identified significant examples of machine-learning model failure when those models are applied to data other than what they were trained on, raising questions about the need to test whenever a model is deployed in a new setting.</p><p>“We demonstrate that even when you train models on large amounts of data, and choose the best average model, in a new setting this ‘best model’ could be the worst model for 6-75 percent of the new data,” says Marzyeh Ghassemi, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS), a member of the Institute for Medical Engineering and Science, and principal investigator at the Laboratory for Information and Decision Systems.</p><p>In a <a href=\"https://arxiv.org/pdf/2510.24884\">paper</a> that was presented at the Neural Information Processing Systems (NeurIPS 2025) conference in December, the researchers point out that models trained to effectively diagnose illness in chest X-rays at one hospital, for example, may be considered effective in a different hospital, on average. The researchers’ performance assessment, however, revealed that some of the best-performing models at the first hospital were the worst-performing on up to 75 percent of patients at the second hospital, even though when all patients are aggregated in the second hospital, high average performance hides this failure.</p><p>Their findings demonstrate that although spurious correlations — a simple example of which is when a machine-learning system, not having “seen” many cows pictured at the beach, classifies a photo of a beach-going cow as an orca simply because of its background — are thought to be mitigated by just improving model performance on observed data, they actually still occur and remain a risk to a model’s trustworthiness in new settings. In many instances — including areas examined by the researchers such as chest X-rays, cancer histopathology images, and hate speech detection — such spurious correlations are much harder to detect.</p><p>In the case of a medical diagnosis model trained on chest X-rays, for example, the model may have learned to correlate a specific and irrelevant marking on one hospital’s X-rays with a certain pathology. At another hospital where the marking is not used, that pathology could be missed.</p><p>Previous research by Ghassemi’s group has shown that models can spuriously correlate such factors as age, gender, and race with medical findings. If, for instance, a model has been trained on more older people’s chest X-rays that have pneumonia and hasn’t “seen” as many X-rays belonging to younger people, it might predict that only older patients have pneumonia.</p><p>“We want models to learn how to look at the anatomical features of the patient and then make a decision based on that,” says Olawale Salaudeen, an MIT postdoc and the lead author of the paper, “but really anything that’s in the data that’s correlated with a decision can be used by the model. And those correlations might not actually be robust with changes in the environment, making the model predictions unreliable sources of decision-making.”</p><p>Spurious correlations contribute to the risks of biased decision-making. In the NeurIPS conference paper, the researchers showed that, for example, chest X-ray models that improved overall diagnosis performance actually performed worse on patients with pleural conditions or enlarged cardiomediastinum, meaning enlargement of the heart or central chest cavity.</p><p>Other authors of the paper included PhD students Haoran Zhang and Kumail Alhamoud, EECS Assistant Professor Sara Beery, and Ghassemi.</p><p>While previous work has generally accepted that models ordered best-to-worst by performance will preserve that order when applied in new settings, called accuracy-on-the-line, the researchers were able to demonstrate examples of when the best-performing models in one setting were the worst-performing in another.</p><p>Salaudeen devised an algorithm called OODSelect to find examples where accuracy-on-the-line was broken. Basically, he trained thousands of models using in-distribution data, meaning the data were from the first setting, and calculated their accuracy. Then he applied the models to the data from the second setting. When those with the highest accuracy on the first-setting data were wrong when applied to a large percentage of examples in the second setting, this identified the problem subsets, or sub-populations. Salaudeen also emphasizes the dangers of aggregate statistics for evaluation, which can obscure more granular and consequential information about model performance.</p><p>In the course of their work, the researchers separated out the “most miscalculated examples” so as not to conflate spurious correlations within a dataset with situations that are simply difficult to classify.</p><p>The NeurIPS paper releases the researchers’ code and some identified subsets for future work.</p><p>Once a hospital, or any organization employing machine learning, identifies subsets on which a model is performing poorly, that information can be used to improve the model for its particular task and setting. The researchers recommend that future work adopt OODSelect in order to highlight targets for evaluation and design approaches to improving performance more consistently.</p><p>“We hope the released code and OODSelect subsets become a steppingstone,” the researchers write, “toward benchmarks and models that confront the adverse effects of spurious correlations.”</p>",
      "author": "Michaela Jarvis | MIT Laboratory for Information and Decision Systems",
      "publishedAt": "2026-01-20T21:30:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "bf10f32d-bd79-421c-b2cf-6d86515ee4b7",
      "guid": "https://news.mit.edu/2026/continued-commitment-to-understanding-intelligence-0114",
      "title": "At MIT, a continued commitment to understanding intelligence",
      "link": "https://news.mit.edu/2026/continued-commitment-to-understanding-intelligence-0114",
      "content": "<p>The MIT Siegel Family Quest for Intelligence (SQI), a research unit in the MIT Schwarzman College of Computing, brings together researchers from across MIT who combine their diverse expertise to understand intelligence through tightly coupled scientific inquiry and rigorous engineering. These researchers engage in collaborative efforts spanning science, engineering, the humanities, and more.&nbsp;</p><p>SQI seeks to comprehend how brains produce intelligence and how it can be replicated in artificial systems to address real-world problems that exceed the capabilities of current artificial intelligence technologies.</p><p>“In SQI, we are studying intelligence scientifically and generically, in the hope that by studying neuroscience and behavior in humans and animals, and also studying what we can build as intelligent engineering artifacts, we'll be able to understand the fundamental underlying principles of intelligence,” says <a href=\"https://sqi.mit.edu/about/people/leslie-kaelbling\">Leslie Pack Kaelbling</a>, SQI director of research and the Panasonic Professor in the MIT Department of Electrical Engineering and Computer Science.</p><p>“We in SQI believe that understanding human intelligence is one of the greatest open questions in science — right up there with the origin of the universe and our place in it, and the origin of life. The question of human intelligence has two parts: how it works, and where it comes from. If we understand those, we will see payoffs well beyond our current imaginings,\" says <a href=\"https://sqi.mit.edu/about/people/james-dicarlo\">Jim DiCarlo</a>, SQI director and the Peter de Florez Professor of Neuroscience in the MIT Department of Brain and Cognitive Sciences.</p><p><strong>Exploring the great mysteries of the mind</strong></p><p>The MIT Siegel Family Quest for Intelligence was recently renamed in recognition of a major gift from the <a href=\"https://www.siegelendowment.org/\">Siegel Family Endowment</a> that is enabling further growth in <a href=\"https://sqi.mit.edu/research/missions\">SQI’s research</a> and activities.</p><p>SQI’s efforts are organized around missions — long-term, collaborative projects rooted in foundational questions about intelligence and supported by platforms — systems, and software that enable new research and create benchmarking and testing interfaces.&nbsp;</p><p>“Ours is the only unit at MIT dedicated to building a scientific understanding of intelligence while working with researchers across the entire Institute,” DiCarlo says. “There has been remarkable progress in AI over the past decade, but I believe the next decade will bring even greater advances in our understanding of human intelligence — advances that will reshape what we call AI. By supporting us, David Siegel, the Siegel Family Endowment, and our other donors are demonstrating their confidence in our approach.\"</p><p><strong>A legacy of interdisciplinary support</strong></p><p>In 2011, David Siegel SM ’86, PhD ’91 founded the Siegel Family Endowment (SFE) to support organizations working at the intersections of learning, workforce, and infrastructure. SFE funds organizations addressing society’s most critical challenges while supporting innovative civic and community leaders, social entrepreneurs, researchers, and others driving this work forward. Siegel is a computer scientist, entrepreneur, and philanthropist. While in graduate school at MIT’s Artificial Intelligence Lab, he worked on robotics in the group of Tomás Lozano-Pérez — currently the School of Engineering Professor of Teaching Excellence — focusing on sensing and grasping. Later, he co-founded Two Sigma with the belief that innovative technology, AI, and data science could help uncover value in the world’s data. Today, Two Sigma drives transformation across the financial services industry in investment management, venture capital, private equity, and real estate.</p><p>Siegel explains, “The human brain may very well be the most complex physical system in the universe, yet most people haven't shown much interest in how it works. People take the mind for granted, yet wonder so much about other scientific mysteries, such as the origin of the universe. My fascination with the brain and its intersection with artificial intelligence stems from this. I don’t care whether there are commercial applications for this quest; instead, we should pursue research like that done at the MIT Siegel Family Quest for Intelligence to advance our understanding of ourselves. As we uncover more about human intelligence, I am hopeful that we will lay the groundwork not only for advancing artificial intelligence but also for extending our own thinking.”</p><p>As a long-time champion of the Center for Brains, Minds, and Machines (CBMM), a National Science Foundation-funded collaborative interdisciplinary research thrust, and one of the first donors to the MIT Quest for Intelligence, David Siegel helped lay the foundation for the research underway today. In early 2024, he founded Open Athena, a nonprofit that bridges the gap between academic research and the cutting edge of AI. Open Athena equips universities with elite AI and data engineering talent to accelerate breakthrough discoveries at scale. Siegel serves on the MIT Corporation Executive Committee, is vice-chair of the Scratch Foundation, and is a member of the Cornell Tech Council. He also sits on the boards of Re:Build Manufacturing, Khan Academy, NYC FIRST, and Carnegie Hall.</p><p><strong>A Catalyst for Global Collaboration</strong></p><p>MIT President Sally Kornbluth says, “Of all the donors and supporters whose generosity fueled the Quest for Intelligence, no one has been more important from the beginning than David Siegel. Without his longstanding commitment to CBMM and his support for the Quest, this community might never have formed. There’s every reason to think that David’s recent gift, which renames the Quest for Intelligence and also supports the Schwarzman College of Computing, will be even more powerful in shaping the future of this initiative and of the field itself.” She continues, “Fueled by generous donors — particularly David Siegel’s transformative gift — SQI is poised to take on an even more important role.”</p><p>SQI scientists and engineers are presenting their work broadly, publishing papers, and developing new tools and technologies that are used in research institutions worldwide, as they engage with colleagues in disciplines across the Institute and in universities and institutions around the globe. DiCarlo explains, “We're part of the Schwarzman College of Computing, at the nexus between the people interested in biology and various forms of intelligence and the people interested in AI. We're working with partners at other universities, in nonprofits, and in industry — we can't do it alone.”</p><p>“Fundamentally, we're not an AI effort. We're a human intelligence effort using the tools of engineering,” DiCarlo says. “That gives us, among other things, very useful insights for human learning and health, but also very useful tools for AI — including AI that will just work a lot better in a human world.”&nbsp;</p><p>The entire SQI community of faculty, students, and staff is excited to face new challenges in the efforts to understand the fundamentals of intelligence.</p><p><strong>New missions and next horizons</strong></p><p>SQI research is broadening: Mission principal investigators are integrating their efforts across areas of interest, increasing their impact on the field. In the coming months, the organization plans to launch a new Social Intelligence Mission.</p><p>\"We need to focus on problems that mirror natural and artificial intelligence — making sure that we are evaluating new models on tasks that mirror what humans and other natural intelligence can do,” says <a href=\"https://sqi.mit.edu/about/people/nicholas-roy\">Nick Roy</a>, SQI director of systems engineering and professor of aeronautics and astronautics at MIT. He predicts that SQI’s future research will rely on asking the right questions: “[While] we are good at picking tasks that test our computational models, and we're extremely good at picking tasks that kind of align with what our models can already do, we need to get better at choosing tasks and benchmarks that also elicit something about natural intelligence,” he says.</p><p>On November 24, 2025, faculty, staff, students, and supporters gathered at an event titled “<a href=\"https://sqi.mit.edu/events/next-horizon-quests-future\">The Next Horizon: Quest’s Future</a>” to celebrate SQI’s next chapter. The event consisted of an afternoon of research updates, a panel discussion, and a poster session on new and evolving research, and was attended by David Siegel, representatives from the Siegel Family Endowment, and various members of the MIT Corporation. Recordings of the presentations from the event are available on SQI’s <a href=\"https://www.youtube.com/playlist?list=PL_uB5rf5tx_aCobk7vuihOmn4PSAIW-dV\">YouTube channel</a>.</p>",
      "author": "Rachel Kemper | MIT Quest for Intelligence",
      "publishedAt": "2026-01-14T21:50:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "b74de576-e11f-4e70-8489-7c878a511f04",
      "guid": "https://news.mit.edu/2026/genai-tool-helps-3d-print-personal-items-sustain-daily-use-0114",
      "title": "Generative AI tool helps 3D print personal items that sustain daily use",
      "link": "https://news.mit.edu/2026/genai-tool-helps-3d-print-personal-items-sustain-daily-use-0114",
      "content": "<p dir=\"ltr\" id=\"docs-internal-guid-e3a8d821-7fff-c750-a6f1-f37cf5dc6658\">Generative artificial intelligence models have left such an indelible impact on digital content creation that it’s getting harder to recall what the internet was like before it. You can call on these AI tools for clever projects such as videos and photos — but their flair for the creative hasn’t quite crossed over into the physical world just yet.<br><br>So why haven’t we seen generative AI-enabled personalized objects, such as phone cases and pots, in places like homes, offices, and stores yet? According to MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers, a key issue is the mechanical integrity of the 3D model.</p><p dir=\"ltr\">While AI can help generate personalized 3D models that you can fabricate, those systems don’t often consider the physical properties of the 3D model. MIT Department of Electrical Engineering and Computer Science (EECS) PhD student and CSAIL engineer Faraz Faruqi has explored this trade-off, creating generative AI-based systems that can make aesthetic changes to designs while&nbsp;<a href=\"https://news.mit.edu/2023/ai-driven-tool-personalize-3d-printable-models-0915\">preserving functionality,</a> and another that modifies structures with the desired&nbsp;<a href=\"https://news.mit.edu/2025/3d-modeling-you-can-feel-0422\">tactile properties</a> users want to feel.<br><br><strong>Making it real</strong>&nbsp;</p><p dir=\"ltr\">Together with researchers at Google, Stability AI, and Northeastern University, Faruqi has now found a way to make real-world objects with AI, creating items that are both durable and exhibit the user’s intended appearance and texture. With the AI-powered “<a href=\"https://dl.acm.org/doi/10.1145/3745778.3766655\" target=\"_blank\">MechStyle</a>” system, users simply upload a 3D model or select a preset asset of things like vases and hooks, and prompt the tool using images or text to create a personalized version. A generative AI model then modifies the 3D geometry, while MechStyle simulates how those changes will impact particular parts, ensuring vulnerable areas remain structurally sound. When you’re happy with this AI-enhanced blueprint, you can 3D print it and use it in the real world.</p><p dir=\"ltr\">You could select a model of, say, a wall hook, and the material you’ll be printing it with (for example, plastics like polylactic acid). Then, you can prompt the system to create a personalized version, with directions like, “generate a cactus-like hook.” The AI model will work in tandem with the simulation module and generate a 3D model resembling a cactus while also having the structural properties of a hook. This green, ridged accessory can then be used to hang up mugs, coats, and backpacks. Such creations are possible thanks, in part, to a stylization process, where the system changes a model’s geometry based on its understanding of the text prompt, and working with the feedback received from the simulation module.</p><p dir=\"ltr\">According to CSAIL researchers, 3D stylization used to come with unintended consequences. Their formative study revealed that only about 26 percent of 3D models remained structurally viable after they were modified, meaning that the AI system didn’t understand the physics of the models it was modifying.</p><p dir=\"ltr\">“We want to use AI to create models that you can actually fabricate and use in the real world,” says Faruqi, who is a lead author on a&nbsp;<a href=\"https://dl.acm.org/doi/10.1145/3745778.3766655\">paper</a> presenting the project. “So MechStyle actually simulates how GenAI-based changes will impact a structure. Our system allows you to personalize the tactile experience for your item, incorporating your personal style into it while ensuring the object can sustain everyday use.”</p><p dir=\"ltr\">This computational thoroughness could eventually help users personalize their belongings, creating a unique pair of glasses with speckled blue and beige dots resembling fish scales, for example. It also produced a pillbox with a rocky texture that’s checkered with pink and aqua spots. The system’s potential extends to crafting unique home and office decor, like a lampshade resembling red magma. It can even design assistive technology fit to users’ specifications, such as finger splints to aid with dexterous injuries and utensil grips to aid with motor impairments.</p><p dir=\"ltr\">In the future, MechStyle could also be useful in creating prototypes for accessories and other handheld products you might sell in a toy shop, hardware store, or craft boutique. The goal, CSAIL researchers say, is for both expert and novice designers to spend more time brainstorming and testing out different 3D designs, instead of assembling and customizing items by hand.</p><p dir=\"ltr\"><strong>Staying strong</strong></p><p dir=\"ltr\">To ensure MechStyle’s creations could withstand daily use, the researchers augmented their generative AI technology with a type of physics simulation called a finite element analysis (FEA). You can imagine a 3D model of an item, such as a pair of glasses, with a sort of heat map indicating which regions are structurally viable under a realistic amount of weight, and which ones aren’t. As AI refines this model, the physics simulations highlight which parts of the model are getting weaker and prevent further changes.<br><br>Faruqi adds that running these simulations every time a change is made drastically slows down the AI process, so MechStyle is designed to know when and where to do additional structural analyses. “MechStyle’s adaptive scheduling strategy keeps track of what changes are happening in specific points in the model. When the genAI system makes tweaks that endanger certain regions of the model, our approach simulates the physics of the design again. MechStyle will make subsequent modifications to make sure the model doesn’t break after fabrication.”</p><p dir=\"ltr\">Combining the FEA process with adaptive scheduling allowed MechStyle to generate objects that were as high as 100 percent structurally viable. Testing out 30 different 3D models with styles resembling things like bricks, stones, and cacti, the team found that the most efficient way to create structurally viable objects was to dynamically identify weak regions and tweak the generative AI process to mitigate its effect. In these scenarios, the researchers found that they could either stop stylization completely when a particular stress threshold was reached, or gradually make smaller refinements to prevent at-risk areas from approaching that mark.</p><p dir=\"ltr\">The system also offers two different modes: a freestyle feature that allows AI to quickly visualize different styles on your 3D model, and a MechStyle one that carefully analyzes the structural impacts of your tweaks. You can explore different ideas, then try the MechStyle mode to see how those artistic flourishes will affect the durability of particular regions of the model.</p><p dir=\"ltr\">CSAIL researchers add that while their model can ensure your model remains structurally sound before being 3D printed, it’s not yet able to improve 3D models that weren’t viable to begin with. If you upload such a file to MechStyle, you’ll receive an error message, but Faruqi and his colleagues intend to improve the durability of those faulty models in the future.</p><p dir=\"ltr\">What’s more, the team hopes to use generative AI to create 3D models for users, instead of stylizing presets and user-uploaded designs. This would make the system even more user-friendly, so that those who are less familiar with 3D models, or can’t find their design online, can simply generate it from scratch. Let’s say you wanted to fabricate a unique type of bowl, and that 3D model wasn’t available in a repository; AI could create it for you instead.<br><br>“While style-transfer for 2D images works incredibly well, not many works have explored how this transfer to 3D,” says Google Research Scientist Fabian Manhardt, who wasn’t involved in the paper. “Essentially, 3D is a much more difficult task, as training data is scarce and changing the object’s geometry can harm its structure, rendering it unusable in the real world. MechStyle helps solve this problem, allowing for 3D stylization without breaking the object’s structural integrity via simulation. This gives people the power to be creative and better express themselves through products that are tailored towards them.”<br><br>Farqui wrote the paper with senior author Stefanie Mueller, who is an MIT associate professor and CSAIL principal investigator, and two other CSAIL colleagues: researcher Leandra Tejedor SM ’24, and postdoc Jiaji Li. Their co-authors are Amira Abdel-Rahman PhD ’25, now an assistant professor at Cornell University, and Martin Nisser SM ’19, PhD ’24; Google researcher Vrushank Phadnis; Stability AI Vice President of Research Varun Jampani; MIT Professor and Center for Bits and Atoms Director Neil Gershenfeld; and Northeastern University Assistant Professor Megan Hofmann.<br><br>Their work was supported by the MIT-Google Program for Computing Innovation. It was presented at the Association for Computing Machinery’s Symposium on Computational Fabrication in November.</p>",
      "author": "Alex Shipps | MIT CSAIL",
      "publishedAt": "2026-01-14T21:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "32ef34b9-11e0-4507-924b-61371d652a08",
      "guid": "https://news.mit.edu/2026/3-questions-how-ai-could-optimize-power-grid-0109",
      "title": "3 Questions: How AI could optimize the power grid",
      "link": "https://news.mit.edu/2026/3-questions-how-ai-could-optimize-power-grid-0109",
      "content": "<p><em>Artificial intelligence has captured headlines recently for its&nbsp;</em><a href=\"https://www.iea.org/reports/energy-and-ai/energy-demand-from-ai\" target=\"_blank\"><em>rapidly growing energy demands</em></a><em>, and particularly the surging&nbsp;</em><a href=\"https://www.pewresearch.org/short-reads/2025/10/24/what-we-know-about-energy-use-at-us-data-centers-amid-the-ai-boom/\" target=\"_blank\"><em>electricity usage of data centers</em></a><em> that enable the training and deployment of the latest generative AI models. But it’s not all bad news — some AI tools have the potential to reduce some forms of energy consumption and enable cleaner grids.</em></p><p><em>One of the most promising applications is using AI to optimize the power grid, which would improve efficiency, increase resilience to extreme weather, and enable the integration of more renewable energy. To learn more,&nbsp;</em>MIT News<em> spoke with&nbsp;</em><a href=\"https://news.mit.edu/2025/fighting-health-planet-ai-priya-donti-1007\" target=\"_blank\"><em>Priya Donti</em></a><em>, the Silverman Family Career Development Professor in the MIT Department of Electrical Engineering and Computer Science (EECS) and a principal investigator at the Laboratory for Information and Decision Systems (LIDS), whose work focuses on applying machine learning to optimize the power grid.</em></p><p><strong>Q:</strong> Why does the power grid need to be optimized in the first place?</p><p><strong>A:</strong> We need to maintain an exact balance between the amount of power that is put into the grid and the amount that comes out at every moment in time. But on the demand side, we have some uncertainty. Power companies don’t ask customers to pre-register the amount of energy they are going to use ahead of time, so some estimation and prediction must be done.</p><p>Then, on the supply side, there is typically some variation in costs and fuel availability that grid managers need to be responsive to. That has become an even bigger issue because of the integration of energy from time-varying renewable sources, like solar and wind, where uncertainty in the weather can have a major impact on how much power is available. Then, at the same time, depending on how power is flowing in the grid, there is some power lost through resistive heat on the power lines. So, as a grid operator, how do you make sure all that is working all the time? That is where optimization comes in.</p><p><strong>Q:&nbsp;</strong>How can AI be most useful in power grid optimization?</p><p><strong>A:&nbsp;</strong>One way AI can be helpful is to use a combination of historical and real-time data to make more precise predictions about how much renewable energy will be available at a certain time. This could lead to a cleaner power grid by allowing us to handle and better utilize these resources.</p><p>AI could also help tackle the complex optimization problems that power grid operators must solve to balance supply and demand in a way that also reduces costs. These optimization problems are used to determine which power generators should produce power, how much they should produce, and when they should produce it, as well as when batteries should be charged and discharged, and whether we can leverage flexibility in power loads. These optimization problems are so computationally expensive that operators use approximations so they can solve them in a feasible amount of time. But these approximations are often wrong, and when we integrate more renewable energy into the grid, they are thrown off even farther. AI can help by providing more accurate approximations in a faster manner, which can be deployed in real-time to help grid operators responsively and proactively manage the grid.</p><p>AI could also be useful in the planning of next-generation power grids. Planning for power grids requires one to use huge simulation models, so AI can play a big role in running those models more efficiently. The technology can also help with predictive maintenance by detecting where anomalous behavior on the grid is likely to happen, reducing inefficiencies that come from outages. More broadly, AI could also be applied to accelerate experimentation aimed at creating better batteries, which would allow the integration of more energy from renewable sources into the grid.</p><p><strong>Q:&nbsp;</strong>How should we think about the pros and cons of AI, from an energy sector perspective?</p><p><strong>A:</strong> One important thing to remember is that AI refers to a heterogeneous set of technologies. There are different types and sizes of models that are used, and different ways that models are used. If you are using a model that is trained on a smaller amount of data with a smaller number of parameters, that is going to consume much less energy than a large, general-purpose model.</p><p>In the context of the energy sector, there are a lot of places where, if you use these application-specific AI models for the applications they are intended for, the cost-benefit tradeoff works out in your favor. In these cases, the applications are enabling benefits from a sustainability perspective — like incorporating more renewables into the grid and supporting decarbonization strategies.</p><p>Overall, it’s important to think about whether the types of investments we are making into AI are actually matched with the benefits we want from AI. On a societal level, I think the answer to that question right now is “no.” There is a lot of development and expansion of a particular subset of AI technologies, and these are not the technologies that will have the biggest benefits across energy and climate applications. I’m not saying these technologies are useless, but they are incredibly resource-intensive, while also not being responsible for the lion’s share of the benefits that could be felt in the energy sector.</p><p>I’m excited to develop AI algorithms that respect the physical constraints of the power grid so that we can credibly deploy them. This is a hard problem to solve. If an LLM says something that is slightly incorrect, as humans, we can usually correct for that in our heads. But if you make the same magnitude of a mistake when you are optimizing a power grid, that can cause a large-scale blackout. We need to build models differently, but this also provides an opportunity to benefit from our knowledge of how the physics of the power grid works.</p><p>And more broadly, I think it’s critical that those of us in the technical community put our efforts toward fostering a more democratized system of AI development and deployment, and that it’s done in a way that is aligned with the needs of on-the-ground applications.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2026-01-09T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "6bd118ed-6635-44e7-9422-62e2c8e688ae",
      "guid": "https://news.mit.edu/2026/decoding-arctic-to-predict-winter-weather-0108",
      "title": "Decoding the Arctic to predict winter weather",
      "link": "https://news.mit.edu/2026/decoding-arctic-to-predict-winter-weather-0108",
      "content": "<p>Every autumn, as the Northern Hemisphere moves toward winter, <a href=\"http://www.judahcohen.org/\" target=\"_blank\">Judah Cohen</a> starts to piece together a complex atmospheric puzzle. Cohen, a research scientist in MIT’s Department of Civil and Environmental Engineering (CEE), has spent decades studying how conditions in the Arctic set the course for winter weather throughout Europe, Asia, and North America. His research dates back to his postdoctoral work with Bacardi and Stockholm Water Foundations Professor Dara Entekhabi that looked at snow cover in the Siberian region and its connection with winter forecasting.</p><p>Cohen’s outlook for the 2025–26 winter highlights a season characterized by indicators emerging from the Arctic using a new generation of artificial intelligence tools that help develop the full atmospheric picture.</p><p><strong>Looking beyond the usual climate drivers</strong></p><p>Winter forecasts rely heavily on El Niño–Southern Oscillation (ENSO) diagnostics, which are the tropical Pacific Ocean and atmosphere conditions that influence weather around the world. However, Cohen notes that ENSO is relatively weak this year.</p><p>“When ENSO is weak, that’s when climate indicators from the Arctic becomes especially important,” Cohen says.</p><p>Cohen monitors high-latitude diagnostics in his subseasonal forecasting, such as October snow cover in Siberia, early-season temperature changes, Arctic sea-ice extent, and the stability of the polar vortex. “These indicators can tell a surprisingly detailed story about the upcoming winter,” he says.&nbsp;</p><p>One of Cohen’s most consistent data predictors is October’s weather in Siberia. This year, when the Northern Hemisphere experienced an unusually warm October, Siberia was colder than normal with an early snow fall. “Cold temperatures paired with early snow cover tend to strengthen the formation of cold air masses that can later spill into Europe and North America,” says Cohen — weather patterns that are historically linked to more frequent cold spells later in winter.</p><p>Warm ocean temperatures in the Barents–Kara Sea and an “easterly” phase of the quasi-biennial oscillation also suggest a potentially weaker polar vortex in early winter. When this disturbance couples with surface conditions in December, it leads to lower-than-normal temperatures across parts of Eurasia and North America earlier in the season.</p><p><strong>AI subseasonal forecasting</strong></p><p>While AI weather models have made impressive strides showcasing in short-range (one-to–10-day) forecasts, these advances have not yet applied to longer periods. The subseasonal prediction covering two to six weeks remains one of the toughest challenges in the field.</p><p>That gap is why this year could be a turning point for subseasonal weather forecasting. A team of researchers working with Cohen won first place for the fall season in the 2025&nbsp;AI WeatherQuest&nbsp;subseasonal forecasting competition, held by the European Centre for Medium-Range Weather Forecasts (ECMWF). The challenge evaluates how well AI models capture temperature patterns over multiple weeks, where forecasting has been historically limited.</p><p>The winning model combined machine-learning pattern recognition with the same Arctic diagnostics Cohen has refined over decades. The system demonstrated significant gains in multi-week forecasting, surpassing leading AI and statistical baselines.</p><p>“If this level of performance holds across multiple seasons, it could represent a real step forward for subseasonal prediction,” Cohen says</p><p>The model also detected a potential cold surge in mid-December for the U.S. East Coast much earlier than usual, weeks before such signals typically arise. The forecast was widely publicized in the media in real-time. If validated, Cohen explains, it would show how combining Arctic indicators with AI could extend the lead time for predicting impactful weather.</p><p>“Flagging a potential extreme event three to four weeks in advance would be a watershed moment,” he adds. “It would give utilities, transportation systems, and public agencies more time to prepare.”</p><p><strong>What this winter may hold</strong></p><p>Cohen’s model shows a greater chance of colder-than-normal conditions across parts of Eurasia and central North America later in the winter, with the strongest anomalies likely mid-season.</p><p>“We’re still early, and patterns can shift,” Cohen says. “But the ingredients for a colder winter pattern are there.”</p><p>As Arctic warming speeds up, its impact on winter behavior is becoming more evident, making it increasingly important to understand these connections for energy planning, transportation, and public safety. Cohen’s work shows that the Arctic holds untapped subseasonal forecasting power, and AI may help unlock it for time frames that have long been challenging for traditional models.</p><p>In November, Cohen even appeared as a clue in <em>The Washington Post</em> <a href=\"https://www.washingtonpost.com/games-static/games-crossword-PDF/sunday/2025/11/09/-1756614807_full_unsolved.pdf\">crossword</a>, a small sign of how widely his research has entered public conversations about winter weather.</p><p>“For me, the Arctic has always been the place to watch,” he says. “Now AI is giving us new ways to interpret its signals.”</p><p>Cohen will continue to update his outlook throughout the season on his <a href=\"https://published.aer.com/aoblog/aoblog.html#PLS\" target=\"_blank\">blog</a>.</p>",
      "author": "Stephanie Martinovich | Department of Civil and Environmental Engineering",
      "publishedAt": "2026-01-08T21:55:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "775331e4-ded3-475e-b417-37fb879722bc",
      "guid": "https://news.mit.edu/2026/stone-center-inequality-shaping-future-work-launches-0107",
      "title": "Stone Center on Inequality and Shaping the Future of Work Launches at MIT",
      "link": "https://news.mit.edu/2026/stone-center-inequality-shaping-future-work-launches-0107",
      "content": "<p>The <a href=\"https://shapingwork.mit.edu/\">James M. and Cathleen D. Stone Center on Inequality and Shaping the Future of Work</a> officially&nbsp;<a href=\"https://www.youtube.com/playlist?list=PL0GFr20m_eBK5Qv1wtGEDtVUy-L_kNbCJ\">launched</a> on Nov. 3, 2025, bringing together scholars, policymakers, and practitioners to explore critical questions about economic opportunity, technology, and democracy.</p><p>Co-directed by MIT professors&nbsp;<a href=\"https://shapingwork.mit.edu/about-us/daron-acemoglu/\">Daron Acemoglu</a>,&nbsp;<a href=\"https://shapingwork.mit.edu/about-us/david-autor/\">David Autor</a>, and&nbsp;<a href=\"https://shapingwork.mit.edu/about-us/simon-johnson/\">Simon Johnson</a>, the new&nbsp;Stone Center analyzes the forces that contribute to growing income and wealth inequality through the erosion of job quality and labor market opportunities for workers without a college degree. The center identifies innovative ways to move the economy onto a more equitable trajectory.</p><p>MIT Provost&nbsp;<a href=\"https://chandrakasan.mit.edu/about/\">Anantha Chandrakasan</a>&nbsp;<a href=\"https://youtu.be/TzNVgzIS6zQ\">opened the launch event</a> by emphasizing the urgency and importance of the center's mission. “As artificial intelligence tools become more powerful, and as they are deployed more broadly,” he said, “we will need to strive to ensure that people from all kinds of backgrounds can find opportunity in the economy.”</p><p>Here are some of the key takeaways from participants in the afternoon’s discussions on&nbsp;<a href=\"https://youtu.be/vMfJ-_Z2j0A\">wealth inequality</a>,&nbsp;<a href=\"https://youtu.be/WEmmIk_2Wdo\">liberalism</a>, and&nbsp;<a href=\"https://youtu.be/14CYkxY37Bs\">pro-worker AI</a>.</p><p><strong>Wealth inequality is driven by private business and public policy</strong></p><p><a href=\"https://zidar.princeton.edu/\">Owen Zidar</a> of Princeton University stressed that owners of businesses like car dealerships, construction firms, and franchises make up a significant portion of the top 1 percent. “For every public company CEO that gets a lot of attention,” he explained, “there are a thousand private business owners who have at least $25 million in wealth.” These business owners have outsized political influence through overrepresentation, lobbying, and donations.</p><p><a href=\"https://atif.scholar.princeton.edu/\">Atif Mian</a> of Princeton University connected high inequality to the U.S. debt crisis, arguing that massive savings at the top aren’t being channeled into productive investment. Instead, falling interest rates push the government to run increasingly large fiscal deficits.</p><p>To mitigate wealth inequality, speakers highlighted policy proposals including rolling back the 20 percent deduction for private business owners and increasing taxes on wealth.</p><p>However, policies must be carefully designed.&nbsp;<a href=\"https://mitsloan.mit.edu/faculty/directory/antoinette-schoar\">Antoinette Schoar</a> of the MIT Sloan School of Management explained how mortgage subsidy policies after the 2008 financial crisis actually worsened inequality by disadvantaging poorer potential homeowners.</p><p><strong>Governments must provide basic public goods and economic security</strong></p><p><a href=\"https://home.watson.brown.edu/people/faculty/faculty-fellows/marc-dunkelman\">Marc Dunkelman</a> of the Watson School of International and Public Affairs at Brown University identified excessive red tape as a key problem for modern liberal democracy. “We can’t build high-speed rail. You can’t build enough housing,” he explained. “That spurs ordinary people who want government to work into the populist camp. We did this to ourselves.”</p><p><a href=\"https://philosophy.berkeley.edu/people/detail/420\">Josh Cohen</a> of Apple University/the University of California at Berkeley emphasized that liberalism must deliver shared prosperity and fair opportunities, not just protect individual freedoms. When people lack economic security, they may turn to leaders who abandon liberal principles altogether.</p><p><strong>Liberal democracy needs to adapt while keeping its core values</strong></p><p><a href=\"https://www.gc.cuny.edu/people/helena-rosenblatt-dhar\">Helena Rosenblatt Dhar</a> of the City University of New York Graduate Center noted that liberalism and democracy have not always been allies. Historically, “civil equality was very important, but not political equality,” she said. “Liberals were very wary of the masses.”</p><p>Speakers emphasized that liberalism’s challenge today is maintaining its commitments to limiting authoritarian power and protecting fundamental freedoms, while addressing its failures.</p><p>Doing so, in Dunkelman’s view, would mean working to “eliminate the sowing [of] the seeds of populism by making government properly balance individual rights and the will of the many.”</p><p><strong>People-centric politics requires regulating social media</strong></p><p>In&nbsp;<a href=\"https://youtu.be/EyiL2TA5vZY\">his keynote</a> at the launch, U.S. Representative&nbsp;<a href=\"https://auchincloss.house.gov/\">Jake Auchincloss</a> (Massachusetts 4th District) connected these notions of government effectiveness and public trust to the influence of technology. He emphasized the need to regulate social media platforms.</p><p>“In my opinion, media is upstream of culture, which is upstream of politics,” he said. “If we want a better culture, and certainly if we want a better politics, we need a better media.”</p><p>Auchincloss proposed that regulation should include holding social media companies liable for content and banning targeted advertising to minors.</p><p>He also echoed the urgency and importance of the center’s research agenda, particularly to understand whether AI will augment or replace labor.</p><p>“My bias has always been: Technology creates more jobs,” he said. “Maybe it’s different this time. Maybe I’m wrong.”</p><p><strong>Augmentation is key to pro-worker AI — but it may require alternative AI architectures</strong></p><p>Stone Center co-director Daron Acemoglu argued that expanding what humans can do, rather than automating their tasks, is essential for achieving pro-worker AI.</p><p>However, Acemoglu cautioned that this won’t happen by itself, noting that the business models of tech companies and their focus on artificial general intelligence are not aligned with a pro-worker vision for AI. This vision may require public investment in alternative AI architectures focused on “domain-specific, reliable knowledge.”</p><p><a href=\"https://mgmt.wharton.upenn.edu/profile/emollick/\">Ethan Mollick</a> of the Wharton School of the University of Pennsylvania noted that AI labs are explicitly trying to “replace people at everything” and are “absolutely convinced that they can do this in the very near term.”</p><p>Meanwhile, companies have “no model for AI adoption,” Mollick explained. “There is absolute confusion.” Even so, “there’s enough money at stake [that] the machine keeps moving forward,” underscoring the urgency of intervention.</p><p>In a glimpse of what such intervention could look like,&nbsp;<a href=\"https://zbucinca.github.io/\">Zana Buçinca</a> of Microsoft shared research findings that accounting for workers’ values and cognition in AI design can enable better complementarity.</p><p>“The impact of AI on human work is not destiny,” she emphasized. “It’s design.”</p>",
      "author": "Julia Regier | Blueprint Labs",
      "publishedAt": "2026-01-07T20:30:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "f254fc7e-319f-42b9-b796-a611f17c41d5",
      "guid": "https://news.mit.edu/2026/mit-scientists-investigate-memorization-risk-clinical-ai-0105",
      "title": "MIT scientists investigate memorization risk in the age of clinical AI",
      "link": "https://news.mit.edu/2026/mit-scientists-investigate-memorization-risk-clinical-ai-0105",
      "content": "<p dir=\"ltr\">What is patient privacy for? The Hippocratic Oath, thought to be one of the earliest and most widely known medical ethics texts in the world, reads: “Whatever I see or hear in the lives of my patients, whether in connection with my professional practice or not, which ought not to be spoken of outside, I will keep secret, as considering all such things to be private.”&nbsp;</p><p dir=\"ltr\">As privacy becomes increasingly scarce in the age of data-hungry algorithms and cyberattacks, medicine is one of the few remaining domains where confidentiality remains central to practice, enabling patients to trust their physicians with sensitive information.</p><p dir=\"ltr\">But&nbsp;<a href=\"https://neurips.cc/virtual/2025/loc/san-diego/poster/118370\">a paper</a> co-authored by MIT researchers investigates how artificial intelligence models trained on de-identified electronic health records (EHRs) can memorize patient-specific information. The work, which was recently presented at the 2025 Conference on Neural Information Processing Systems (NeurIPS), recommends a rigorous testing setup to ensure targeted prompts cannot reveal information, emphasizing that leakage must be evaluated in a health care context to determine whether it meaningfully compromises patient privacy.</p><p dir=\"ltr\">Foundation models trained on EHRs should normally generalize knowledge to make better predictions, drawing upon many patient records. But in “memorization,” the model draws upon a singular patient record to deliver its output, potentially violating patient privacy. Notably, foundation models are already known to be&nbsp;<a href=\"https://icml.cc/virtual/2025/workshop/39996\">prone to data leakage</a>.</p><p dir=\"ltr\">“Knowledge in these high-capacity models can be a resource for many communities, but adversarial attackers can prompt a model to extract information on training data,” says Sana Tonekaboni, a postdoc at the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard and first author of the paper. Given the risk that foundation models could also memorize private data, she notes, “this work is a step towards ensuring there are practical evaluation steps our community can take before releasing models.”</p><p dir=\"ltr\">To conduct research on the potential risk EHR foundation models could pose in medicine, Tonekaboni approached MIT Associate Professor&nbsp;<a href=\"https://jclinic.mit.edu/team-member/marzyeh-ghassemi/\">Marzyeh Ghassemi</a>, who is a principal investigator at the <a href=\"https://jclinic.mit.edu/\">Abdul Latif Jameel Clinic for Machine Learning in Health</a> (Jameel Clinic) and a member of the Computer Science and Artificial Intelligence Lab. Ghassemi, a faculty member in the MIT Department of Electrical Engineering and Computer Science and Institute for Medical Engineering and Science, runs the&nbsp;<a href=\"https://healthyml.org/\">Healthy ML group</a>, which focuses on robust machine learning in health.</p><p dir=\"ltr\">Just how much information does a bad actor need to expose sensitive data, and what are the risks associated with the leaked information? To assess this, the research team developed a series of tests that they hope will lay the groundwork for future privacy evaluations. These tests are designed to measure various types of uncertainty, and assess their practical risk to patients by measuring various tiers of attack possibility.&nbsp;&nbsp;</p><p dir=\"ltr\">“We really tried to emphasize practicality here; if an attacker has to know the date and value of a dozen laboratory tests from your record in order to extract information, there is very little risk of harm. If I already have access to that level of protected source data, why would I need to attack a large foundation model for more?” says Ghassemi.&nbsp;</p><p dir=\"ltr\">With the inevitable digitization of medical records, data breaches have become more commonplace. In the past 24 months, the&nbsp;U.S. Department of Health and Human Services has recorded <a href=\"https://ocrportal.hhs.gov/ocr/breach/breach_report.jsf\">747 data breaches</a> of health information affecting more than 500 individuals, with the majority categorized as hacking/IT incidents.</p><p dir=\"ltr\">Patients with unique conditions are especially vulnerable, given how easy it is to pick them out. “Even with de-identified data, it depends on what sort of information you leak about the individual,” Tonekaboni says. “Once you identify them, you know a lot more.”</p><p dir=\"ltr\">In their structured tests, the researchers found that the more information the attacker has about a particular patient, the more likely the model is to leak information. They demonstrated how to distinguish model generalization cases from patient-level memorization, to properly assess privacy risk.&nbsp;</p><p dir=\"ltr\">The paper also emphasized that some leaks are more harmful than others. For instance, a model revealing a patient’s age or demographics could be characterized as a more benign leakage than the model revealing more sensitive information, like an HIV diagnosis or alcohol abuse.&nbsp;</p><p dir=\"ltr\">The researchers note that patients with unique conditions are especially vulnerable given how easy it is to pick them out, which may require higher levels of protection. “Even with de-identified data, it really depends on what sort of information you leak about the individual,” Tonekaboni says. The researchers plan to expand the work to become more interdisciplinary, adding clinicians and privacy experts as well as legal experts.&nbsp;</p><p dir=\"ltr\">“There’s a reason our health data is private,” Tonekaboni says. “There’s no reason for others to know about it.”</p><p dir=\"ltr\">This work supported by the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, Wallenberg AI, the Knut and Alice Wallenberg Foundation, the U.S. National Science Foundation (NSF), a Gordon and Betty Moore Foundation award, a Google Research Scholar award, and the AI2050 Program at Schmidt Sciences. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.</p>",
      "author": "Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health",
      "publishedAt": "2026-01-05T21:55:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "c99a444b-921c-421f-934f-a920312ad6fe",
      "guid": "https://news.mit.edu/2026/using-design-interpret-past-envision-future-c-jacob-payne-0105",
      "title": "Using design to interpret the past and envision the future ",
      "link": "https://news.mit.edu/2026/using-design-interpret-past-envision-future-c-jacob-payne-0105",
      "content": "<p>Some of designer C Jacob Payne’s projects present new, futuristic products — such as zero-gravity footwear for astronauts, and electronic-embedded ceramics — using technological tools and processes of digital fabrication, material innovation, and interactive interfaces. Other projects travel back in time to past centuries, considering the challenge of preserving and reconstructing Black architectural heritage.</p><p>Payne graduated from Yale University with a bachelor’s degree in architecture and environmental studies, and then worked briefly at architecture firms in New York and Los Angeles. He decided to pursue a professional degree in order to become a licensed architect and to try out different types of design. He began the&nbsp;<a href=\"https://oge.mit.edu/gpp/advanced-degrees/masters-degree/master-of-architecture/\">MIT Master of Architecture</a> (MArch) program in 2023, and is aiming to graduate in January 2027.</p><p>“I have especially valued the academic freedom to make my own path,” says Payne. “Although the MArch program requires certain classes each semester, I’ve been able to find a way to tailor the degree in a way that really reflects my interests.”</p><p>Payne says he appreciates how his experiences in the program have allowed him to work on design projects at a variety of scales — from the smaller scale in industrial and product design classes, to the larger scale in classes in the&nbsp;<a href=\"https://dusp.mit.edu/\">Department of Urban Studies and Planning</a>. He is a collaborator at the&nbsp;<a href=\"https://designintelligence.mit.edu/\">Design Intelligence Lab</a> and has served as a teaching assistant in MIT’s architecture wood shop, helping students to bring together digital design techniques with hands-on fabrication. Payne says he values the off-campus opportunities he has had, including working at a furniture and product design company in Barcelona through&nbsp;<a href=\"https://misti.mit.edu/\">MISTI</a> and spending a summer working at the experience design firm 2x4 in New York.</p><p><strong>Rediscovering the architecture of the past</strong></p><p>Through his graduate classes, Payne became especially interested in research into different types of vernacular architecture in America, especially in the American South. During his second semester, he took the class 4.182 (Brick x Brick: Drawing a Particular Survey), taught by Assistant Professor Carrie Norman, director of the architecture department’s undergraduate major and minor programs. As part of the curriculum, the class traveled to Tuskegee University to research the history and works of Robert R. Taylor, the first Black graduate of MIT (in 1892) and also the first licensed Black architect in America.</p><p>Following the class, Payne continued working on models and drawings reconstructing some important Tuskegee architecture. He created&nbsp;<a href=\"https://www.cjacobpayne.com/work/tuskegee-chapels\">models</a> of Taylor’s original 1896 Tuskegee University Chapel, lost to fire in 1957, and the subsequent chapel built in its place in 1969, designed by Paul Rudolph in collaboration with Tuskegee University. He also produced a set of speculative drawings reconstructing Taylor’s 1896 chapel, using the very sparse remaining archival materials (including a few photographs and one drawing), the standards of the Historic American Buildings Survey, and inferred details.</p><p>“A lot of the work was figuring out how we can better understand and reconstruct historic spaces with very limited information,” says Payne. “I think it’s important to not treat the past as something static or fixed — because there’s so much that we don’t know, that has been unexplored.”</p><p>Payne received the 2025-26&nbsp;L. Dennis Shapiro (1955) Graduate Fellowship in the History of African American Experience of Technology. He is currently looking into different typologies of architecture that were in the American South, with a particular focus on “juke joints,” structures that came about during the Jim Crow era. These were intended as secret social spaces for Black people to congregate, dance, sing, and play blues music — at a time when they were often barred from many establishments. Since there is very little documentation still remaining to use in this research, Payne says, the challenge is identifying which current techniques of architecture and design can be used to better understand and visualize these spaces.</p><p><a name=\"_Hlk211959657\"></a>“As his advisor, I have watched Jacob develop a body of work that treats architectural representation as both record and repair, recovering lost and overlooked Black-built traditions as vital expressions of Black spatial agency,” says Norman. “Through drawings, models, and speculative reconstructions, he expands the tools of the discipline to engage histories of cultural identity and heritage.\"</p><p><strong>Incorporating AI to design for the future</strong></p><p>While much of Payne’s research is rooted in the past, he is also interested in artificial intelligence and its implications for future innovations. Last spring, he took the class 4.154 (Space Architecture) and learned how to design for the particular challenges of working in space. Along with his team,&nbsp;he designed a <a href=\"https://www.cjacobpayne.com/work/air-0-space-shoe\">footwear system for astronauts</a> that could anchor to spacecraft structures with a mechanical, rotating sole, and inflatable bladders around the ankle for support.</p><p>In addition, Payne took a class about large language objects taught by associate professor of the practice Marcelo Coelho, director of the Design Intelligence Lab. “Designing products that integrate large language models involves thinking about how people can interact with AI in the physical world,” says Payne. “We are able create new experiences that challenge the ways that people think about how AI will look in the future.”</p><p>For the class, Payne and his team worked on a project using AI in the kitchen, developing a countertop device called the&nbsp;<a href=\"https://www.cjacobpayne.com/work/kitchen-cosmo\">Kitchen Cosmo</a>. A camera at the top scans the ingredients placed in front of it. The user can input information such as how many people will be eating the meal and how much time is available to prepare the meal, and the device prints out a recipe.</p><p>Payne also worked on a project with Coelho for the Venice Biennale: a lamp that used geopolymers — a more sustainable alternative to concrete or other castable materials. Because this ceramic material doesn’t need to be fired in a kiln to harden, it can have electronics embedded within it. Payne now continues to work on AI research and product design in the Design Intelligence Lab.</p><p>“Jacob is an exceptional designer who deeply embodies MIT’s ‘mens et manus’ [‘mind and hand’] ethos by approaching product and interaction design with an exciting combination of intellectual rigor and high-quality, hands-on making,” says Coelho. “He is equally comfortable thinking conceptually about the cultural implications of artificial intelligence and working on the technical and craft detailing needed to bring his ideas to life.”</p>",
      "author": "Stefanie Koperniak | Office of Graduate Education",
      "publishedAt": "2026-01-05T20:25:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "66a51b45-96de-4515-8f75-9a2a736b2807",
      "guid": "https://news.mit.edu/2025/mit-media-review-1222",
      "title": "MIT in the media: 2025 in review ",
      "link": "https://news.mit.edu/2025/mit-media-review-1222",
      "content": "<p><em>“At MIT, innovation ranges from awe-inspiring technology to down-to-Earth creativity,” noted&nbsp;</em>Chronicle<em>, during a campus visit this year for an episode of the program. In 2025, MIT researchers made headlines across print publications, podcasts, and video platforms for key scientific advances, from breakthroughs in quantum and artificial intelligence to new efforts aimed at improving pediatric health care and cancer diagnosis.</em></p><p><em>MIT faculty, researchers, students, alumni and staff helped demystify new technologies, highlighted the practical hands-on learning the Institute is known for, and shared what inspires their research with viewers, readers and listeners around the world. Below is a sampling of news moments to revisit.</em></p><p><strong>Let’s take a closer look at MIT: It’s alarming to see such a complex, important institution subject to the whims of today’s politics</strong><br><em>Washington Post&nbsp;</em>columnist George F. Will reflects on MIT and his view of “the damage that can be done to America’s meritocracy by policies motivated by hostility toward institutions vital to it.”&nbsp;Will notes that MIT has an “astonishing economic multiplier effect: MIT graduates have founded companies that have generated almost&nbsp;$1.9 trillion&nbsp;in annual revenue (a sum almost equal to Russia’s GDP) and 4.6 million jobs.”<br><a href=\"https://www.washingtonpost.com/opinions/2025/12/12/mit-trump-education-universities/\" target=\"_blank\">Full story via The Washington Post</a></p><p><strong>At MIT, groundbreaking ideas blend science and breast cancer detection innovation</strong><br><em>Chronicle</em>&nbsp;visited MIT this spring to learn more about how the Institute “nurtures groundbreaking efforts, reminding us that creativity and science thrive together, inspiring future advancements in engineering, medicine, and beyond.”<br><a href=\"https://www.wcvb.com/article/at-mit-groundbreaking-ideas-blend-science-and-breast-cancer-detection-innovation/64368819\" target=\"_blank\">Full story via Chronicle</a></p><p><strong>New MIT provost looks to build more bridges with CEOs</strong><br>Provost Anantha Chandrakasan shares his energy and enthusiasm for MIT, and his goals for the Institute.<br><a href=\"https://www.bostonglobe.com/2025/07/07/business/mit-provost-anantha-chandrakasan/\" target=\"_blank\">Full story via The Boston Globe</a></p><p><strong>Five things New England researchers helped develop with federal funding</strong><br>Professors John Guttag and David Mindell discuss MIT’s long history of developing foundational technologies — including the internet and the first widely used electronic navigation system — with the support of federal funding.<br><a href=\"https://www.bostonglobe.com/2025/06/10/metro/federal-funding-spurs-new-england-inventions/\" target=\"_blank\">Full story via The Boston Globe</a></p><p><strong>Bostonians of the Year 2025: First responders, university presidents, and others who exemplified courage</strong><br>President Sally Kornbluth is honored by&nbsp;<em>The Boston Globe</em>&nbsp;as one of the Bostonians of the Year, a list that spotlights individuals across the region who, in choosing the difficult path, “showed us what strength looks like.” Kornbluth was recognized for her work being of the “most prominent voices rallying to protect academic freedom.”<br><a href=\"https://www.bostonglobe.com/2025/12/09/magazine/bostonians-year-2025/\" target=\"_blank\">Full story via The Boston Globe</a></p><p><strong><u>Practical education and workforce preparation</u></strong></p><p><strong>College students flock to a new major: AI</strong><br>MIT’s new Artificial Intelligence and Decision Making major is aimed at teaching students to “develop AI systems and study how technologies like robots interact with humans and the environment.”<br><a href=\"https://www.nytimes.com/2025/12/01/technology/college-computer-science-ai-boom.html\" target=\"_blank\">Full story via New York Times</a></p><p><strong>50 colleges with the best ROI</strong><br>MIT has been named among the top colleges in the country for return on investment<em>.&nbsp;</em>MIT “is need-blind and full-need for undergraduate students. Six out of 10 students receive financial aid, and almost 88% of the Class of 2025 graduated debt-free.”<br><a href=\"https://www.boston25news.com/news/50-colleges-with-best-roi/J3HE4RZVWBN2RPKTSNWE4TCCL4/\" target=\"_blank\">Full story via Boston 25</a></p><p><strong>Desirée Plata: Chemist, oceanographer, engineer, entrepreneur</strong><br>Professor Desirée Plata explains that she is most proud of her work as an educator. “The faculty of the world are training the next generation of researchers,” says Plata. “We need a trained workforce. We need patient chemists who want to solve important problems.”<br><a href=\"https://cen.acs.org/people/profiles/Desire-Plata-Chemist-oceanographer-engineer/103/i4\" target=\"_blank\">Full story via Chemical &amp; Engineering News</a></p><p><strong><u>Taking a quantum leap</u></strong></p><p><strong>MIT launches quantum initiative to tackle challenges in science, health care, national security</strong><br>MIT is “taking a quantum leap with the launch of the new MIT Quantum Initiative (QMIT). “There isn't a more important technological field right now than quantum with its enormous potential for impact on both fundamental research and practical problems,” said President Sally Kornbluth.<br><a href=\"https://www.bizjournals.com/boston/news/2025/12/08/healey-backs-mits-quantum-push.html\" target=\"_blank\">Full story via State House News Service</a></p><p><strong>Peter Shor on how quantum tech can help climate</strong><br>Professor Peter Shor helps disentangle quantum technologies.<br><a href=\"https://www.youtube.com/watch?v=Aj9Xi_gtA0I\" target=\"_blank\">Full story via The Quantum Kid</a></p><p><strong>MIT researchers develop device to enable direct communication between multiple quantum processors</strong><br>MIT researchers made a key advance in the creation of a practical quantum computer.<br><a href=\"https://www.militaryaerospace.com/computers/article/55279991/mit-researchers-develop-device-to-enable-direct-communication-between-multiple-quantum-processors\" target=\"_blank\">Full story via Military &amp; Aerospace Electronics</a></p><p><strong><u>Fortifying national security and aiding disaster response</u></strong></p><p><strong><u><o:p></o:p></u></strong></p><p><strong>Nano-material breakthrough could revolutionize night vision</strong><br>MIT researchers developed “a new way to make large ultrathin infrared sensors that don’t need cryogenic cooling and could radically change night vision for the military.”<br><a href=\"https://www.defenseone.com/technology/2025/04/nano-material-breakthrough-could-revolutionize-night-vision/404758/\" target=\"_blank\">Full story via Defense One</a></p><p><strong>MIT researchers develop robot designed to help first-responders in disaster situations</strong><br>Researchers at MIT engineered SPROUT (Soft Pathfinding Robotic Observation Unit),&nbsp;a robot aimed at assisting first-responders.<br><a href=\"https://whdh.com/news/mit-researchers-develop-robot-designed-to-help-first-responders-in-disaster-situations/\" target=\"_blank\">Full story via WHDH</a></p><p><strong>MIT scientists make “smart” clothes that warn you when you’re sick</strong><br>As part of an effort to help keep service members safe, MIT scientists created a programmable fiber that can be stitched into clothing to help monitor the wearer’s health.<br><a href=\"https://www.fox28spokane.com/\" target=\"_blank\">Full story via FOX 28</a></p><p><strong>MIT Lincoln Lab develops ocean-mapping technology</strong><br>MIT Lincoln Laboratory researchers are developing “automated electric vessels to map the ocean floor and improve search and rescue missions.”<br><a href=\"https://www.wcvb.com/article/mit-lincoln-laboratory-develops-ocean-mapping-technology-in-boston-harbor/68284552\" target=\"_blank\">Full story via Chronicle</a></p><p><strong><u>Transformative tech</u></strong></p><p><strong>This MIT scientist is rewiring robots to keep the humanity in tech</strong><br>Professor Daniela Rus, director of the Computer Science and Artificial Intelligence Lab, discusses her work revolutionizing the field of robotics by bringing “empathy into engineering and proving that responsibility is as radical and as commercially attractive as unguarded innovation.”<br><a href=\"https://www.forbes.com/sites/gemmaallen/2025/10/31/the-humanity-hack-how-one-mit-scientist-is-rewiring-robots-to-beat-big-tech/\" target=\"_blank\">Full story via Forbes</a></p><p><strong>Watch this tiny robot somersault through the air like an insect</strong><br>Professor Kevin Chen designed a tiny, insect-sized aerial microrobot.<br><a href=\"https://www.science.org/content/article/watch-tiny-robot-somersault-through-air-insect\" target=\"_blank\">Full story via Science</a></p><p><strong>It's actually really hard to make a robot, guys</strong><br>Professor Pulkit Agrawal delves into<em>&nbsp;</em>his work engineering a simulator that can be used to train robots.<br><a href=\"https://www.npr.org/transcripts/1250811320\" target=\"_blank\">Full story via NPR</a></p><p><strong>Shape-shifting fabrics and programmable materials redefine design at MIT</strong><br>Associate Professor Skylar Tibbits is embedding intelligence into the materials around us, while Professor Caitlin Mueller and Sandy Curth PhD ’25 are digging into eco-friendly construction.<br><a href=\"https://www.wcvb.com/article/shape-shifting-fabrics-and-programmable-materials-redefine-design-at-mit/64368581\" target=\"_blank\">Full story via Chronicle</a></p><p><strong><u>Building a healthier future</u></strong></p><p><strong><u><o:p></o:p></u></strong></p><p><strong>MIT launches pediatric research hub to address access gaps</strong><br>The Hood Pediatric Innovation Hub is addressing “underinvestment in pediatric healthcare innovations.”<br><a href=\"https://www.bizjournals.com/boston/news/2025/04/18/mit-pediatric-research-hub-launch.html\" target=\"_blank\">Full story via Boston Business Journal</a></p><p><strong>Bionic knee helps amputees walk naturally again</strong><br>Professor Hugh Herr developed a prosthetic that could increase mobility for above-the-knee amputees. “The bionic knee developed by MIT doesn’t just restore function, it redefines it.”<br><a href=\"https://www.foxnews.com/tech/bionic-knee-helps-amputees-walk-naturally-again\" target=\"_blank\">Full story via Fox News</a></p><p><strong>MIT drug hunters are using AI to design completely new antibiotics</strong><br>Professor James Collins is using AI to develop new compounds to combat antibiotic resistance.<br><a href=\"https://www.fastcompany.com/91451161/mit-drug-hunters-are-using-ai-to-design-completely-new-antibiotics\" target=\"_blank\">Full story via Fast Company</a></p><p><strong>Innovative once-weekly capsule helps quell schizophrenia symptoms</strong><br>A new pill from the lab of Associate Professor Giovanni Traverso “can greatly simplify the drug schedule faced by schizophrenia patients.”<br><a href=\"https://www.newsmax.com/health/health-news/schizophrenia-pill-medicine/2025/06/16/id/1215118/\" target=\"_blank\">Full story via Newsmax</a></p><p><strong><u>Renewing American manufacturing</u></strong></p><p><strong>US manufacturing is in “pretty bad shape.” MIT hopes to change that.</strong><br>MIT launched the Initiative for New Manufacturing to help “build the tools and talent to shape a more productive and sustainable future for manufacturing.”<br><a href=\"https://www.manufacturingdive.com/news/us-manufacturing-pretty-bad-shape-mit-hopes-to-change-that-GE-Vernova-Siemens/751904/\" target=\"_blank\">Full story via Manufacturing Dive</a></p><p><strong>Giving US manufacturing a boost</strong><br>Ben Armstrong of the MIT Industrial Performance Center discusses how to reinvigorate manufacturing in America.<br><a href=\"https://www.marketplace.org/story/2025/04/02/manufacturing-sector-trump-tariffs-workforce-skilled-labor-factories\" target=\"_blank\">Full story via Marketplace</a></p><p><strong>New England companies are sparking an industrial revolution. Here’s how to harness it.</strong><br>Professor David Mindell spotlights how “a new wave of industrial companies, many in New England, are leveraging new technologies to create jobs and empower workers.”<br><a href=\"https://www.bostonglobe.com/2025/05/29/magazine/next-industrial-revolution-new-england-companies/\" target=\"_blank\">Full story via The Boston Globe</a>&nbsp;</p><p><u><o:p></o:p></u></p><p><strong><u>Improving aging</u></strong></p><p><strong>My day as an 80-year-old. What an age-simulation suit taught me.</strong><br>To get a better sense of the experience of aging, <em>Wall Street Journal&nbsp;</em>reporter Amy Dockser Marcus donned the MIT AgeLab’s age-simulation suit and embarked on multiple activities.<br><a href=\"https://www.wsj.com/health/wellness/mit-age-simulation-suit-678afa33\" target=\"_blank\">Full story via The Wall Street Journal</a></p><p><strong>New mobile robot helps seniors walk safely and prevent falls</strong><br>A mobile robot created by MIT engineers is designed to help prevent falls. “It's easy to see how something like this could make a big difference for seniors wanting to stay independent.”<br><a href=\"https://www.foxnews.com/tech/new-mobile-robot-helps-seniors-walk-safely-prevent-falls\" target=\"_blank\">Full story via Fox News</a></p><p><strong>The senior population is booming. Caregiving is struggling to keep up</strong><br>Professor Jonathan Gruber discusses the labor shortages impacting senior care.<br><a href=\"https://www.cnbc.com/2025/11/21/senior-caregiving-labor.html\" target=\"_blank\">Full story via CNBC</a></p><p><strong><u>Upping our energy resilience</u></strong></p><p><strong>New MIT collaboration with GE Vernova aims to accelerate energy transition</strong><br>“A great amount of innovation happens in academia. We have a longer view into the future,” says Provost Anantha Chandrakasan of the MIT-GE Vernova Energy and Climate Alliance.<br><a href=\"https://www.bostonglobe.com/2025/04/04/business/ge-vernova-mit-energy/\" target=\"_blank\">Full story via The Boston Globe</a></p><p><strong>The environmental impacts of generative AI</strong><br>Noman Bashir, a fellow with MIT’s Climate and Sustainability Consortium, explores the environmental impacts of generative AI.<br><a href=\"https://www.fox13seattle.com/video/1664374\" target=\"_blank\">Full story via Fox 13</a></p><p><strong>Is the clean energy economy doomed?</strong><br>Professor Christopher Knittel discusses how the U.S. can be in the best position for global energy dominance.<br><a href=\"https://www.marketplace.org/episode/2025/05/06/is-the-clean-energy-economy-doomed\" target=\"_blank\">Full story via Marketplace</a></p><p><strong><u>Advancing American workers</u></strong></p><p><strong>WTH can we do to prevent a second China shock? Professor David Autor explains</strong><br>Professor David Autor shares his research examining the long-term impact of China entering the World Trade Organization, how the U.S. can protect vital industries from unfair trade practices, and the potential impacts of AI on workers.<br><a href=\"https://www.aei.org/podcast/wth-can-we-do-to-prevent-a-second-china-shock-professor-david-autor-explains/\" target=\"_blank\">Full story via American Enterprise Institute</a></p><p><strong>The fight over robots threatening American jobs</strong><br>Professor Daron Acemoglu highlights the economic and societal implications of integrating automation in the workforce, advocating for policies aimed at assisting workers.<br><a href=\"https://www.ft.com/content/eb11f69c-e45c-4f23-8793-0ca5866b4b67\" target=\"_blank\">Full story via Financial Times</a></p><p><strong>Moving toward automation</strong><br>Research Scientist Eva Ponce of the MIT Center for Transportation and Logistics notes that robotics and AI technologies are “replacing some jobs — particularly more manual tasks including heavy lifting — but have also offered new opportunities within warehouse operations.”<br><a href=\"https://www.ft.com/content/31ec6a78-97cf-47a2-b229-d63c44b81073\" target=\"_blank\">Full story via Financial Times</a></p><p><strong><u>Planetary defense and out-of-this world exploration</u></strong></p><p><strong>MIT researchers create new asteroid detection methods to help protect Earth</strong><br>Associate Professor Julien de Wit and Research Scientist Artem Burdanov discuss their work developing a new method to track asteroids that could impact Earth.<br><a href=\"https://wbznewsradio.iheart.com/content/mit-researchers-discover-asteroids-that-could-pose-threat-to-earth/\" target=\"_blank\">Full story via WBZ Radio</a></p><p><strong>What happens to the bodies of NASA astronauts returning to Earth?</strong><br>Professor Dava Newman speaks about how long-duration stays in space can affect the human body.<br><a href=\"https://www.newsnationnow.com/the-hill/what-happens-to-the-bodies-of-nasa-astronauts-returning-to-earth/\" target=\"_blank\">Full story via News Nation</a></p><p><strong>Lunar lander Athena is packed and ready to explore the moon. Here’s what on board</strong><br>MIT engineers sent three payloads into space on a course set for the moon’s south polar region.<br><a href=\"https://www.usatoday.com/story/news/nation/2025/02/25/lunar-lander-mission-athena-intuitive-machines-nasa/79999435007/\" target=\"_blank\">Full story via USA Today</a></p><p><strong>Scanning the heavens at the Vatican Observatory</strong><br>Br. Guy Consolmagno '74, SM '75, director of the Vatican Observatory, and graduate student Isabella Macias share their experiences studying astronomy and planetary formation at the Vatican Observatory. “The Vatican has such a deep, rich history of working with astronomers,” says Macias. “It shows that science is not only for global superpowers around the world, but it's for students, it's for humanity.”<br><a href=\"https://www.cbsnews.com/video/scanning-the-heavens-at-the-vatican-observatory/\" target=\"_blank\">Full story via CBS News Sunday Morning</a></p><p><strong>The story of real-life rocket scientists</strong><br>Professor Kerri Cahoy takes viewers on an out-of-this-world journey into how a college internship inspired her research on space and satellites.<br><a href=\"https://www.youtube.com/watch?v=jvftn7z_ELo\" target=\"_blank\">Full story via Bloomberg Television</a>&nbsp;</p><p><u><o:p></o:p></u></p><p><strong><u>On the air&nbsp;</u></strong></p><p><strong><u><o:p></o:p></u></strong></p><p><strong>While digital currency initiatives expand, we ask: What’s the future of cash?</strong><br>Neha Narula, director of the MIT Digital Currency Initiative, examines the future of cash as the use of digital currencies expands.<br><a href=\"https://www.usatoday.com/story/money/2025/05/16/digital-currency-initiatives-future-cash-the-excerpt/83675663007/?gnt-cfr=1&amp;gca-cat=p\" target=\"_blank\">Full story via USA Today</a></p><p><strong>The high stakes of the AI economy</strong><br>Professor Asu Ozdaglar, head of the Department of Electrical Engineering and Computer Science and deputy dean of the MIT Schwarzman College of Computing, explores AI’s opportunities and risks — and whether it can be regulated without stifling progress.<br><a href=\"https://podcasts.apple.com/us/podcast/the-high-stakes-of-the-ai-economy-live-from-the-wbur-festival/id1741997589?i=1000715576073\" target=\"_blank\">Full story via Is Business Broken?&nbsp;</a></p><p><strong>The LIGO Lab is pushing the boundaries of gravitational-wave research</strong><br>Associate Professor Matt Evans explores the future of gravitational wave research and how Cosmic Explorer, the next-generation gravitational wave observatory, will help unearth secrets of the early universe.<br><a href=\"https://www.scientificamerican.com/podcast/episode/cosmic-explorer-laser-breakthroughs-and-the-next-generation-of-gravitational/\" target=\"_blank\">Full story via Scientific American</a></p><p><strong>Space junk: The impact of global warming on satellites</strong><br>Graduate student Will Parker discusses his research examining the impact of climate change on satellites.<br><a href=\"https://www.usatoday.com/story/tech/science/2025/04/04/space-junk-satellites-the-excerpt/82872514007/?gnt-cfr=1&amp;gca-cat=p\" target=\"_blank\">Full story via USA Today</a></p><p><strong>Endometriosis is common. Why is getting diagnosed so hard?</strong><br>Professor Linda Griffith shares her work studying endometriosis and her efforts to improve healthcare for women.<br><a href=\"https://www.sciencefriday.com/segments/endometriosis-diagnosis-science/\" target=\"_blank\">Full story via Science Friday</a></p><p><strong>There’s nothing small about this nanoscale research</strong><br>Professor Vladimir Bulović takes listeners on a tour of MIT.nano, MIT’s “clean laboratory facility that is critical to nanoscale research, from microelectronics to medical nanotechnology.”<br><a href=\"https://www.scientificamerican.com/podcast/episode/studying-science-medicine-and-engineering-at-a-nanoscale-at-an-m-i-t-clean/\" target=\"_blank\">Full story via Scientific American</a></p><p><strong><u>Marrying science and athletics</u></strong></p><p><strong>The MIT scientist behind the “torpedo bats” that are blowing up baseball</strong><br>Aaron Leanhardt PhD ’03 went from an MIT graduate student who was part of a research team that “cooled sodium gas to the lowest temperature ever recorded in human history” to inventor of the torpedo baseball bat, “perhaps the most significant development in bat technology in decades.”<br><a href=\"https://www.wsj.com/sports/baseball/mit-scientist-torpedo-bats-new-york-yankees-bfdeacd9\" target=\"_blank\">Full story via The Wall Street Journal</a></p><p><strong>Engineering athletes redefine routine</strong><br>After suffering a concussion during her sophomore year, Emiko Pope ’25 was inspired to explore the effectiveness of concussion headbands.<br><a href=\"https://asmedigitalcollection.asme.org/memagazineselect/article-abstract/147/3/1/1214637/Engineering-Athletes-Redefine-RoutineTackling?redirectedFrom=fulltext\" target=\"_blank\">Full story via American Society of Mechanical Engineers</a></p><p><strong>“I missed talking math with people”: why John Urschel left the NFL for MIT</strong><br>Assistant Professor John Urschel shares his decision to call an audible and leave his NFL career to focus on his love for math at MIT.<br><a href=\"https://www.theguardian.com/sport/2025/may/07/john-urschel-nfl-mit\" target=\"_blank\">Full story via The Guardian</a></p><p><strong>Making a statement, MIT’s football team dons extra head padding for safety</strong><br>It’s a piece of equipment that may become more widely used as research continues into its effectiveness — including from at least one of the players on the current team.<br><a href=\"https://www.wgbh.org/news/local/2025-11-14/making-a-statement-mits-football-team-dons-extra-head-padding-for-safety\" target=\"_blank\">Full story via GBH Morning Edition</a></p><p><strong><u>Agricultural efficiency</u></strong></p><p><strong>New MIT breakthrough could save farmers billions on pesticides</strong><br>MIT engineers developed a system that helps pesticides adhere more effectively to plant leaves, allowing farmers to use fewer chemicals.<br><a href=\"https://www.michiganfarmnews.com/new-mit-breakthrough-could-save-farmers-billions-on-pesticides\" target=\"_blank\">Full story via Michigan Farm News</a></p><p><strong>Bug-sized robots could help pollination on future farms</strong><br>Insect-sized robots crafted by MIT researchers could one day be used to help with farming practices like artificial pollination.<br><a href=\"https://www.reuters.com/video/watch/idRW569423012025RP1/\" target=\"_blank\">Full story via Reuters</a></p><p><strong>See how MIT researchers harvest water from the air</strong><br>An ultrasonic device created by MIT engineers can extract clean drinking water from atmospheric moisture.<br><a href=\"https://www.cnn.com/2025/11/24/science/video/mit-harvest-drinking-water-from-air-digvid-vrtc\" target=\"_blank\">Full story via CNN</a></p><p><strong><u>Appreciating art</u></strong></p><p><strong><u><o:p></o:p></u></strong></p><p><strong>Meet the engineer using deep learning to restore Renaissance art</strong><br>Graduate student Alex Kachkine talks about his work applying AI to develop a restoration method for damaged artwork.<br><a href=\"https://www.nature.com/articles/d41586-025-01776-8\" target=\"_blank\">Full story via Nature</a></p><p><strong>MIT’s Linde Music Building opens with a free festival</strong><br>“The extent of art-making on the MIT campus is equal to that of a major city,” says Institute Professor Marcus Thompson. “It’s a miracle that it’s all right here, by people in science and technology who are absorbed in creating a new world and who also value the past, present and future of music and the arts.”<br><a href=\"https://www.cambridgeday.com/2025/02/10/mits-linde-music-building-opens-with-a-festival-through-may-2-thats-free-and-open-to-the-public/\" target=\"_blank\">Full story via Cambridge Day</a></p><p><strong>“Remembering the Future” on display at the MIT Museum</strong><br>The “Remembering the Future” exhibit at the MIT Museum features a sculptural installation that uses “climate data from the last ice age to the present, as well as projected future environments, to create a geometric design.”<br><a href=\"https://www.nytimes.com/2025/10/15/arts/design/climate-change-art-museum.html\" target=\"_blank\">Full story via The New York Times</a>&nbsp;</p>",
      "author": "MIT News",
      "publishedAt": "2025-12-22T22:20:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "2e4817d7-f4e1-4d1c-8f4d-28c6a092b3a5",
      "guid": "https://news.mit.edu/2025/guided-learning-lets-untrainable-neural-networks-realize-their-potential-1218",
      "title": "Guided learning lets “untrainable” neural networks realize their potential",
      "link": "https://news.mit.edu/2025/guided-learning-lets-untrainable-neural-networks-realize-their-potential-1218",
      "content": "<p dir=\"ltr\" id=\"docs-internal-guid-fb3bf34a-7fff-7c39-06e2-626dc3313469\">Even networks long considered “untrainable” can learn effectively with a bit of a helping hand. Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have shown that a brief period of alignment between neural networks, a method they call guidance, can dramatically improve the performance of architectures previously thought unsuitable for modern tasks.</p><p dir=\"ltr\">Their findings suggest that many so-called “ineffective” networks may simply start from less-than-ideal starting points, and that short-term guidance can place them in a spot that makes learning easier for the network.&nbsp;</p><p dir=\"ltr\">The team’s guidance method works by encouraging a target network to match the internal representations of a guide network during training. Unlike traditional methods like knowledge distillation, which focus on mimicking a teacher’s outputs, guidance transfers structural knowledge directly from one network to another. This means the target learns how the guide organizes information within each layer, rather than simply copying its behavior. Remarkably, even untrained networks contain architectural biases that can be transferred, while trained guides additionally convey learned patterns.&nbsp;</p><p dir=\"ltr\">“We found these results pretty surprising,” says Vighnesh Subramaniam ’23, MEng ’24, MIT Department of Electrical Engineering and Computer Science (EECS) PhD student and CSAIL researcher, who is a lead author on a&nbsp;<a href=\"https://arxiv.org/abs/2410.20035\">paper</a> presenting these findings. “It’s impressive that we could use representational similarity to make these traditionally ‘crappy’ networks actually work.”</p><p dir=\"ltr\"><strong>Guide-ian angel&nbsp;</strong></p><p dir=\"ltr\">A central question was whether guidance must continue throughout training, or if its primary effect is to provide a better initialization. To explore this, the researchers performed an experiment with deep fully connected networks (FCNs). Before training on the real problem, the network spent a few steps practicing with another network using random noise, like stretching before exercise. The results were striking: Networks that typically overfit immediately remained stable, achieved lower training loss, and avoided the classic performance degradation seen in something called standard FCNs. This alignment acted like a helpful warmup for the network, showing that even a short practice session can have lasting benefits without needing constant guidance.</p><p dir=\"ltr\">The study also compared guidance to knowledge distillation, a popular approach in which a student network attempts to mimic a teacher’s outputs. When the teacher network was untrained, distillation failed completely, since the outputs contained no meaningful signal. Guidance, by contrast, still produced strong improvements because it leverages internal representations rather than final predictions. This result underscores a key insight: Untrained networks already encode valuable architectural biases that can steer other networks toward effective learning.</p><p dir=\"ltr\">Beyond the experimental results, the findings have broad implications for understanding neural network architecture. The researchers suggest that success — or failure — often depends less on task-specific data, and more on the network’s position in parameter space. By aligning with a guide network, it’s possible to separate the contributions of architectural biases from those of learned knowledge. This allows scientists to identify which features of a network’s design support effective learning, and which challenges stem simply from poor initialization.</p><p dir=\"ltr\">Guidance also opens new avenues for studying relationships between architectures. By measuring how easily one network can guide another, researchers can probe distances between functional designs and reexamine theories of neural network optimization. Since the method relies on representational similarity, it may reveal previously hidden structures in network design, helping to identify which components contribute most to learning and which do not.</p><p dir=\"ltr\"><strong>Salvaging the hopeless</strong></p><p dir=\"ltr\">Ultimately, the work shows that so-called “untrainable” networks are not inherently doomed. With guidance, failure modes can be eliminated, overfitting avoided, and previously ineffective architectures brought into line with modern performance standards. The CSAIL team plans to explore which architectural elements are most responsible for these improvements and how these insights can influence future network design. By revealing the hidden potential of even the most stubborn networks, guidance provides a powerful new tool for understanding — and hopefully shaping — the foundations of machine learning.</p><p dir=\"ltr\">“It’s generally assumed that different neural network architectures have particular strengths and weaknesses,” says Leyla Isik, Johns Hopkins University assistant professor of cognitive science, who wasn’t involved in the research. “This exciting research shows that one type of network can inherit the advantages of another architecture, without losing its original capabilities. Remarkably, the authors show this can be done using small, untrained ‘guide’ networks. This paper introduces a novel and concrete way to add different inductive biases into neural networks, which is critical for developing more efficient and human-aligned AI.”</p><p dir=\"ltr\">Subramaniam wrote the paper with CSAIL colleagues: Research Scientist Brian Cheung; PhD student David Mayo ’18, MEng ’19; Research Associate Colin Conwell; principal investigators Boris Katz, a CSAIL principal research scientist, and Tomaso Poggio, an MIT professor in brain and cognitive sciences; and former CSAIL research scientist Andrei Barbu. Their work was supported, in part, by the Center for Brains, Minds, and Machines, the National Science Foundation, the MIT CSAIL Machine Learning Applications Initiative, the MIT-IBM Watson AI Lab, the U.S. Defense Advanced Research Projects Agency (DARPA), the U.S. Department of the Air Force Artificial Intelligence Accelerator, and the U.S. Air Force Office of Scientific Research.<br><br>Their work was recently presented at the Conference and Workshop on Neural Information Processing Systems (NeurIPS).</p>",
      "author": "Rachel Gordon | MIT CSAIL",
      "publishedAt": "2025-12-18T21:20:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "a80dbefb-f1a3-4c11-9fe7-e175cedba9b9",
      "guid": "https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217",
      "title": "A new way to increase the capabilities of large language models",
      "link": "https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217",
      "content": "<p>Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&nbsp;</p><p>Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.</p><p>An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&nbsp;</p><p>Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.</p><p>“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.</p><p>A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.</p><p><strong>Path to understanding&nbsp;</strong></p><p>Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.</p><p>The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.</p><p>“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”</p><p><strong>Thinking bigger and more efficiently&nbsp;</strong></p><p>The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&nbsp;</p><p>Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”</p><p>This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.</p>",
      "author": "Lauren Hinkel | MIT-IBM Watson AI Lab",
      "publishedAt": "2025-12-18T04:10:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "862fcb5b-c5bf-4364-b9cf-8eb9835c8e60",
      "guid": "https://news.mit.edu/2025/scientific-sandbox-lets-researchers-explore-evolution-vision-systems-1217",
      "title": "A “scientific sandbox” lets researchers explore the evolution of vision systems",
      "link": "https://news.mit.edu/2025/scientific-sandbox-lets-researchers-explore-evolution-vision-systems-1217",
      "content": "<p>Why did humans evolve the eyes we have today?</p><p>While scientists can’t go back in time to study the environmental pressures that shaped the evolution of the diverse vision systems that exist in nature, a new computational framework developed by MIT researchers allows them to explore this evolution in artificial intelligence agents.</p><p>The framework they developed, in which embodied AI agents evolve eyes and learn to see over many generations, is like a “scientific sandbox” that allows researchers to recreate different evolutionary trees. The user does this by changing the structure of the world and the tasks AI agents complete, such as finding food or telling objects apart.</p><p>This allows them to study why one animal may have evolved simple, light-sensitive patches as eyes, while another has complex, camera-type eyes.</p><p>The researchers’ experiments with this framework showcase how tasks drove eye evolution in the agents. For instance, they found that navigation tasks often led to the evolution of compound eyes with many individual units, like the eyes of insects and crustaceans.</p><p>On the other hand, if agents focused on object discrimination, they were more likely to evolve camera-type eyes with irises and retinas.</p><p>This framework could enable scientists to probe “what-if” questions about vision systems that are difficult to study experimentally. It could also guide the design of novel sensors and cameras for robots, drones, and wearable devices that balance performance with real-world constraints like energy efficiency and manufacturability.</p><p>“While we can never go back and figure out every detail of how evolution took place, in this work we’ve created an environment where we can, in a sense, recreate evolution and probe the environment in all these different ways. This method of doing science opens to the door to a lot of possibilities,” says Kushagra Tiwary, a graduate student at the MIT Media Lab and co-lead author of a paper on this research.</p><p>He is joined on the paper by co-lead author and fellow graduate student Aaron Young; graduate student Tzofi Klinghoffer; former postdoc Akshat Dave, who is now an assistant professor at Stony Brook University; Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences, an investigator in the McGovern Institute, and co-director of the Center for Brains, Minds, and Machines; co-senior authors Brian Cheung, a postdoc in the&nbsp; Center for Brains, Minds, and Machines and an incoming assistant professor at the University of California San Francisco; and Ramesh Raskar, associate professor of media arts and sciences and leader of the Camera Culture Group at MIT; as well as others at Rice University and Lund University. The research <a href=\"https://doi.org/10.1126/sciadv.ady2888\" target=\"_blank\">appears today in <em>Science Advances</em></a>.</p><p><strong>Building a scientific sandbox</strong></p><p>The paper began as a conversation among the researchers about discovering new vision systems that could be useful in different fields, like robotics. To test their “what-if” questions, the researchers decided to <a href=\"https://mit-genai.pubpub.org/pub/bcfcb6lu/release/3\" target=\"_blank\">use AI to explore the many evolutionary possibilities</a>.</p><p>“What-if questions inspired me when I was growing up to study science. With AI, we have a unique opportunity to create these embodied agents that allow us to ask the kinds of questions that would usually be impossible to answer,” Tiwary says.</p><p>To build this evolutionary sandbox, the researchers took all the elements of a camera, like the sensors, lenses, apertures, and processors, and converted them into parameters that an embodied AI agent could learn.</p><p>They used those building blocks as the starting point for an algorithmic learning mechanism an agent would use as it evolved eyes over time.</p><p>“We couldn’t simulate the entire universe atom-by-atom. It was challenging to determine which ingredients we needed, which ingredients we didn’t need, and how to allocate resources over those different elements,” Cheung says.</p><p>In their framework, this evolutionary algorithm can choose which elements to evolve based on the constraints of the environment and the task of the agent.</p><p>Each environment has a single task, such as navigation, food identification, or prey tracking, designed to mimic real visual tasks animals must overcome to survive. The agents start with a single photoreceptor that looks out at the world and an associated neural network model that processes visual information.</p><p>Then, over each agent’s lifetime, it is trained using reinforcement learning, a trial-and-error technique where the agent is rewarded for accomplishing the goal of its task. The environment also incorporates constraints, like a certain number of pixels for an agent’s visual sensors.</p><p>“These constraints drive the design process, the same way we have physical constraints in our world, like the physics of light, that have driven the design of our own eyes,” Tiwary says.</p><p>Over many generations, agents evolve different elements of vision systems that maximize rewards.</p><p>Their framework uses a genetic encoding mechanism to computationally mimic evolution, where individual genes mutate to control an agent’s development.</p><p>For instance, morphological genes capture how the agent views the environment and control eye placement; optical genes determine how the eye interacts with light and dictate the number of photoreceptors; and neural genes control the learning capacity of the agents.</p><p><strong>Testing hypotheses</strong></p><p>When the researchers set up experiments in this framework, they found that tasks had a major influence on the vision systems the agents evolved.</p><p>For instance, agents that were focused on navigation tasks developed eyes designed to maximize spatial awareness through low-resolution sensing, while agents tasked with detecting objects developed eyes focused more on frontal acuity, rather than peripheral vision.</p><p>Another experiment indicated that a bigger brain isn’t always better when it comes to processing visual information. Only so much visual information can go into the system at a time, based on physical constraints like the number of photoreceptors in the eyes.</p><p>“At some point a bigger brain doesn’t help the agents at all, and in nature that would be a waste of resources,” Cheung says.</p><p>In the future, the researchers want to use this simulator to explore the best vision systems for specific applications, which could help scientists develop task-specific sensors and cameras. They also want to integrate LLMs into their framework to make it easier for users to ask “what-if” questions and study additional possibilities.</p><p>“There’s a real benefit that comes from asking questions in a more imaginative way. I hope this inspires others to create larger frameworks, where instead of focusing on narrow questions that cover a specific area, they are looking to answer questions with a much wider scope,” Cheung says.</p><p>This work was supported, in part, by the Center for Brains, Minds, and Machines and&nbsp;the Defense Advanced Research Projects Agency (DARPA) Mathematics for the Discovery of Algorithms and Architectures (DIAL) program.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-12-17T19:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "689d4324-4875-417b-a029-baeec39aa0ae",
      "guid": "https://news.mit.edu/2025/robot-makes-chair-1216",
      "title": "“Robot, make me a chair” ",
      "link": "https://news.mit.edu/2025/robot-makes-chair-1216",
      "content": "<p>Computer-aided design (CAD) systems are tried-and-true tools used to design many of the physical objects we use each day. But CAD software requires extensive expertise to master, and many tools incorporate such a high level of detail they don’t lend themselves to brainstorming or rapid prototyping.</p><p>In an effort to make design faster and more accessible for non-experts, researchers from MIT and elsewhere developed an AI-driven robotic assembly system that allows people to build physical objects by simply describing them in words.</p><p>Their system uses a generative AI model to build a 3D representation of an object’s geometry based on the user’s prompt. Then, a second generative AI model reasons about the desired object and figures out where different components should go, according to the object’s function and geometry.</p><p>The system can automatically build the object from a set of prefabricated parts using robotic assembly. It can also iterate on the design based on feedback from the user.</p><p>The researchers used this end-to-end system to fabricate furniture, including chairs and shelves, from two types of premade components. The components can be disassembled and reassembled at will, reducing the amount of waste generated through the fabrication process.</p><p>They evaluated these designs through a user study and found that more than 90 percent of participants preferred the objects made by their AI-driven system, as compared to different approaches.</p><p>While this work is an initial demonstration, the framework could be especially useful for rapid prototyping complex objects like aerospace components and architectural objects. In the longer term, it could be used in homes to fabricate furniture or other objects locally, without the need to have bulky products shipped from a central facility.</p><p>“Sooner or later, we want to be able to communicate and talk to a robot and AI system the same way we talk to each other to make things together. Our system is a first step toward enabling that future,” says lead author Alex Kyaw, a graduate student in the MIT departments of Electrical Engineering and Computer Science (EECS) and Architecture.</p><p>Kyaw is joined on the paper by Richa Gupta, an MIT architecture graduate student; Faez Ahmed, associate professor of mechanical engineering; Lawrence Sass, professor and chair of the Computation Group in the Department of Architecture; senior author Randall Davis, an EECS professor and member of the Computer Science and Artificial Intelligence Laboratory (CSAIL); as well as others at Google Deepmind and Autodesk Research. The <a href=\"https://arxiv.org/pdf/2511.02162\" target=\"_blank\">paper</a> was recently presented at the Conference on Neural Information Processing Systems.</p><p><strong>Generating a multicomponent design</strong></p><p>While generative AI models are good at generating 3D representations, known as meshes,&nbsp; from text prompts, most do not produce uniform representations of an object’s geometry that have the component-level details needed for robotic assembly.</p><p>Separating these meshes into components is challenging for a model because assigning components depends on the geometry and functionality of the object and its parts.</p><p>The researchers tackled these challenges using a vision-language model (VLM), a powerful generative AI model that has been pre-trained to understand images and text. They task the VLM with figuring out how two types of prefabricated parts, structural components and panel components, should fit together to form an object.</p><p>“There are many ways we can put panels on a physical object, but the robot needs to see the geometry and reason over that geometry to make a decision about it. By serving as both the eyes and brain of the robot, the VLM enables the robot to do this,” Kyaw says.</p><p>A user prompts the system with text, perhaps by typing “make me a chair,” and gives it an AI-generated image of a chair to start.</p><p>Then, the VLM reasons about the chair and determines where panel components go on top of structural components, based on the functionality of many example objects it has seen before. For instance, the model can determine that the seat and backrest should have panels to have surfaces for someone sitting and leaning on the chair.</p><p>It outputs this information as text, such as “seat” or “backrest.” Each surface of the chair is then labeled with numbers, and the information is fed back to the VLM.</p><p>Then the VLM chooses the labels that correspond to the geometric parts of the chair that should receive panels on the 3D mesh to complete the design.</p><p><strong>Human-AI co-design</strong></p><p>The user remains in the loop throughout this process and can refine the design by giving the model a new prompt, such as “only use panels on the backrest, not the seat.”</p><p>“The design space is very big, so we narrow it down through user feedback. We believe this is the best way to do it because people have different preferences, and building an idealized model for everyone would be impossible,” Kyaw says.</p><p>“The human‑in‑the‑loop process allows the users to steer the AI‑generated designs and have a sense of ownership in the final result,” adds Gupta.</p><p>Once the 3D mesh is finalized, a robotic assembly system builds the object using prefabricated parts. These reusable parts can be disassembled and reassembled into different configurations.</p><p>The researchers compared the results of their method with an algorithm that places panels on all horizontal surfaces that are facing up, and an algorithm that places panels randomly. In a user study, more than 90 percent of individuals preferred the designs made by their system.</p><p>They also asked the VLM to explain why it chose to put panels in those areas.</p><p>“We learned that the vision language model is able to understand some degree of the functional aspects of a chair, like leaning and sitting, to understand why it is placing panels on the seat and backrest. It isn’t just randomly spitting out these assignments,” Kyaw says.</p><p>In the future, the researchers want to enhance their system to handle more complex and nuanced user prompts, such as a table made out of glass and metal. In addition, they want to incorporate additional prefabricated components, such as gears, hinges, or other moving parts, so objects could have more functionality.</p><p>“Our hope is to drastically lower the barrier of access to design tools. We have shown that we can use generative AI and robotics to turn ideas into physical objects in a fast, accessible, and sustainable manner,” says Davis.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-12-16T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "c10730c1-622b-4ba2-b51f-e6823935d00e",
      "guid": "https://news.mit.edu/2025/3-questions-yunha-hwang-using-computation-study-worlds-best-single-celled-chemists-1215",
      "title": "3 Questions: Using computation to study the world’s best single-celled chemists",
      "link": "https://news.mit.edu/2025/3-questions-yunha-hwang-using-computation-study-worlds-best-single-celled-chemists-1215",
      "content": "<p dir=\"ltr\"><em>Today, out of an estimated 1 trillion species on Earth, 99.999 percent are considered microbial — bacteria, archaea, viruses, and single-celled eukaryotes. For much of our planet’s history, microbes ruled the Earth, able to live and thrive in the most extreme of environments. Researchers have only just begun in the last few decades to contend with the diversity of microbes — it’s estimated that less than 1 percent of known genes have laboratory-validated functions. Computational approaches offer researchers the opportunity to strategically parse this truly astounding amount of information.</em></p><p dir=\"ltr\"><em>An environmental microbiologist and computer scientist by training, new MIT faculty member </em><a href=\"https://biology.mit.edu/profile/yunha-hwang/\" target=\"_blank\"><em>Yunha Hwang</em></a><em> is interested in the novel biology revealed by the most diverse and prolific life form on Earth. In a shared faculty position as the Samuel A. Goldblith Career Development Professor in the </em><a href=\"https://biology.mit.edu/\" target=\"_blank\"><em>Department of Biology,</em></a><em> as well as an assistant professor at the </em><a href=\"https://www.eecs.mit.edu/\"><em>Department of Electrical Engineering and Computer Science</em></a><em> and the </em><a href=\"https://computing.mit.edu/\" target=\"_blank\"><em>MIT Schwarzman College of Computing</em></a><em>, Hwang is exploring the intersection of computation and biology.&nbsp;&nbsp;</em></p><p dir=\"ltr\"><strong>Q: </strong>What drew you to research microbes in extreme environments, and what are the challenges in studying them?&nbsp;</p><p dir=\"ltr\"><strong>A: </strong>Extreme environments are great places to look for interesting biology. I wanted to be an astronaut growing up, and the closest thing to astrobiology is examining extreme environments on Earth. And the only thing that lives in those extreme environments are microbes. During a sampling expedition that I took part in off the coast of Mexico, we discovered a colorful microbial mat about 2 kilometers underwater that flourished because the bacteria breathed sulfur instead of oxygen — but none of the microbes I was hoping to study would grow in the lab.&nbsp;</p><p dir=\"ltr\">The biggest challenge in studying microbes is that a majority of them cannot be cultivated, which means that the only way to study their biology is through a method called metagenomics. My latest work is genomic language modeling. We’re hoping to develop a computational system so we can probe the organism as much as possible “in silico,” just using sequence data.&nbsp;A genomic language model is technically a large language model, except the language is DNA as opposed to human language. It’s trained in a similar way, just in biological language as opposed to English or French. If our objective is to learn the language of biology, we should leverage the diversity of microbial genomes. Even though we have a lot of data, and even as more samples become available, we’ve just scratched the surface of microbial diversity.&nbsp;</p><p dir=\"ltr\"><strong>Q: </strong>Given how diverse microbes are and how little we understand about them, how can studying microbes in silico, using genomic language modeling, advance our understanding of the microbial genome?&nbsp;</p><p dir=\"ltr\"><strong>A: </strong>A genome is many millions of letters. A human cannot possibly look at that and make sense of it. We can program a machine, though, to segment data into pieces that are useful. That’s sort of how bioinformatics works with a single genome. But if you’re looking at a gram of soil, which can contain thousands of unique genomes, that’s just too much data to work with — a human and a computer together are necessary in order to grapple with that data.&nbsp;</p><p dir=\"ltr\">During my PhD and master’s degree, we were only just discovering new genomes and new lineages that were so different from anything that had been characterized or grown in the lab. These were things that we just called “microbial dark matter.” When there are a lot of uncharacterized things, that’s where machine learning can be really useful, because we’re just looking for patterns — but that’s not the end goal. What we hope to do is to map these patterns to evolutionary relationships between each genome, each microbe, and each instance of life.&nbsp;</p><p dir=\"ltr\">Previously, we’ve been thinking about proteins as a standalone entity — that gets us to a decent degree of information because proteins are related by homology, and therefore things that are evolutionarily related might have a similar function.&nbsp;</p><p dir=\"ltr\">What is known about microbiology is that proteins are encoded into genomes, and the context in which that protein is bounded — what regions come before and after — is evolutionarily conserved, especially if there is a functional coupling. This makes total sense because when you have three proteins that need to be expressed together because they form a unit, then you might want them located right next to each other.&nbsp;</p><p dir=\"ltr\">What I want to do is incorporate more of that genomic context in the way that we search for and annotate proteins and understand protein function, so that we can go beyond sequence or structural similarity to add contextual information to how we understand proteins and hypothesize about their functions.&nbsp;</p><p dir=\"ltr\"><strong>Q: </strong>How can your research be applied to harnessing the functional potential of microbes?&nbsp;</p><p dir=\"ltr\"><strong>A: </strong>Microbes are possibly the world’s best chemists. Leveraging microbial metabolism and biochemistry will lead to more sustainable and more efficient methods for producing new materials, new therapeutics, and new types of polymers.&nbsp;</p><p dir=\"ltr\">But it’s not just about efficiency — microbes are doing chemistry we don’t even know how to think about. Understanding how microbes work, and being able to understand their genomic makeup and their functional capacity, will also be really important as we think about how our world and climate are changing. A majority of carbon sequestration and nutrient cycling is undertaken by microbes; if we don’t understand how a given microbe is able to fix nitrogen or carbon, then we will face difficulties in modeling the nutrient fluxes of the Earth.&nbsp;</p><p dir=\"ltr\">On the more therapeutic side, infectious diseases are a real and growing threat. Understanding how microbes behave in diverse environments relative to the rest of our microbiome is really important as we think about the future and combating microbial pathogens.&nbsp;</p>",
      "author": "Lillian Eden | Department of Biology",
      "publishedAt": "2025-12-15T22:15:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "af5f3eba-93d6-405f-9ee0-922006f268b7",
      "guid": "https://news.mit.edu/2025/dauren-sarsenbayev-working-to-eliminate-barriers-adopting-nuclear-energy-1215",
      "title": "Working to eliminate barriers to adopting nuclear energy",
      "link": "https://news.mit.edu/2025/dauren-sarsenbayev-working-to-eliminate-barriers-adopting-nuclear-energy-1215",
      "content": "<p role=\"document\" aria-multiline=\"true\" id=\"block-4155ad9e-8d29-42f6-80b5-e2980e3c1927\" aria-label=\"Block: Paragraph\" data-block=\"4155ad9e-8d29-42f6-80b5-e2980e3c1927\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">What if there were a way to solve one of the most significant obstacles to the use of nuclear energy — the disposal of high-level nuclear waste (HLW)?&nbsp;Dauren Sarsenbayev, a third-year doctoral student at the MIT Department of Nuclear Science and Engineering (NSE), is addressing the challenge as part of his research.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-22dd3f1a-1992-49d1-8a7b-c606b753b1a3\" aria-label=\"Block: Paragraph\" data-block=\"22dd3f1a-1992-49d1-8a7b-c606b753b1a3\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Sarsenbayev focuses on one of the primary problems related to HLW: decay heat released by radioactive waste. The basic premise of his solution is to extract the heat from spent fuel, which simultaneously takes care of two objectives: gaining more energy from an existing carbon-free resource while decreasing the challenges associated with storage and handling of HLW. “The value of carbon-free energy continues to rise each year, and we want to extract as much of it as possible,” Sarsenbayev explains.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-1da96552-1014-494c-ae90-4d35fe558835\" aria-label=\"Block: Paragraph\" data-block=\"1da96552-1014-494c-ae90-4d35fe558835\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">While the safe management and disposal of HLW has seen significant progress, there can be more creative ways to manage or take advantage of the waste. Such a move would be especially important for the public’s acceptance of nuclear energy. “We’re reframing the problem of nuclear waste, transforming it from a liability to an energy source,” Sarsenbayev says.</p><p><strong>The nuances of nuclear</strong></p><p role=\"document\" aria-multiline=\"true\" id=\"block-5fbcb667-9232-42f7-a731-ee9b9402e0ca\" aria-label=\"Block: Paragraph\" data-block=\"5fbcb667-9232-42f7-a731-ee9b9402e0ca\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Sarsenbayev had to do a bit of reframing himself in how he perceived nuclear energy. Growing up in Almaty, the largest city in Kazakhstan, the collective trauma of Soviet nuclear testing loomed large over the public consciousness. Not only does the country, once a part of the Soviet Union, carry the scars of nuclear weapon testing, Kazakhstan is the world’s largest producer of uranium. It’s hard to escape the collective psyche of such a legacy.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-8b66a9b2-2b82-480e-8f57-49c56e0a56b3\" aria-label=\"Block: Paragraph\" data-block=\"8b66a9b2-2b82-480e-8f57-49c56e0a56b3\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">At the same time, Sarsenbayev saw his native Almaty choking under heavy smog every winter, due to the burning of fossil fuels for heat. Determined to do his part to accelerate the process of decarbonization, Sarsenbayev gravitated to undergraduate studies in environmental engineering at Kazakh-German University. It was during this time that Sarsenbayev realized practically every energy source, even the promising renewable ones, came with challenges, and decided nuclear was the way to go for its reliable, low-carbon power. “I was exposed to air pollution from childhood; the horizon would be just black. The biggest incentive for me with nuclear power was that as long as we did it properly, people could breathe cleaner air,” Sarsenbayev says.</p><p><strong>Studying transport of radionuclides</strong></p><p role=\"document\" aria-multiline=\"true\" id=\"block-063fce8f-c7f3-4da6-90d4-c551d622807c\" aria-label=\"Block: Paragraph\" data-block=\"063fce8f-c7f3-4da6-90d4-c551d622807c\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Part of “doing nuclear properly” involves studying — and reliably predicting — the long-term behavior of radionuclides in geological repositories.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-34bbad52-7075-488d-95ed-15473628e949\" aria-label=\"Block: Paragraph\" data-block=\"34bbad52-7075-488d-95ed-15473628e949\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Sarsenbayev discovered an interest in studying nuclear waste management during an internship at Lawrence Berkeley National Laboratory as a junior undergraduate student.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-e10d47d2-0807-40a2-bf18-2a8aa65b07fe\" aria-label=\"Block: Paragraph\" data-block=\"e10d47d2-0807-40a2-bf18-2a8aa65b07fe\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">While at Berkeley, Sarsenbayev focused on modeling the transport of radionuclides from the nuclear waste repository’s barrier system to the surrounding host rock. He discovered how to use the tools of the trade to predict long-term behavior. “As an undergrad, I was really fascinated by how far in the future something could be predicted. It’s kind of like foreseeing what future generations will encounter,” Sarsenbayev says.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-932f4044-383c-41ca-9cfe-c1069d89f2c0\" aria-label=\"Block: Paragraph\" data-block=\"932f4044-383c-41ca-9cfe-c1069d89f2c0\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">The timing of the Berkeley internship was fortuitous. It was at the laboratory that he worked with <a href=\"https://nse.mit.edu/people/haruko-wainwright/\">Haruko Murakami Wainwright</a>, who was herself getting started at MIT NSE. (Wainwright is the Mitsui Career Development Professor in Contemporary Technology, and an assistant professor of NSE and of civil and environmental engineering).</p><p role=\"document\" aria-multiline=\"true\" id=\"block-6c77880c-2520-4382-97f6-6ebc8de63804\" aria-label=\"Block: Paragraph\" data-block=\"6c77880c-2520-4382-97f6-6ebc8de63804\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Looking to pursue graduate studies in the field of nuclear waste management, Sarsenbayev followed Wainwright to MIT, where he has further researched the modeling of radionuclide transport. He is the first author on a <a href=\"https://www.pnas.org/doi/10.1073/pnas.2511885122\">paper</a> that details mechanisms to increase the robustness of models describing the transport of radionuclides. The work captures the complexity of interactions between engineered barrier components, including cement-based materials and clay barriers, the typical medium proposed for the storage and disposal of spent nuclear fuel.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-dbb1e92b-61bb-44b3-9b56-77e5aec2b3c9\" aria-label=\"Block: Paragraph\" data-block=\"dbb1e92b-61bb-44b3-9b56-77e5aec2b3c9\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Sarsenbayev is pleased with the results of the model’s prediction, which closely mirrors experiments conducted at the <a href=\"https://www.mont-terri.ch/en\">Mont Terri research site</a> in Switzerland, famous for studies in the interactions between cement and clay. “I was fortunate to work with Doctor Carl Steefel and Professor Christophe Tournassat, leading experts in computational geochemistry,” he says.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-13356962-2600-4993-877d-0ea6f0bb1865\" aria-label=\"Block: Paragraph\" data-block=\"13356962-2600-4993-877d-0ea6f0bb1865\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Real-life transport mechanisms involve many physical and chemical processes, the complexities of which increase the size of the computational model dramatically. Reactive transport modeling — which combines the simulation of fluid flow, chemical reactions, and the transport of substances through subsurface media — has evolved significantly over the past few decades. However, running accurate simulations comes with trade-offs: The software can require days to weeks of computing time on high-performance clusters running in parallel.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-d270caf8-2dac-4ca6-9392-dcfff12a30f7\" aria-label=\"Block: Paragraph\" data-block=\"d270caf8-2dac-4ca6-9392-dcfff12a30f7\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">To arrive at results faster by saving on computing time, Sarsenbayev is developing a framework that integrates AI-based “surrogate models,” which train on simulated data and approximate the physical systems. The AI algorithms make predictions of radionuclide behavior faster and less computationally intensive than the traditional equivalent.</p><p><strong>Doctoral research focus</strong></p><p role=\"document\" aria-multiline=\"true\" id=\"block-f6486876-d88f-4a06-87c6-f437318523b8\" aria-label=\"Block: Paragraph\" data-block=\"f6486876-d88f-4a06-87c6-f437318523b8\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Sarsenbayev is using his modeling expertise in his primary doctoral work as well — in evaluating the potential of spent nuclear fuel as an anthropogenic geothermal energy source. “In fact, geothermal heat is largely due to the natural decay of radioisotopes in Earth’s crust, so using decay heat from spent fuel is conceptually similar,” he says. A canister of nuclear waste can generate, under conservative assumptions, the energy equivalent of 1,000 square meters (a little under a quarter of an acre) of solar panels.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-655fe702-e48f-4c73-a2b8-73cfda854cff\" aria-label=\"Block: Paragraph\" data-block=\"655fe702-e48f-4c73-a2b8-73cfda854cff\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Because the potential for heat from a canister is significant — a typical one (depending on how long it was cooled in the spent fuel pool) has a temperature of around 150 degrees Celsius — but not enormous, extracting heat from this source makes use of a process called a binary cycle system. In such a system, heat is extracted indirectly: the canister warms a closed water loop, which in turn transfers that heat to a secondary low-boiling-point fluid that powers the turbine.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-190deca4-cd51-473a-967b-9b65087b828f\" aria-label=\"Block: Paragraph\" data-block=\"190deca4-cd51-473a-967b-9b65087b828f\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Sarsenbayev’s work develops a conceptual model of a binary-cycle geothermal system powered by heat from high-level radioactive waste. Early modeling results have been <a href=\"https://www.researchgate.net/publication/380065256_Harnessing_Geothermal_Energy_Potential_from_High-Level_Nuclear_Waste_Repositories\">published</a> and look promising. While the potential for such energy extraction is at the proof-of-concept stage in modeling, Sarsenbayev is hopeful that it will find success when translated to practice. “Converting a liability into an energy source is what we want, and this solution delivers,” he says.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-e27f7135-a14e-48bd-8dbb-b643d5ec657f\" aria-label=\"Block: Paragraph\" data-block=\"e27f7135-a14e-48bd-8dbb-b643d5ec657f\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Despite work being all-consuming — “I’m almost obsessed with and love my work” — Sarsenbayev finds time to write reflective poetry in both Kazakh, his native language, and Russian, which he learned growing up. He’s also enamored by astrophotography, taking pictures of celestial bodies. Finding the right night sky can be a challenge, but the canyons near his home in Almaty are an especially good fit. He goes on photography sessions whenever he visits home for the holidays, and his love for Almaty shines through. “Almaty means 'the place where apples originated.' This part of Central Asia is very beautiful; although we have environmental pollution, this is a place with a rich history,” Sarsenbayev says.</p><p role=\"document\" aria-multiline=\"true\" id=\"block-e53afe24-6d88-4451-8d63-52c2089519cb\" aria-label=\"Block: Paragraph\" data-block=\"e53afe24-6d88-4451-8d63-52c2089519cb\" data-type=\"core/paragraph\" data-title=\"Paragraph\" data-empty=\"false\" contenteditable=\"true\" data-wp-block-attribute-key=\"content\">Sarsenbayev is especially keen on finding ways to communicate both the arts and sciences to future generations. “Obviously, you have to be technically rigorous and get the modeling right, but you also have to understand and convey the broader picture of why you’re doing the work, what the end goal is,” he says. Through that lens, the impact of Sarsenbayev’s doctoral work is significant. The end goal? Removing the bottleneck for nuclear energy adoption by producing carbon-free power and ensuring the safe disposal of radioactive waste.</p>",
      "author": "Poornima Apte | Department of Nuclear Science and Engineering",
      "publishedAt": "2025-12-15T21:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "d40f783d-b37a-4246-a02c-d1657db6df8e",
      "guid": "https://news.mit.edu/2025/deep-learning-model-predicts-how-fruit-flies-form-1215",
      "title": "Deep-learning model predicts how fruit flies form, cell by cell",
      "link": "https://news.mit.edu/2025/deep-learning-model-predicts-how-fruit-flies-form-1215",
      "content": "<p>During early development, tissues and organs begin to bloom through the shifting, splitting, and growing of many thousands of cells.</p><p>A team of MIT engineers has now developed a way to predict, minute by minute, how individual cells will fold, divide, and rearrange during a fruit fly’s earliest stage of growth. The new method may one day be applied to predict the development of more complex tissues, organs, and organisms. It could also help scientists identify cell patterns that correspond to early-onset diseases, such as asthma and cancer.</p><p>In a study <a href=\"https://www.nature.com/articles/s41592-025-02983-x\" target=\"_blank\">appearing today in the journal <em>Nature Methods</em></a>, the team presents a new deep-learning model that learns, then predicts, how certain geometric properties of individual cells will change as a fruit fly develops. The model records and tracks properties such as a cell’s position, and whether it is touching a neighboring cell at a given moment.</p><p>The team applied the model to videos of developing fruit fly embryos, each of which starts as a cluster of about 5,000 cells. They found the model could predict, with 90 percent accuracy, how each of the 5,000 cells would fold, shift, and rearrange, minute by minute, during the first hour of development, as the embryo morphs from a smooth, uniform shape into more defined structures and features.</p><p>“This very initial phase is known as gastrulation, which takes place over roughly one hour, when individual cells are rearranging on a time scale of minutes,” says study author Ming Guo, associate professor of mechanical engineering at MIT. “By accurately modeling this early period, we can start to uncover how local cell interactions give rise to global tissues and organisms.”</p><p>The researchers hope to apply the model to predict the cell-by-cell development in other species, such zebrafish and mice. Then, they can begin to identify patterns that are common across species. The team also envisions that the method could be used to discern early patterns of disease, such as in asthma. Lung tissue in people with asthma looks markedly different from healthy lung tissue. How asthma-prone tissue initially develops is an unknown process that the team’s new method could potentially reveal.</p><p>“Asthmatic tissues show different cell dynamics when imaged live,” says co-author and MIT graduate student Haiqian Yang. “We envision that our model could capture these subtle dynamical differences and provide a more comprehensive representation of tissue behavior, potentially improving diagnostics or drug-screening assays.”</p><p>The study’s co-authors are Markus Buehler, the&nbsp;McAfee Professor of Engineering in MIT’s Department of Civil and Environmental Engineering; George Roy and Tomer Stern of the University of Michigan; and Anh Nguyen and Dapeng Bi of Northeastern University.<br><br><strong>Points and foams</strong></p><p>Scientists typically model how an embryo develops in one of two ways: as a point cloud, where each point represents an individual cell as point that moves over time; or as a “foam,” which represents individual cells as bubbles that shift and slide against each other, similar to the bubbles in shaving foam.</p><p>Rather than choose between the two approaches, Guo and Yang embraced both.</p><p>“There’s a debate about whether to model as a point cloud or a foam,” Yang says. “But both of them are essentially different ways of modeling the same underlying graph, which is an elegant way to represent living tissues. By combining these as one graph, we can highlight more structural information, like how cells are connected to each other as they rearrange over time.”</p><p>At the heart of the new model is a “dual-graph” structure that represents a developing embryo as both moving points and bubbles. Through this dual representation, the researchers hoped to capture more detailed geometric properties of individual cells, such as the location of a cell’s nucleus, whether a cell is touching a neighboring cell, and whether it is folding or dividing at a given moment in time.</p><p>As a proof of principle, the team trained the new model to “learn” how individual cells change over time during fruit fly gastrulation.</p><p>“The overall shape of the fruit fly at this stage is roughly an ellipsoid, but there are gigantic dynamics going on at the surface during gastrulation,” Guo says. “It goes from entirely smooth to forming a number of folds at different angles. And we want to predict all of those dynamics, moment to moment, and cell by cell.”</p><p><strong>Where and when</strong></p><p>For their new study, the researchers applied the new model to high-quality videos of fruit fly gastrulation taken by their collaborators at the University of Michigan. The videos are one-hour recordings of developing fruit flies, taken at single-cell resolution. What’s more, the videos contain labels of individual cells’ edges and nuclei — data that are incredibly detailed and difficult to come by.</p><p>“These videos are of extremely high quality,” Yang says. “This data is very rare, where you get submicron resolution of the whole 3D volume at a pretty fast frame rate.”</p><p>The team trained the new model with data from three of four fruit fly embryo videos, such that the model might “learn” how individual cells interact and change as an embryo develops. They then tested the model on an entirely new fruit fly video, and found that it was able to predict with high accuracy how most of the embryo’s 5,000 cells changed from minute to minute.</p><p>Specifically, the model could predict properties of individual cells, such as whether they will fold, divide, or continue sharing an edge with a neighboring cell, with about 90 percent accuracy.</p><p>“We end up predicting not only whether these things will happen, but also when,” Guo says. “For instance, will this cell detach from this cell seven minutes from now, or eight? We can tell when that will happen.”</p><p>The team believes that, in principle, the new model, and the dual-graph approach, should be able to predict the cell-by-cell development of other multiceullar systems, such as more complex species, and even some human tissues and organs. The limiting factor is the availability of high-quality video data.</p><p>“From the model perspective, I think it’s ready,” Guo says. “The real bottleneck is the data. If we have good quality data of specific tissues, the model could be directly applied to predict the development of many more structures.”</p><p>This work is supported, in part, by the U.S. National Institutes of Health.</p>",
      "author": "Jennifer Chu | MIT News",
      "publishedAt": "2025-12-15T10:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "9f6660b4-8988-45d9-80c0-ea79f476a34c",
      "guid": "https://news.mit.edu/2025/enabling-small-language-models-solve-complex-reasoning-tasks-1212",
      "title": "Enabling small language models to solve complex reasoning tasks",
      "link": "https://news.mit.edu/2025/enabling-small-language-models-solve-complex-reasoning-tasks-1212",
      "content": "<p dir=\"ltr\" id=\"docs-internal-guid-7a6dc935-7fff-5776-c2a2-b03a9816a853\">As language models (LMs) improve at tasks like image generation, trivia questions, and simple math, you might think that human-like reasoning is around the corner. In reality, they still trail us by a wide margin on complex tasks. Try playing Sudoku with one, for instance, where you fill in numbers one through nine in such a way that each appears only once across the columns, rows, and sections of a nine-by-nine grid. Your AI opponent will either fail to fill in boxes on its own or do so inefficiently, although it can verify if you’ve filled yours out correctly.</p><p dir=\"ltr\">Whether an LM is trying to solve advanced puzzles, design molecules, or write math proofs, the system struggles to answer open-ended requests that have strict rules to follow. The model is better at telling users how to approach these challenges than attempting them itself. Moreover, hands-on problem-solving requires LMs to consider a wide range of options while following constraints. Small LMs can’t do this reliably on their own; large language models (LLMs) sometimes can, particularly if they’re optimized for reasoning tasks, but they take a while to respond, and they use a lot of computing power.</p><p dir=\"ltr\">This predicament led researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) to develop a collaborative approach where an LLM does the planning, then divvies up the legwork of that strategy among smaller ones. Their method helps small LMs provide more accurate responses than leading LLMs like OpenAI’s&nbsp;<a href=\"https://openai.com/index/hello-gpt-4o/\">GPT-4o</a>, and approach the precision of top reasoning systems such as&nbsp;<a href=\"https://openai.com/o1/\">o1</a>, while being more efficient than both. Their framework, called “Distributional Constraints by Inference Programming with Language Models” (or “DisCIPL”), has a large model steer smaller “follower” models toward precise responses when writing things like text blurbs, grocery lists with budgets, and travel itineraries.<br><br>The inner workings of DisCIPL are much like contracting a company for a particular job. You provide a “boss” model with a request, and it carefully considers how to go about doing that project. Then, the LLM relays these instructions and guidelines in a clear way to smaller models. It corrects follower LMs’ outputs where needed — for example, replacing one model’s phrasing that doesn’t fit in a poem with a better option from another.</p><p dir=\"ltr\">The LLM communicates with its followers using a language they all understand — that is, a programming language for controlling LMs called&nbsp;<a href=\"https://arxiv.org/abs/2306.03081\">“LLaMPPL.”</a>&nbsp;Developed by MIT's Probabilistic Computing Project in 2023, this program allows users to encode specific rules that steer a model toward a desired result. For example, LLaMPPL can be used to produce&nbsp;<a href=\"https://news.mit.edu/2025/making-ai-generated-code-more-accurate-0418\">error-free code</a> by incorporating the rules of a particular language within its instructions. Directions like “write eight lines of poetry where each line has exactly eight words” are encoded in LLaMPPL, queuing smaller models to contribute to different parts of the answer.</p><p dir=\"ltr\">MIT PhD student Gabriel Grand, who is the lead author on a&nbsp;<a href=\"https://openreview.net/pdf?id=XvCBtm5PgF\" target=\"_blank\">paper</a> presenting this work, says that DisCIPL allows LMs to guide each other toward the best responses, which improves their overall efficiency. “We’re working toward improving LMs’ inference efficiency, particularly on the many modern applications of these models that involve generating outputs subject to constraints,” adds Grand, who is also a CSAIL researcher. “Language models are consuming more energy as people use them more, which means we need models that can provide accurate answers while using minimal computing power.”</p><p dir=\"ltr\">“It's really exciting to see new alternatives to standard language model inference,” says University of California at Berkeley Assistant Professor Alane Suhr, who wasn’t involved in the research. “This work invites new approaches to language modeling and LLMs that significantly reduce inference latency via parallelization, require significantly fewer parameters than current LLMs, and even improve task performance over standard serialized inference. The work also presents opportunities to explore transparency, interpretability, and controllability of model outputs, which is still a huge open problem in the deployment of these technologies.”<br><br><strong>An underdog story</strong><br><br>You may think that larger-scale LMs are “better” at complex prompts than smaller ones when it comes to accuracy and efficiency. DisCIPL suggests a surprising counterpoint for these tasks: If you can combine the strengths of smaller models instead, you may just see an efficiency bump with similar results.</p><p dir=\"ltr\">The researchers note that, in theory, you can plug in dozens of LMs to work together in the DisCIPL framework, regardless of size. In writing and reasoning experiments, they went with GPT-4o as their “planner LM,” which is one of the models that helps ChatGPT generate responses. It brainstormed a plan for several&nbsp;<a href=\"https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/\">“Llama-3.2-1B”</a> models (smaller systems developed by Meta), in which those LMs filled in each word (or token) of the response.</p><p dir=\"ltr\">This collective approach competed against three comparable ones: a follower-only baseline powered by Llama-3.2-1B, GPT-4o working on its own, and the industry-leading o1 reasoning system that helps ChatGPT figure out more complex questions, such as coding requests and math problems.<br><br>DisCIPL first presented an ability to write sentences and paragraphs that follow explicit rules. The models were given very specific prompts — for example, writing a sentence that has exactly 18 words, where the fourth word must be “Glasgow,” the eighth should be “in”, and the 11th must be “and.” The system was remarkably adept at handling this request, crafting coherent outputs while achieving accuracy and coherence similar to o1.<br><br><strong>Faster, cheaper, better</strong></p><p dir=\"ltr\">This experiment also revealed that key components of DisCIPL were much cheaper than state-of-the-art systems. For instance, whereas existing reasoning models like OpenAI’s o1 perform reasoning in text, DisCIPL “reasons” by writing Python code, which is more compact. In practice, the researchers found that DisCIPL led to 40.1 percent shorter reasoning and 80.2 percent cost savings over o1.</p><p dir=\"ltr\">DisCIPL’s efficiency gains stem partly from using small Llama models as followers, which are 1,000 to 10,000 times cheaper per token than comparable reasoning models. This means that DisCIPL is more “scalable” — the researchers were able to run dozens of Llama models in parallel for a fraction of the cost.<br><br>Those weren’t the only surprising findings, according to CSAIL researchers. Their system also performed well against o1 on real-world tasks, such as making ingredient lists, planning out a travel itinerary, and writing grant proposals with word limits. Meanwhile, GPT-4o struggled with these requests, and with writing tests, it often couldn’t place keywords in the correct parts of sentences. The follower-only baseline essentially finished in last place across the board, as it had difficulties with following instructions.</p><p dir=\"ltr\">“Over the last several years, we’ve seen some impressive results from approaches that use language models to&nbsp;‘<a href=\"https://news.mit.edu/2024/natural-language-boosts-llm-performance-coding-planning-robotics-0501\" target=\"_blank\">auto-formalize</a>’ problems in math and robotics by representing them with code,” says senior author Jacob Andreas, who is an MIT electrical engineering and computer science associate professor and CSAIL principal investigator. “What I find most exciting about this paper is the fact that we can now use LMs to auto-formalize text generation itself, enabling the same kinds of efficiency gains and guarantees that we’ve seen in these other domains.”&nbsp;</p><p dir=\"ltr\">In the future, the researchers plan on expanding this framework into a more fully-recursive approach, where you can use the same model as both the leader and followers. Grand adds that DisCIPL could be extended to mathematical reasoning tasks, where answers are harder to verify. They also intend to test the system on its ability to meet users’ fuzzy preferences, as opposed to following hard constraints, which can’t be outlined in code so explicitly. Thinking even bigger, the team hopes to use the largest possible models available, although they note that such experiments are computationally expensive.</p><p dir=\"ltr\">Grand and Andreas wrote the paper alongside CSAIL principal investigator and MIT Professor Joshua Tenenbaum, as well as MIT Department of Brain and Cognitive Sciences Principal Research Scientist Vikash Mansinghka and Yale University Assistant Professor Alex Lew SM ’20 PhD ’25. CSAIL researchers presented the work at the Conference on Language Modeling in October and IVADO’s “Deploying Autonomous Agents: Lessons, Risks and Real-World Impact” workshop in November.<br><br>Their work was supported, in part, by the MIT Quest for Intelligence, Siegel Family Foundation, the MIT-IBM Watson AI Lab, a Sloan Research Fellowship, Intel, the Air Force Office of Scientific Research, the Defense Advanced Research Projects Agency, the Office of Naval Research, and the National Science Foundation.</p>",
      "author": "Alex Shipps | MIT CSAIL",
      "publishedAt": "2025-12-12T20:30:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "2dfb7a1c-60a6-4a4b-9133-38087e745aac",
      "guid": "https://news.mit.edu/2025/applied-ai-program-train-military-leaders-ai-age-1212",
      "title": "New MIT program to train military leaders for the AI age",
      "link": "https://news.mit.edu/2025/applied-ai-program-train-military-leaders-ai-age-1212",
      "content": "<p>Artificial intelligence can enhance decision-making and enable action with reduced risk and greater precision, making it a critical tool for national security. A new program offered jointly by the MIT departments of Mechanical Engineering (Course 2, MechE) and Electrical Engineering and Computer Science (Course 6, EECS) will provide breadth and depth in technical studies for naval officers, as well as a path for non-naval officers studying at MIT, to grow in their understanding of applied AI for naval and military applications.</p><p>“The potential for artificial intelligence is just starting to be fully realized. It’s a tool that dramatically improves speed, efficiency, and decision-making with countless applications,” says Commander Christopher MacLean, MIT associate professor of the practice in mechanical engineering, naval construction, and engineering. “AI is a force multiplier that can be used for data processing, decision support, unmanned and autonomous systems, cyber defense, logistics and supply chains, energy management, and many other fields.”</p><p>The program, called “<a href=\"https://2n6.mit.edu/\">2N6: Applied Artificial Intelligence Program for Naval Officers</a>,” comprises a two-year master of science degree in mechanical engineering with an accompanying AI certificate awarded by the MIT Schwarzman College of Computing.</p><p>“The officers entering this program will learn from the world’s experts, and conduct cutting-edge relevant research, and will exit the program best prepared for their roles as leaders across the U.S. naval enterprise,” says MacLean.</p><p>The <a href=\"https://2n6.mit.edu/curriculum/\">2N6 curriculum</a> is application focused, and the content is built to satisfy the U.S. Navy’s sub-specialty code for Applied Artificial Intelligence. Students will learn core AI concepts, as well as applications to special topics, such as decision-making for computational exercises; AI for manufacturing and design, with special emphasis on navy applications; and AI for marine autonomy of surface and underwater vehicles.</p><p>“The expanding influence of artificial intelligence is redefining our approach to problem-solving.&nbsp;AI holds the potential to address some of the most pressing issues in nearly every field,” says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and Henry Ellis Warren Professor of Electrical Engineering and Computer Science. “I’m honored that the college can contribute to and support such a vital program that will equip our nation’s naval officers with the technical expertise they need for mission-relevant challenges.”</p><p>MIT has been a leading center of ship research and design for over a century, with work at the Institute today representing significant advancements in fluid mechanics and hydrodynamics, acoustics, offshore mechanics, marine robotics and sensors, and ocean sensing and forecasting. The <a href=\"https://2n.mit.edu/\">2N program</a> will celebrate its 125th&nbsp;year at MIT in 2026.</p><p>“In MechE, we are embracing the use of AI to explore new frontiers in research and education, with deep grounding in the fundamentals, design, and scaling of physical systems,” says John Hart, the Class of 1922 Professor and head of MechE. “With the 2N6 program, we’re proud to be at the helm of such an important charge in training the next generation of leaders for the Navy.”</p><p>“Breakthroughs in artificial intelligence are reshaping society and advancing human decision-making and creativity,” says Asu Ozdaglar, deputy dean of the MIT Schwarzman College of Computing, head of&nbsp;EECS, and MathWorks Professor. “We are delighted to partner with the Department of Mechanical Engineering in launching this important collaboration with the U.S. Navy. The program will explore not only the forefront of AI advances, but also its effective application in Navy operations.”</p><p>2N6 was created following a visit to campus from Admiral Samuel Paparo, commander of the U.S. Indo-Pacific Command, with MIT Provost Anantha Chandrakasan, who was then dean of engineering and chief innovation and strategy officer.</p><p>“[Admiral Paparo] was given an overview of some of the cutting-edge work and research that MIT has done and is doing in the field of AI, [and was introduced to the 2N program],” says MacLean. “The admiral made the connection, envisioning an applied AI program similar to 2N.”</p><p>2N6 will run as a pilot program for at least two years. The program’s first cohort will comprise only U.S. Navy officers, with plans to expand more broadly.</p><p>“We are thrilled to build on the long-standing relationship between MIT and the U.S. Navy with this new program,” says Themis Sapsis, William I. Koch Professor in mechanical engineering and the director of the Center for Ocean Engineering at MIT. “It is specifically designed to train naval officers on the fundamentals and applications of AI, but also involve them in research that has direct impact to the Navy. We believe that 2N6 can model a new paradigm for advanced AI education focused more&nbsp;broadly on supporting national security.”</p><p>“The launch of 2N6 builds on more than 125 years of excellence in naval construction and marine engineering at MIT. By pairing our long-standing Course 2N program with advanced training in artificial intelligence, we’re preparing naval officers to navigate an increasingly complex technological landscape,” adds Maria C. Yang, interim dean of engineering and William E. Leonhard (1940) Professor of Mechanical Engineering. \"This integrated approach ensures the next generation of naval leaders can develop innovative solutions and to advance technologies essential to our national security.”</p>",
      "author": "Anne Wilson | Department of Mechanical Engineering",
      "publishedAt": "2025-12-12T18:10:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "7036dee6-ba6e-4d9c-a3dc-92712a51ec70",
      "guid": "https://news.mit.edu/2025/new-method-improves-reliability-statistical-estimations-1212",
      "title": "New method improves the reliability of statistical estimations",
      "link": "https://news.mit.edu/2025/new-method-improves-reliability-statistical-estimations-1212",
      "content": "<p>Let’s say an environmental scientist is studying whether exposure to air pollution is associated with lower birth weights in a particular county.</p><p>They might train a machine-learning model to estimate the magnitude of this association, since machine-learning methods are especially good at learning complex relationships.</p><p>Standard machine-learning methods excel at making predictions and sometimes provide uncertainties, like confidence intervals, for these predictions. However, they generally don’t provide estimates or confidence intervals when determining whether two variables are related. Other methods have been developed specifically to address this association problem and provide confidence intervals. But, in spatial settings, MIT researchers found these confidence intervals can be completely off the mark.</p><p>When variables like air pollution levels or precipitation change across different locations, common methods for generating confidence intervals may claim a high level of confidence when, in fact, the estimation completely failed to capture the actual value. These faulty confidence intervals can mislead the user into trusting a model that failed.</p><p>After identifying this shortfall, the researchers developed a new method designed to generate valid confidence intervals for problems involving data that vary across space. In simulations and experiments with real data, their method was the only technique that consistently generated accurate confidence intervals.</p><p>This work could help researchers in fields like environmental science, economics, and epidemiology better understand when to trust the results of certain experiments.</p><p>“There are so many problems where people are interested in understanding phenomena over space, like weather or forest management. We’ve shown that, for this broad class of problems, there are more appropriate methods that can get us better performance, a better understanding of what is going on, and results that are more trustworthy,” says Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS), a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society, an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and senior author of this <a href=\"https://arxiv.org/pdf/2502.06067\" target=\"_blank\">study</a>.</p><p>Broderick is joined on the paper by co-lead authors David R. Burt, a postdoc, and Renato Berlinghieri, an EECS graduate student; and Stephen Bates an assistant professor in EECS and member of LIDS. The research was recently presented at the Conference on Neural Information Processing Systems.</p><p><strong>Invalid assumptions</strong></p><p>Spatial association involves studying how a variable and a certain outcome are related over a geographic area. For instance, one might want to study how tree cover in the United States relates to elevation.</p><p>To solve this type of problem, a scientist could gather observational data from many locations and use it to estimate the association at a different location where they do not have data.</p><p>The MIT researchers realized that, in this case, existing methods often generate confidence intervals that are completely wrong. A model might say it is 95 percent confident its estimation captures the true relationship between tree cover and elevation, when it didn’t capture that relationship at all.</p><p>After exploring this problem, the researchers determined that the assumptions these confidence interval methods rely on don’t hold up when data vary spatially.</p><p>Assumptions are like rules that must be followed to ensure results of a statistical analysis are valid. Common methods for generating confidence intervals operate under various assumptions.</p><p>First, they assume that the source data, which is the observational data one gathered to train the model, is independent and identically distributed. This assumption implies that the chance of including one location in the data has no bearing on whether another is included. But, for example, U.S. Environmental Protection Agency (EPA) air sensors are placed with other air sensor locations in mind.</p><p>Second, existing methods often assume that the model is perfectly correct, but this assumption is never true in practice. Finally, they assume the source data are similar to the target data where one wants to estimate.</p><p>But in spatial settings, the source data can be fundamentally different from the target data because the target data are in a different location than where the source data were gathered.</p><p>For instance, a scientist might use data from EPA pollution monitors to train a machine-learning model that can predict health outcomes in a rural area where there are no monitors. But the EPA pollution monitors are likely placed in urban areas, where there is more traffic and heavy industry, so the air quality data will be much different than the air quality data in the rural area.</p><p>In this case, estimates of association using the urban data suffer from bias because the target data are systematically different from the source data.</p><p><strong>A smooth solution</strong></p><p>The new method for generating confidence intervals explicitly accounts for this potential bias.</p><p>Instead of assuming the source and target data are similar, the researchers assume the data vary smoothly over space.</p><p>For instance, with fine particulate air pollution, one wouldn’t expect the pollution level on one city block to be starkly different than the pollution level on the next city block. Instead, pollution levels would smoothly taper off as one moves away from a pollution source.</p><p>“For these types of problems, this spatial smoothness assumption is more appropriate. It is a better match for what is actually going on in the data,” Broderick says.</p><p>When they compared their method to other common techniques, they found it was the only one that could consistently produce reliable confidence intervals for spatial analyses. In addition, their method remains reliable even when the observational data are distorted by random errors.</p><p>In the future, the researchers want to apply this analysis to different types of variables and explore other applications where it could provide more reliable results.</p><p>This research was funded, in part, by an MIT Social and Ethical Responsibilities of Computing (SERC) seed grant, the Office of Naval Research, Generali, Microsoft, and the National Science Foundation (NSF).</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-12-12T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "d9ed1cd0-e086-4fee-b99c-bf4104563564",
      "guid": "https://news.mit.edu/2025/new-materials-could-boost-energy-efficiency-microelectronics-1211",
      "title": "New materials could boost the energy efficiency of microelectronics",
      "link": "https://news.mit.edu/2025/new-materials-could-boost-energy-efficiency-microelectronics-1211",
      "content": "<p>MIT researchers have developed a new fabrication method that could enable the production of more energy efficient electronics by stacking multiple functional components on top of one existing circuit.</p><p>In traditional circuits, logic devices that perform computation, like transistors, and memory devices that store data are built as separate components, forcing data to travel back and forth between them, which wastes energy.</p><p>This new electronics integration platform allows scientists to fabricate transistors and memory devices in one compact stack on a semiconductor chip. This eliminates much of that wasted energy while boosting the speed of computation.</p><p>Key to this advance is a newly developed material with unique properties and a more precise fabrication approach that reduces the number of defects in the material. This allows the researchers to make extremely tiny transistors with built-in memory that can perform faster than state-of-the-art devices while consuming less electricity than similar transistors.</p><p>By improving the energy efficiency of electronic devices, this new approach could help reduce the burgeoning electricity consumption of computation, especially for demanding applications like generative AI, deep learning, and computer vision tasks.</p><p>“We have to minimize the amount of energy we use for AI and other data-centric computation in the future because it is simply not sustainable. We will need new technology like this integration platform to continue that progress,” says Yanjie Shao, an MIT postdoc and lead author of two papers on these new transistors.</p><p>The new technique is described in <a href=\"https://mtlsites.mit.edu/users/alamo/pdf/2025/paper%201.pdf\" target=\"_blank\">two</a> <a href=\"https://mtlsites.mit.edu/users/alamo/pdf/2025/paper%202.pdf\" target=\"_blank\">papers</a> (one invited) that were presented at the IEEE International Electron Devices Meeting. Shao is joined on the papers by senior authors Jesús del Alamo, the Donner Professor of Engineering in the MIT Department of Electrical Engineering and Computer Science (EECS); Dimitri Antoniadis, the Ray and Maria Stata Professor of Electrical Engineering and Computer Science at MIT; as well as others at MIT, the University of Waterloo, and Samsung Electronics.</p><p><strong>Flipping the problem</strong></p><p>Standard CMOS (complementary metal-oxide semiconductor) chips traditionally have a front end, where the active components like transistors and capacitors are fabricated, and a back end that includes wires called interconnects and other metal bonds that connect components of the chip.</p><p>But some energy is lost when data travel between these bonds, and slight misalignments can hamper performance. Stacking active components would reduce the distance data must travel and improve a chip’s energy efficiency.</p><p>Typically, it is difficult to stack silicon transistors on a CMOS chip because the high temperature required to fabricate additional devices on the front end would destroy the existing transistors underneath.</p><p>The MIT researchers turned this problem on its head, developing an integration technique to stack active components on the back end of the chip instead.</p><p>“If we can use this back-end platform to put in additional active layers of transistors, not just interconnects, that would make the integration density of the chip much higher and improve its energy efficiency,” Shao explains.</p><p>The researchers accomplished this using a new material, amorphous indium oxide, as the active channel layer of their back-end transistor. The active channel layer is where the transistor’s essential functions take place.</p><p>Due to the unique properties of indium oxide, they can “grow” an extremely thin layer of this material at a temperature of only about 150 degrees Celsius on the back end of an existing circuit without damaging the device on the front end.</p><p><strong>Perfecting the process</strong></p><p>They carefully optimized the fabrication process, which minimizes the number of defects in a layer of indium oxide material that is only about 2 nanometers thick.</p><p>A few defects, known as oxygen vacancies, are necessary for the transistor to switch on, but with too many defects it won’t work properly. This optimized fabrication process allows the researchers to produce an extremely tiny transistor that operates rapidly and cleanly, eliminating much of the additional energy required to switch a transistor between off and on.</p><p>Building on this approach, they also fabricated back-end transistors with integrated memory that are only about 20 nanometers in size. To do this, they added a layer of material called ferroelectric hafnium-zirconium-oxide as the memory component.</p><p>These compact memory transistors demonstrated switching speeds of only 10 nanoseconds, hitting the limit of the team’s measurement instruments. This switching also requires much lower voltage than similar devices, reducing electricity consumption.</p><p>And because the memory transistors are so tiny, the researchers can use them as a platform to study the fundamental physics of individual units of ferroelectric hafnium-zirconium-oxide.</p><p>“If we can better understand the physics, we can use this material for many new applications. The energy it uses is very minimal, and it gives us a lot of flexibility in how we can design devices. It really could open up many new avenues for the future,” Shao says.</p><p>The researchers also worked with a team at the University of Waterloo to develop a model of the performance of the back-end transistors, which is an important step before the devices can be integrated into larger circuits and electronic systems.</p><p>In the future, they want to build upon these demonstrations by integrating back-end memory transistors onto a single circuit. They also want to enhance the performance of the transistors and study how to more finely control the properties of ferroelectric hafnium-zirconium-oxide.</p><p>“Now, we can build a platform of versatile electronics on the back end of a chip that enable us to achieve high energy efficiency and many different functionalities in very small devices. We have a good device architecture and material to work with, but we need to keep innovating to uncover the ultimate performance limits,” Shao says.</p><p>This work is supported, in part, by Semiconductor Research Corporation (SRC) and Intel. Fabrication was carried out at the MIT Microsystems Technology Laboratories and MIT.nano facilities.&nbsp;</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-12-11T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "c59046ca-3b35-4d1d-9200-5459f6914407",
      "guid": "https://news.mit.edu/2025/mit-affiliates-named-schmidt-sciences-ai2050-fellows-1208",
      "title": "MIT affiliates named 2025 Schmidt Sciences AI2050 Fellows",
      "link": "https://news.mit.edu/2025/mit-affiliates-named-schmidt-sciences-ai2050-fellows-1208",
      "content": "<p dir=\"ltr\">Two current MIT affiliates and seven additional alumni are among those named to the 2025 cohort of&nbsp;<a href=\"https://www.schmidtsciences.org/2025-ai2050-fellows-announcement/\">AI2050 Fellows</a>. &nbsp;</p><p dir=\"ltr\"><a href=\"https://ai2050.schmidtsciences.org/fellow/zongyi-li/\" target=\"_blank\" title=\"https://ai2050.schmidtsciences.org/fellow/zongyi-li/\" data-outlook-id=\"33c2b06d-c669-4d13-9ea4-6fa32154457e\">Zongyi Li</a>, a postdoc in the MIT Computer Science and Artificial Intelligence Lab, and <a href=\"https://ai2050.schmidtsciences.org/fellow/tess-smidt/\" target=\"_blank\">Tess Smidt</a> ’12, an associate professor of electrical engineering and computer science (EECS), were both named as AI2050 Early Career Fellows.&nbsp;</p><p dir=\"ltr\">Seven additional MIT alumni were also honored. AI2050 Early Career Fellows include Brian Hie SM '19, PhD '21; Natasha Mary Jaques PhD '20; Martin Anton Schrimpf PhD '22; Lindsey Raymond SM '19, PhD '24, who will join the MIT faculty in EECS, the Department of Economics, and the MIT Schwarzman College of Computing in 2026; and Ellen Dee Zhong PhD ’22. AI2050 Senior Fellows include Surya Ganguli ’98, MNG ’98; and Luke Zettlemoyer SM ’03, PhD ’09.&nbsp;</p><p dir=\"ltr\">AI2050 Fellows are announced annually by Schmidt Sciences, a nonprofit organization founded in 2024 by Eric and Wendy Schmidt that works to accelerate scientific knowledge and breakthroughs with the most promising, advanced tools to support a thriving planet. The organization prioritizes research in areas poised for impact including AI and advanced computing, astrophysics, biosciences, climate, and space — as well as supporting researchers in a variety of disciplines through its science systems program.&nbsp;</p><p dir=\"ltr\">Li is postdoc in CSAIL working with associate professor of EECS Kaiming He. Li's research focuses on developing neural operator methods to accelerate scientific computing. He received his PhD in computing and mathematical sciences from Caltech, where he was advised by Anima Anandkumar and Andrew Stuart. He holds undergraduate degrees in computer science and mathematics from Washington University in St. Louis.&nbsp;</p><p dir=\"ltr\">Li's work has been supported by a Kortschak Scholarship, PIMCO Fellowship, Amazon AI4Science Fellowship, Nvidia Fellowship, and MIT-Novo Nordisk AI Fellowship. He has also completed three summer internships at Nvidia. Li will join the NYU Courant Institute of Mathematical Sciences as an assistant professor of mathematics and data science in fall 2026.</p><p dir=\"ltr\">Smidt, associate professor of electrical engineering and computer science (EECS), is the principal investigator of the Atomic Architects group at the Research Laboratory of Electronics (RLE), where she works at the intersection of physics, geometry, and machine learning to design algorithms that aid in the understanding of physical systems under physical and geometric constraints, with applications to the design both of new materials and new molecules. She has a particular focus on symmetries present in 3D physical systems, such as rotation, translation, and reflection.</p><p dir=\"ltr\">Smidt earned her BS in physics from MIT in 2012 and her PhD in physics from the University of California at Berkeley in 2018. Prior to joining the MIT EECS faculty in 2021, she was the 2018 Alvarez Postdoctoral Fellow in Computing Sciences at Lawrence Berkeley National Laboratory, and a software engineering intern on the Google Accelerated Sciences team, where she developed Euclidean symmetry equivariant neural networks that naturally handle 3D geometry and geometric tensor data. Besides the AI2050 fellowship, she has received an Air Force Office of Scientific Research Young Investigator Program award, the EECS Outstanding Educator Award, and a Transformative Research Fund award.</p><p>Conceived and co-chaired by Eric Schmidt and James Manyika, AI2050 is a philanthropic initiative aimed at helping to solve&nbsp;<a href=\"https://ai2050.schmidtsciences.org/hard-problems/\">hard problems in AI</a>. Within their research, each fellow will contend with the central motivating question of AI2050: “It’s 2050. AI has turned out to be hugely beneficial to society. What happened? What are the most important problems we solved and the opportunities and possibilities we realized to ensure this outcome?”&nbsp;</p>",
      "author": "Jane Halpern | Department of Electrical Engineering and Computer Science",
      "publishedAt": "2025-12-08T20:15:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "b9b58c2c-43ec-4d9f-a26a-e1f4e668374a",
      "guid": "https://news.mit.edu/2025/mit-researchers-speak-objects-existence-using-ai-robotics-1205",
      "title": "MIT researchers “speak objects into existence” using AI and robotics",
      "link": "https://news.mit.edu/2025/mit-researchers-speak-objects-existence-using-ai-robotics-1205",
      "content": "<p>Generative AI and robotics are moving us ever closer to the day when we can ask for an object and have it created within a few minutes. In fact, MIT researchers have developed a speech-to-reality system, an AI-driven workflow that allows them to provide input to a robotic arm and “speak objects into existence,” creating things like furniture in as little as five minutes.&nbsp;&nbsp;</p><p>With the speech-to-reality system, a robotic arm mounted on a table is able&nbsp;to receive spoken input from a human, such as “I want a simple stool,” and then construct the objects out of modular components. To date, the researchers have used the system to create stools, shelves, chairs, a small table, and even decorative items such as a dog statue.</p><p>“We’re connecting natural language processing, 3D generative AI, and robotic assembly,” says Alexander Htet Kyaw, an MIT graduate student and Morningside Academy for Design (MAD) fellow. “These are rapidly advancing areas of research that haven’t been brought together before in a way that you can actually make physical objects just from a simple speech prompt.”&nbsp;&nbsp;</p><p>The idea started when Kyaw — a graduate student in the departments of Architecture and Electrical Engineering and Computer Science — took Professor Neil Gershenfeld’s course, “How to Make Almost Anything.” In that class, he built the&nbsp;speech-to-reality system. He continued working on the project at the MIT Center for Bits and Atoms (CBA), directed by Gershenfeld, collaborating with graduate students Se Hwan Jeon of the Department of Mechanical Engineering and Miana Smith of CBA.</p><p>The speech-to-reality system begins with speech recognition that processes the user’s request using a&nbsp;large language model, followed by 3D generative AI that creates a digital mesh representation of the object, and a voxelization algorithm that breaks down the 3D mesh into assembly components.</p><p>After that, geometric processing modifies the AI-generated assembly to account for fabrication and physical constraints associated with the real world, such as the number of components, overhangs, and connectivity of the geometry. This is followed by creation of a feasible assembly sequence and automated path planning&nbsp;for the robotic arm to assemble physical objects from user prompts.</p><p>By leveraging natural language, the system makes design and manufacturing more accessible to people without expertise in 3D modeling or robotic programming.&nbsp;And, unlike&nbsp;3D printing, which can take hours or days, this system builds within minutes.</p><p>“This project is an interface between humans, AI, and robots to co-create the world around us,” Kyaw says.&nbsp;“Imagine a scenario where you&nbsp;say ‘I want a chair,’&nbsp;and within five minutes a physical chair materializes in front of you.”</p><p>The team has immediate plans to improve the weight-bearing capability of the furniture by changing the means of connecting the cubes from magnets to more robust connections.&nbsp;</p><p>“We’ve also developed pipelines for converting voxel structures into feasible assembly sequences for small, distributed mobile robots, which could help translate this work to structures at any size scale,” Smith says.</p><p>The&nbsp;purpose of using modular components is to eliminate the waste that goes into making physical objects by disassembling and then reassembling them into something different, for instance turning a sofa into a bed when you no longer need the sofa.</p><p>Because Kyaw also has experience&nbsp;using <a href=\"https://link.springer.com/article/10.1007/s44223-024-00053-4\">gesture recognition and augmented reality to interact</a> with robots in the fabrication process, he is currently working on incorporating both speech and gestural control into the speech-to-reality system.</p><p>Leaning into his memories of&nbsp;the replicator in the “Star Trek” franchise and the robots in the&nbsp;animated film “Big Hero 6,” Kyaw explains his vision.</p><p>“I want to increase access for people to make physical objects in a fast, accessible, and sustainable manner,” he says. “I’m working toward a future where the very essence of matter is truly in your control. One&nbsp;where reality can be generated on demand.”</p><p>The team presented their paper&nbsp;<a href=\"https://doi.org/10.1145/3745778.3766670\">“Speech to Reality: On-Demand Production using Natural Language, 3D Generative AI, and Discrete Robotic Assembly”</a> at the Association for Computing Machinery (ACM) Symposium on Computational Fabrication (SCF ’25) held at MIT on Nov. 21.&nbsp;</p>",
      "author": "Denise Brehm | MIT Morningside Academy for Design",
      "publishedAt": "2025-12-05T15:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "e6e856a8-1af0-44e3-9e1a-94765db6787e",
      "guid": "https://news.mit.edu/2025/robots-spare-warehouse-workers-heavy-lifting-1205",
      "title": "Robots that spare warehouse workers the heavy lifting",
      "link": "https://news.mit.edu/2025/robots-spare-warehouse-workers-heavy-lifting-1205",
      "content": "<p>There are some jobs human bodies just weren’t meant to do. Unloading trucks and shipping containers is a repetitive, grueling task — and a big reason warehouse injury rates are more than twice the national average.</p><p>The Pickle Robot Company wants its machines to do the heavy lifting. The company’s one-armed robots autonomously unload trailers, picking up boxes weighing up to 50 pounds and placing them onto onboard conveyor belts for warehouses of all types.</p><p>The company name, an homage to The Apple Computer Company, hints at the ambitions of founders AJ Meyer ’09, Ariana Eisenstein ’15, SM ’16, and Dan Paluska ’97, SM ’00. The founders want to make the company the technology leader for supply chain automation.</p><p>The company’s unloading robots combine generative AI and machine-learning algorithms with sensors, cameras, and machine-vision software to navigate new environments on day one and improve performance over time. Much of the company’s hardware is adapted from industrial partners. You may recognize the arm, for instance, from car manufacturing lines — though you may not have seen it in bright pickle-green.</p><p>The company is already working with customers&nbsp;like UPS, Ryobi Tools, and Yusen Logistics to take a load off warehouse workers, freeing them to solve other supply chain bottlenecks in the process.</p><p>“Humans are really good edge-case problem solvers, and robots are not,” Paluska says. “How can the robot, which is really good at the brute force, repetitive tasks, interact with humans to solve more problems? Human bodies and minds are so adaptable, the way we sense and respond to the environment is so adaptable, and robots aren’t going to replace that anytime soon. But there’s so much drudgery we can get rid of.”</p><p><strong>Finding problems for robots</strong></p><p>Meyer and Eisenstein majored in computer science and electrical engineering at MIT, but they didn’t work together until after graduation, when Meyer started the technology consultancy Leaf Labs, which specializes in building embedded computer systems for things like robots, cars, and satellites.</p><p>“A bunch of friends from MIT ran that shop,” Meyer recalls, noting it’s still running today. “Ari worked there, Dan consulted there, and we worked on some big projects. We were the primary software and digital design team behind Project Ara, a smartphone for Google, and we worked on a bunch of interesting government projects. It was really a lifestyle company for MIT kids. But 10 years go by, and we thought, ‘We didn’t get into this to do consulting. We got into this to do robots.’”</p><p>When Meyer graduated in 2009, problems like robot dexterity seemed insurmountable. By 2018, the rise of algorithmic approaches like neural networks had brought huge advances to robotic manipulation and navigation.</p><p>To figure out what problem to solve with robots, the founders talked to people in industries as diverse as agriculture, food prep, and hospitality. At some point, they started visiting logistics warehouses, bringing a stopwatch to see how long it took workers to complete different tasks.</p><p>“In 2018, we went to a UPS warehouse and watched 15 guys unloading trucks during a winter night shift,” Meyer recalls. “We spoke to everyone, and not a single person had worked there for more than 90 days. We asked, ‘Why not?’ They laughed at us. They said, ‘Have you tried to do this job before?’”</p><p>It turns out warehouse turnover is one of the industry’s biggest problems, limiting productivity as managers constantly grapple with hiring, onboarding, and training.</p><p>The founders raised a seed funding round and built robots that could sort boxes because it was an easier problem that allowed them to work with technology like grippers and barcode scanners. Their robots eventually worked, but the company wasn’t growing fast enough to be profitable. Worse yet, the founders were having trouble raising money.</p><p>“We were desperately low on funds,” Meyer recalls. “So we thought, ‘Why spend our last dollar on a warm-up task?’”</p><p>With money dwindling, the founders built a proof-of-concept robot that could unload trucks reliably for about 20 seconds at a time and posted a video of it on YouTube. Hundreds of potential customers reached out. The interest was enough to get investors back on board to keep the company alive.</p><p>The company piloted its first unloading system for a year with a customer in the desert of California, sparing human workers from unloading shipping containers that can reach temperatures up to&nbsp;130 degrees in the summer. It has since scaled deployments with multiple customers and gained traction among third-party logistics centers across the U.S.</p><p>The company’s robotic arm is made by the German&nbsp;industrial robotics&nbsp;giant KUKA. The robots are mounted on a custom mobile base with an onboard computing systems&nbsp;so they can navigate to docks and adjust their positions inside trailers autonomously while lifting. The end of each arm features a suction gripper that clings to packages and moves them to the onboard conveyor belt.</p><p>The company’s robots can pick up boxes ranging in size from 5-inch cubes to 24-by-30 inch boxes. The robots can unload anywhere from 400 to 1,500 cases per hour depending on size and weight. The company fine tunes pre-trained generative AI models and uses a number of smaller models to ensure the robot runs smoothly in every setting.</p><p>The company is also developing a&nbsp;software platform it can integrate with third-party hardware, from humanoid robots to autonomous forklifts.</p><p>“Our immediate product roadmap is load and unload,” Meyer says. “But we’re also hoping to connect these third-party platforms. Other companies are also trying to connect robots. What does it mean for the robot unloading a truck to talk to the robot palletizing, or for the forklift to talk to the inventory drone? Can they do the job faster? I think there’s a big network coming in which we need to orchestrate the robots and the automation across the entire supply chain, from the mines to the factories to your front door.”</p><p><strong>“Why not us?”</strong></p><p>The Pickle Robot Company employs about 130 people in its office in Charlestown, Massachusetts, where a standard — if green — office gives way to a warehouse where its robots can be seen loading boxes onto conveyor belts alongside human workers and manufacturing lines.</p><p>This summer, Pickle will be ramping up production of a new version of its system, with further plans to begin designing a two-armed robot sometime after that.</p><p>“My supervisor at Leaf Labs once told me ‘No one knows what they’re doing, so why not us?’” Eisenstein says. “I carry that with me all the time. I’ve been very lucky to be able to work with so many talented, experienced people in my career. They all bring their own skill sets and understanding. That’s a massive opportunity — and it’s the only way something as hard as what we’re doing is going to work.”</p><p>Moving forward, the company sees many other robot-shaped problems for its machines.</p><p>“We didn’t start out by saying, ‘Let’s load and unload a truck,’” Meyers says. “We said, ‘What does it take to make a great robot business?’ Unloading trucks is the first chapter. Now we’ve built a platform to make the next robot that helps with more jobs, starting in logistics but then ultimately in manufacturing, retail, and hopefully the entire supply chain.”</p>",
      "author": "Zach Winn | MIT News",
      "publishedAt": "2025-12-05T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "2dc38135-5849-4268-a541-49bf84664c56",
      "guid": "https://news.mit.edu/2025/smarter-way-large-language-models-think-about-hard-problems-1204",
      "title": "A smarter way for large language models to think about hard problems",
      "link": "https://news.mit.edu/2025/smarter-way-large-language-models-think-about-hard-problems-1204",
      "content": "<p>To make large language models (LLMs) more accurate when answering harder questions, researchers can let the model spend more time thinking about potential solutions.</p><p>But common&nbsp;approaches that give LLMs this capability set a fixed computational budget for every problem, regardless of how complex it is. This means the LLM might waste computational resources on simpler questions or be unable to tackle intricate problems that require more reasoning.</p><p>To address this, MIT researchers developed a smarter way to allocate computational effort as the LLM solves a problem. Their method enables the model to dynamically adjust its computational budget based on the difficulty of the question and the likelihood that each partial solution will lead to the correct answer.</p><p>The researchers found that their new approach enabled LLMs to use as little as one-half the computation as existing methods, while&nbsp;achieving&nbsp;comparable accuracy on a range of questions with varying difficulties. In addition, their method allows smaller, less resource-intensive LLMs to perform as well as or even better than larger models on complex problems.</p><p>By improving the reliability and efficiency of LLMs, especially when they tackle complex reasoning tasks, this technique could reduce the energy consumption of generative AI systems and enable the use of LLMs in more high-stakes and time-sensitive applications.</p><p>“The computational cost of inference has quickly become a major bottleneck for frontier model providers, and they are actively trying to find ways to improve computational efficiency per&nbsp;user queries. For instance, the recent GPT-5.1 release highlights the efficacy of the ‘adaptive reasoning’ approach our paper proposes. By endowing the models with the ability to know what they don’t know, we can enable them to spend more compute on the hardest problems and most promising solution paths, and use far fewer tokens on easy ones. That makes reasoning both more reliable and far more efficient,” says Navid Azizan, the Alfred H. and Jean M. Hayes Career Development Assistant Professor in the Department of Mechanical Engineering and the Institute for Data, Systems, and Society (IDSS), a principal investigator of the Laboratory for Information and Decision Systems (LIDS), and the&nbsp;senior author of a <a href=\"https://www.arxiv.org/pdf/2506.09338\" target=\"_blank\">paper on this technique</a>.</p><p>Azizan is joined on the paper by lead author Young-Jin Park, a LIDS/MechE graduate student; Kristjan Greenewald, a research scientist in the MIT-IBM Watson AI Lab; Kaveh Alim, an IDSS graduate student; and Hao Wang, a research scientist at the MIT-IBM Watson AI Lab and the Red Hat AI Innovation Team. The research is being presented this week at the Conference on Neural Information Processing Systems.</p><p><strong>Computation for contemplation</strong></p><p>A recent approach called inference-time scaling lets a large language model take more time to reason about difficult problems.</p><p>Using inference-time scaling, the LLM might generate multiple solution attempts at once or explore different reasoning paths, then choose the best ones to pursue from those candidates.</p><p>A separate model, known as a process reward model (PRM), scores each potential solution or reasoning path. The LLM uses these scores to identify the most promising ones.&nbsp; &nbsp; &nbsp;</p><p>Typical inference-time scaling approaches assign a fixed amount of computation for the LLM to break the problem down and reason about the steps.</p><p>Instead, the researchers’ method, known as instance-adaptive scaling, dynamically adjusts the number of potential solutions or reasoning steps based on how likely they are to succeed, as the model wrestles with the problem.</p><p>“This is how humans solve problems. We come up with some partial solutions and then decide, should I go further with any of these, or stop and revise, or even go back to my previous step and continue solving the problem from there?” Wang explains.</p><p>To do this, the framework uses the PRM to estimate the difficulty of the question, helping the LLM assess how much computational budget to utilize for generating and reasoning about potential solutions.</p><p>At every step in the model’s reasoning process, the PRM looks at the question and partial answers and evaluates how promising each one is for getting to the right solution. If the LLM is more confident, it can reduce the number of potential solutions or reasoning trajectories to pursue, saving computational resources.</p><p>But the researchers found that existing PRMs often overestimate the model’s probability of success.</p><p><strong>Overcoming overconfidence</strong></p><p>“If we were to just trust current PRMs, which often overestimate the chance of success, our system would reduce the computational budget too aggressively. So we first had to find a way to better calibrate PRMs to make inference-time scaling more efficient and reliable,” Park says.</p><p>The researchers introduced a calibration method that enables PRMs to generate a range of probability scores rather than a single value. In this way, the PRM creates more reliable uncertainty estimates that better reflect the true probability of success.</p><p>With a well-calibrated PRM, their instance-adaptive scaling framework can use the probability scores to effectively reduce computation while maintaining the accuracy of the model’s outputs.</p><p>When they compared their method to standard inference-time scaling approaches on a series of mathematical reasoning tasks, it utilized less computation to solve each problem while achieving similar&nbsp;accuracy.</p><p>“The beauty of our approach is that this adaptation happens on the fly, as the problem is being solved, rather than happening all at once at the beginning of the process,” says Greenewald.</p><p>In the future, the researchers are interested in applying this technique to other applications, such as code generation and AI agents. They are also planning to explore additional uses for their PRM calibration method, like for&nbsp;reinforcement learning and fine-tuning.</p><p>“Human employees learn on the job — some CEOs even started as interns — but today’s agents remain largely static pieces of probabilistic software. Work like this paper is an important step toward changing that: helping agents understand what they don’t know and building mechanisms for continual self-improvement. These capabilities are essential if we want agents that can operate safely, adapt to new situations, and deliver consistent results at scale,” says Akash Srivastava, director and chief architect of Core AI at IBM Software, who was not involved with this work.</p><p>This work was funded, in part, by the MIT-IBM Watson AI Lab, the MIT-Amazon Science Hub, the MIT-Google Program for Computing Innovation, and MathWorks.&nbsp;</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-12-04T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "d0c25ae7-e90d-4e5c-b753-7bbec338284a",
      "guid": "https://news.mit.edu/2025/mit-engineers-design-aerial-microrobot-fly-like-bumblebee-1203",
      "title": "MIT engineers design an aerial microrobot that can fly as fast as a bumblebee",
      "link": "https://news.mit.edu/2025/mit-engineers-design-aerial-microrobot-fly-like-bumblebee-1203",
      "content": "<p>In the future, tiny flying robots could be deployed to aid in the search for survivors trapped beneath the rubble after a devastating earthquake. Like real insects, these robots could flit through tight spaces larger robots can’t reach, while simultaneously dodging stationary obstacles and pieces of falling rubble.</p><p>So far, aerial microrobots have only been able to fly slowly along smooth trajectories, far from the swift, agile flight of real insects — until now.</p><p>MIT researchers have demonstrated aerial microrobots that can fly with speed and agility that is comparable to their biological counterparts. A collaborative team designed a new AI-based controller for the robotic bug that enabled it to follow gymnastic flight paths, such as executing continuous body flips.</p><p>With a two-part control scheme that combines high performance with computational efficiency, the robot’s speed and acceleration increased by about 450 percent and 250 percent, respectively, compared to the researchers’ best previous demonstrations.</p><p>The speedy robot was agile enough to complete 10 consecutive somersaults in 11 seconds, even when wind disturbances threatened to push it off course.</p><img src=\"/sites/default/files/images/inline/MIT-Microrobot-demo-05-press.gif\" data-align=\"center\" data-entity-uuid=\"8534cbb2-d9d5-4e25-af03-746acfb68251\" data-entity-type=\"file\" alt=\"Animation of a flying, flipping microrobot\" width=\"600\" height=\"338\" data-caption=\"A microrobot flips 10 times in 11 seconds.&lt;br&gt;&lt;br&gt;Credit: Courtesy of the&amp;nbsp;Soft and Micro Robotics Laboratory\"><p>“We want to be able to use these robots in scenarios that more traditional quad copter robots would have trouble flying into, but that insects could navigate. Now, with our bioinspired control framework, the flight performance of our robot is comparable to insects in terms of speed, acceleration, and the pitching angle. This is quite an exciting step toward that future goal,” says Kevin Chen, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), head of the Soft and Micro Robotics Laboratory within the Research Laboratory of Electronics (RLE), and co-senior author of a <a href=\"http://doi.org/10.1126/sciadv.aea8716\" target=\"_blank\">paper on the robot</a>.</p><p>Chen is joined on the paper by co-lead authors Yi-Hsuan Hsiao, an EECS MIT graduate student; Andrea Tagliabue PhD ’24; and Owen Matteson, a graduate student in the Department of Aeronautics and Astronautics (AeroAstro); as well as EECS graduate student Suhan Kim; Tong Zhao MEng ’23; and co-senior author Jonathan P. How, the Ford Professor of Engineering in the Department of Aeronautics and Astronautics and a principal investigator in the Laboratory for Information and Decision Systems (LIDS). The research appears today in <em>Science Advances</em>.</p><p><strong>An AI controller</strong></p><p>Chen’s group has been building robotic insects for more than five years.</p><p>They recently developed a&nbsp;<a href=\"https://news.mit.edu/2025/fast-agile-robotic-insect-could-someday-aid-mechanical-pollination-0115\" target=\"_blank\">more durable version of their tiny robot</a>, a microcassette-sized device that weighs less than a paperclip. The new version utilizes larger, flapping wings that enable more agile movements. They are powered by a set of squishy artificial muscles that flap the wings at an extremely fast rate.</p><p>But the controller — the “brain” of the robot that determines its position and tells it where to fly — was hand-tuned by a human, limiting the robot’s performance.</p><p>For the robot to fly quickly and aggressively like a real insect, it needed a more robust controller that could account for uncertainty and perform complex optimizations quickly.</p><p>Such a controller would be too computationally intensive to be deployed in real time, especially with the complicated aerodynamics of the lightweight robot.</p><p>To overcome this challenge, Chen’s group joined forces with How’s team and, together, they crafted a two-step, AI-driven control scheme that provides the robustness necessary for complex, rapid maneuvers, and the computational efficiency needed for real-time deployment.</p><p>“The hardware advances pushed the controller so there was more we could do on the software side, but at the same time, as the controller developed, there was more they could do with the hardware. As Kevin’s team demonstrates new capabilities, we demonstrate that we can utilize them,” How says.</p><p>For the first step, the team built what is known as a model-predictive controller. This type of powerful controller uses a dynamic, mathematical model to predict the behavior of the robot and plan the optimal series of actions to safely follow a trajectory.</p><p>While computationally intensive, it can plan challenging maneuvers like aerial somersaults, rapid turns, and aggressive body tilting. This high-performance planner is also designed to consider constraints on the force and torque the robot could apply, which is essential for avoiding collisions.</p><p>For instance, to perform multiple flips in a row, the robot would need to decelerate in such a way that its initial conditions are exactly right for doing the flip again.</p><p>“If small errors creep in, and you try to repeat that flip 10 times with those small errors, the robot will just crash. We need to have robust flight control,” How says.</p><p>They use this expert planner to train a “policy” based on a deep-learning model, to control the robot in real time, through a process called imitation learning. A policy is the robot’s decision-making engine, which tells the robot where and how to fly.</p><p>Essentially, the imitation-learning process compresses the powerful controller into a computationally efficient AI model that can run very fast.</p><p>The key was having a smart way to create just enough training data, which would teach the policy everything it needs to know for aggressive maneuvers.</p><p>“The robust training method is the secret sauce of this technique,” How explains.</p><p>The AI-driven policy takes robot positions as inputs and outputs control commands in real time, such as thrust force and torques.</p><p><strong>Insect-like performance</strong></p><p>In their experiments, this two-step approach enabled the insect-scale robot to fly 447 percent faster while exhibiting a 255 percent increase in acceleration. The robot was able to complete 10 somersaults in 11 seconds, and the tiny robot never strayed more than 4 or 5 centimeters off its planned trajectory.</p><p>“This work demonstrates that soft and microrobots, traditionally limited in speed, can now leverage advanced control algorithms to achieve agility approaching that of natural insects and larger robots, opening up new opportunities for multimodal locomotion,” says Hsiao.</p><p>The researchers were also able to demonstrate saccade movement, which occurs when insects pitch very aggressively, fly rapidly to a certain position, and then pitch the other way to stop. This rapid acceleration and deceleration help insects localize themselves and see clearly.</p><p>“This bio-mimicking flight behavior could help us in the future when we start putting cameras and sensors on board the robot,” Chen says.</p><p>Adding sensors and cameras so the microrobots can fly outdoors, without being attached to a complex motion capture system, will be a major area of future work.</p><p>The researchers also want to study how onboard sensors could help the robots avoid colliding with one another or coordinate navigation.</p><p>“For the micro-robotics community, I hope this paper signals a paradigm shift by showing that we can develop a new control architecture that is high-performing and efficient at the same time,” says Chen.</p><p>“This work is especially impressive because these robots still perform precise flips and fast turns despite the large uncertainties that come from relatively large fabrication tolerances in small-scale manufacturing, wind gusts of more than 1 meter per second, and even its power tether wrapping around the robot as it performs repeated flips,” says Sarah Bergbreiter, a professor of mechanical engineering at Carnegie Mellon University, who was not involved with this work.</p><p>“Although the controller currently runs on an external computer rather than onboard the robot, the authors demonstrate that similar, but less precise, control policies may be feasible even with the more limited computation available on an insect-scale robot. This is exciting because it points toward future insect-scale robots with agility approaching that of their biological counterparts,” she adds.</p><p>This research is funded, in part, by the National Science Foundation (NSF), the Office of Naval Research, Air Force Office of Scientific Research, MathWorks, and the Zakhartchenko Fellowship.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-12-03T19:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "44af9e06-2c79-4d7a-b07f-5d213fa27a77",
      "guid": "https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202",
      "title": "New control system teaches soft robots the art of staying safe",
      "link": "https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202",
      "content": "<p dir=\"ltr\" id=\"docs-internal-guid-8adfd4f6-7fff-d047-727d-cc029b78b3ad\">Imagine having a continuum soft robotic arm bend around a bunch of grapes or broccoli, adjusting its grip in real time as it lifts the object. Unlike traditional rigid robots that generally aim to avoid contact with the environment as much as possible and stay far away from humans for safety reasons, this arm senses subtle forces, stretching and flexing in ways that mimic more of the compliance of a human hand. Its every motion is calculated to avoid excessive force while achieving the task efficiently. In MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Laboratory for Information and Decisions Systems (LIDS) labs, these seemingly simple movements are the culmination of complex mathematics, careful engineering, and a vision for robots that can safely interact with humans and delicate objects.</p><p dir=\"ltr\">Soft robots, with their deformable bodies, promise a future where machines move more seamlessly alongside people, assist in caregiving, or handle delicate items in industrial settings. Yet that very flexibility makes them difficult to control. Small bends or twists can produce unpredictable forces, raising the risk of damage or injury. This motivates the need for safe control strategies for soft robots.&nbsp;</p><p dir=\"ltr\">“Inspired by advances in safe control and formal methods for rigid robots, we aim to adapt these ideas to soft robotics — modeling their complex behavior and embracing, rather than avoiding, contact — to enable higher-performance designs (e.g., greater payload and precision) without sacrificing safety or embodied intelligence,” says lead senior author and MIT Assistant Professor Gioele Zardini, who is a principal investigator in LIDS and the Department of Civil and Environmental Engineering, and an affiliate faculty with the Institute for Data, Systems, and Society (IDSS). “This vision is shared by recent and parallel work from other groups.”</p><p dir=\"ltr\"><strong>Safety first</strong></p><p dir=\"ltr\">The team developed a new framework that blends nonlinear control theory (controlling systems that involve highly complex dynamics) with advanced physical modeling techniques and efficient real-time optimization to produce what they call “contact-aware safety.” At the heart of the approach are high-order control barrier functions (HOCBFs) and high-order control Lyapunov functions (HOCLFs). HOCBFs define safe operating boundaries, ensuring the robot doesn’t exert unsafe forces. HOCLFs guide the robot efficiently toward its task objectives, balancing safety with performance.</p><p dir=\"ltr\">“Essentially, we’re teaching the robot to know its own limits when interacting with the environment while still achieving its goals,” says MIT Department of Mechanical Engineering PhD student Kiwan Wong, the lead author of a new paper describing the framework. “The approach involves some complex derivation of soft robot dynamics, contact models, and control constraints, but the specification of control objectives and safety barriers is rather straightforward for the practitioner, and the outcomes are very tangible, as you see the robot moving smoothly, reacting to contact, and never causing unsafe situations.”</p><p dir=\"ltr\">“Compared with traditional kinematic CBFs — where forward-invariant safe sets are hard to specify — the HOCBF framework simplifies barrier design, and its optimization formulation accounts for system dynamics (e.g., inertia), ensuring the soft robot stops early enough to avoid unsafe contact forces,” says Worcester Polytechnic Institute Assistant Professor and former CSAIL postdoc Wei Xiao.</p><p dir=\"ltr\">“Since soft robots emerged, the field has highlighted their embodied intelligence and greater inherent safety relative to rigid robots, thanks to passive material and structural compliance. Yet their “cognitive” intelligence — especially safety systems — has lagged behind that of rigid serial-link manipulators,” says co-lead author Maximilian Stölzle, a research intern at Disney Research and formerly a Delft University of Technology PhD student and visiting researcher at MIT LIDS and CSAIL. “This work helps close that gap by adapting proven algorithms to soft robots and tailoring them for safe contact and soft-continuum dynamics.”</p><p dir=\"ltr\">The LIDS and CSAIL team tested the system on a series of experiments designed to challenge the robot’s safety and adaptability. In one test, the arm pressed gently against a compliant surface, maintaining a precise force without overshooting. In another, it traced the contours of a curved object, adjusting its grip to avoid slippage. In yet another demonstration, the robot manipulated fragile items alongside a human operator, reacting in real time to unexpected nudges or shifts. “These experiments show that our framework is able to generalize to diverse tasks and objectives, and the robot can sense, adapt, and act in complex scenarios while always respecting clearly defined safety limits,” says Zardini.</p><p dir=\"ltr\">Soft robots with contact-aware safety could be a real value-add in high-stakes places, of course. In health care, they could assist in surgeries, providing precise manipulation while reducing risk to patients. In industry, they might handle fragile goods without constant supervision. In domestic settings, robots could help with chores or caregiving tasks, interacting safely with children or the elderly — a key step toward making soft robots reliable partners in real-world environments.&nbsp;</p><p dir=\"ltr\">“Soft robots have incredible potential,” says co-lead senior author Daniela Rus, director of CSAIL and a professor in the Department of Electrical Engineering and Computer Science. “But ensuring safety and encoding motion tasks via relatively simple objectives has always been a central challenge. We wanted to create a system where the robot can remain flexible and responsive while mathematically guaranteeing it won’t exceed safe force limits.”</p><p dir=\"ltr\"><strong>Combining soft robot models, differentiable simulation, and control theory</strong></p><p dir=\"ltr\">Underlying the control strategy is a differentiable implementation of something called the Piecewise Cosserat-Segment (PCS) dynamics model, which predicts how a soft robot deforms and where forces accumulate. This model allows the system to anticipate how the robot’s body will respond to actuation and complex interactions with the environment. “The aspect that I most like about this work is the blend of integration of new and old tools coming from different fields like advanced soft robot models, differentiable simulation, Lyapunov theory, convex optimization, and injury-severity–based safety constraints. All of this is nicely blended into a real-time controller fully grounded in first principles,” says co-author Cosimo Della Santina, who is an associate professor at Delft University of Technology.&nbsp;</p><p dir=\"ltr\">Complementing this is the Differentiable Conservative Separating Axis Theorem (DCSAT), which estimates distances between the soft robot and obstacles in the environment that can be approximated with a chain of convex polygons in a differentiable manner. “Earlier differentiable distance metrics for convex polygons either couldn’t compute penetration depth — essential for estimating contact forces — or yielded non-conservative estimates that could compromise safety,” says Wong. “Instead, the DCSAT metric returns strictly conservative, and therefore safe, estimates while simultaneously allowing for fast and differentiable computation.” Together, PCS and DCSAT give the robot a predictive sense of its environment for more proactive, safe interactions.</p><p dir=\"ltr\">Looking ahead, the team plans to extend their methods to three-dimensional soft robots and explore integration with learning-based strategies. By combining contact-aware safety with adaptive learning, soft robots could handle even more complex, unpredictable environments.&nbsp;</p><p dir=\"ltr\">“This is what makes our work exciting,” says Rus. “You can see the robot behaving in a human-like, careful manner, but behind that grace is a rigorous control framework ensuring it never oversteps its bounds.”</p><p dir=\"ltr\">“Soft robots are generally safer to interact with than rigid-bodied robots by design, due to the compliance and energy-absorbing properties of their bodies,” says University of Michigan Assistant Professor Daniel Bruder, who wasn’t involved in the research. “However, as soft robots become faster, stronger, and more capable, that may no longer be enough to ensure safety. This work takes a crucial step towards ensuring soft robots can operate safely by offering a method to limit contact forces across their entire bodies.”<br><br>The team’s work was supported, in part, by The Hong Kong Jockey Club Scholarships, the European Union’s Horizon Europe Program, Cultuurfonds Wetenschapsbeurzen, and the Rudge (1948) and Nancy Allen Chair. Their work was published earlier this month in the Institute of Electrical and Electronics Engineers’ <em>Robotics and Automation Letters</em>.</p>",
      "author": "Rachel Gordon | MIT CSAIL",
      "publishedAt": "2025-12-02T19:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "bed83a51-dfd6-4510-91c4-8befd3f26f61",
      "guid": "https://news.mit.edu/2025/aquaculture-shock-ai-and-autonomy-aquaculture-1201",
      "title": "MIT Sea Grant students explore the intersection of technology and offshore aquaculture in Norway",
      "link": "https://news.mit.edu/2025/aquaculture-shock-ai-and-autonomy-aquaculture-1201",
      "content": "<p dir=\"ltr\">Norway is the world’s largest producer of farmed Atlantic salmon and a top exporter of seafood, while the United States remains the largest importer of these products, according to the Food and Agriculture Organization. Two MIT students recently traveled to Trondheim, Norway to explore the cutting-edge technologies being developed and deployed in offshore aquaculture.&nbsp;</p><p dir=\"ltr\">Beckett Devoe, a senior in artificial intelligence and decision-making, and Tony Tang, a junior in mechanical engineering, first worked with MIT Sea Grant through the Undergraduate Research Opportunities Program (UROP). They contributed to projects focusing on wave generator design and machine learning applications for analyzing oyster larvae health in hatcheries. While near-shore aquaculture is a well-established industry across Massachusetts and the United States, open-ocean farming is still a nascent field here, facing unique and complex challenges.&nbsp;</p><p dir=\"ltr\">To help better understand this emerging industry, MIT Sea Grant created a collaborative initiative, AquaCulture Shock, with funding from an Aquaculture Technologies and Education Travel Grant through the National Sea Grant College Program. Collaborating with the MIT-Scandinavia MISTI (MIT International Science and Technology Initiatives) program, MIT Sea Grant matched Devoe and Tang with aquaculture-related summer internships at SINTEF Ocean, one of the largest research institutes in Europe.&nbsp;</p><p dir=\"ltr\">“The opportunity to work on this hands-on aquaculture project, under a world-renowned research institution, in an area of the world known for its innovation in marine technology — this is what MISTI is all about,” says Madeline Smith, managing director for MIT-Scandinavia. “Not only are students gaining valuable experience in their fields of study, but they’re developing cultural understanding and skills that equip them to be future global leaders.” Both students worked within SINTEF Ocean’s Aquaculture Robotics and Autonomous Systems Laboratory (ACE-Robotic Lab), a facility designed to develop and test new aquaculture technologies.&nbsp;</p><p dir=\"ltr\">“Norway has this unique geography where it has all of these fjords,” says Sveinung Ohrem, research manager for the Aquaculture Robotics and Automation Group at SINTEF Ocean. “So you have a lot of sheltered waters, which makes it ideal to do sea-based aquaculture.” He estimates that there are about a thousand fish farms along Norway’s coast, and walks through some of the tools being used in the industry: decision-making systems to gather and visualize data for the farmers and operators; robots for inspection and cleaning; environmental sensors to measure oxygen, temperature, and currents; echosounders that send out acoustic signals to track where the fish are; and cameras to help estimate biomass and fine-tune feeding. “Feeding is a huge challenge,” he notes. “Feed is the largest cost, by far, so optimizing feeding leads to a very significant decrease in your cost.”</p><p dir=\"ltr\">During the internship, Devoe focused on a project that uses AI for fish feeding optimization. “I try to look at the different features of the farm — so maybe how big the fish are, or how cold the water is ... and use that to try to give the farmers an optimal feeding amount for the best outcomes, while also saving money on feed,” he explains. “It was good to learn some more machine learning techniques and just get better at that on a real-world project.”&nbsp;</p><p dir=\"ltr\">In the same lab, Tang worked on the simulation of an underwater vehicle-manipulator system to navigate farms and repair damage on cage nets with a robotic arm. Ohrem says there are thousands of aquaculture robots operating in Norway today. “The scale is huge,” he says. “You can’t have 8,000 people controlling 8,000 robots — that’s not economically or practically feasible. So the level of autonomy in all of these robots needs to be increased.”</p><p dir=\"ltr\">The collaboration between MIT and SINTEF Ocean began in 2023 when MIT Sea Grant hosted Eleni Kelasidi, a visiting research scientist from the ACE-Robotic Lab. Kelasidi collaborated with MIT Sea Grant director Michael Triantafyllou and professor of mechanical engineering Themistoklis Sapsis developing controllers, models, and underwater vehicles for aquaculture, while also investigating fish-machine interactions.&nbsp;</p><p dir=\"ltr\">“We have had a long and fruitful collaboration with the Norwegian University of Science and Technology (NTNU) and SINTEF, which continues with important efforts such as the aquaculture project with Dr. Kelasidi,” Triantafyllou says. “Norway is at the forefront of offshore aquaculture and MIT Sea Grant is investing in this field, so we anticipate great results from the collaboration.”</p><p dir=\"ltr\">Kelasidi, who is now a professor at NTNU, also leads the Field Robotics Lab, focusing on developing resilient robotic systems to operate in very complex and harsh environments. “Aquaculture is one of the most challenging field domains we can demonstrate any autonomous solutions, because everything is moving,” she says. Kelasidi describes aquaculture as a deeply interdisciplinary field, requiring more students with backgrounds both in biology and technology. “We cannot develop technologies that are applied for industries where we don’t have biological components,” she explains, “and then apply them somewhere where we have a live fish or other live organisms.”&nbsp;</p><p dir=\"ltr\">Ohrem affirms that maintaining fish welfare is the primary driver for researchers and companies operating in aquaculture, especially as the industry continues to grow. “So the big question is,” he says, “how can you ensure that?” SINTEF Ocean has four research licenses for farming fish, which they operate through a collaboration with SalMar, the second-largest salmon farmer in the world. The students had the opportunity to visit one of the industrial-scale farms, Singsholmen, on the island of Hitra. The farm has 10 large, round net pens about 50 meters across that extend deep below the surface, each holding up to 200,000 salmon. “I got to physically touch the nets and see how the [robotic] arm might be able to fix the net,” says Tang.&nbsp;</p><p dir=\"ltr\">Kelasidi emphasizes that the information gained in the field cannot be learned from the office or lab. “That opens up and makes you realize, what is the scale of the challenges, or the scale of the facilities,” she says. She also highlights the importance of international and institutional collaboration to advance this field of research and develop more resilient robotic systems. “We need to try to target that problem, and let’s solve it together.”</p><p dir=\"ltr\">MIT Sea Grant and the MIT-Scandinavia MISTI program are currently recruiting a new cohort of four MIT students to intern in Norway this summer with institutes advancing offshore farming technologies, including NTNU’s Field Robotics Lab in Trondheim. Students interested in autonomy, deep learning, simulation modeling, underwater robotic systems, and other aquaculture-related areas are encouraged to reach out to <a href=\"mailto:keyes@mit.edu\">Lily Keyes</a> at MIT Sea Grant.</p>",
      "author": "Lily Keyes | MIT Sea Grant",
      "publishedAt": "2025-12-01T21:25:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "e4c2acc0-0e88-4980-9342-e248d2c7e0f1",
      "guid": "https://news.mit.edu/2025/benjamin-manning-how-ai-will-shape-future-work-1201",
      "title": "Exploring how AI will shape the future of work",
      "link": "https://news.mit.edu/2025/benjamin-manning-how-ai-will-shape-future-work-1201",
      "content": "<p dir=\"ltr\">“MIT hasn’t just prepared me for the future of work — it’s pushed me to study it. As AI systems become more capable, more of our online activity will be carried out by artificial agents. That raises big questions: How should we design these systems to understand our preferences? What happens when AI begins making many of our decisions?”</p><p dir=\"ltr\">These are some of the questions MIT Sloan School of Management PhD candidate Benjamin Manning is researching. Part of his work investigates how to design and evaluate artificial intelligence agents that act on behalf of people, and how their behavior shapes markets and institutions.&nbsp;</p><p dir=\"ltr\">Previously, he received a master’s degree in public policy from the Harvard Kennedy School and a bachelor’s in mathematics from Washington University in St. Louis. After working as a research assistant, Manning knew he wanted to pursue an academic career.</p><p dir=\"ltr\">“There’s no better place in the world to study economics and computer science than MIT,” he says. “Nobel and Turing award winners are everywhere, and the IT group lets me explore both fields freely. It was my top choice — when I was accepted, the decision was clear.”&nbsp;</p><p dir=\"ltr\">After receiving his PhD, Manning hopes to secure a faculty position at a business school and do the same type of work that MIT Sloan professors — his mentors — do every day.</p><p dir=\"ltr\">“Even in my fourth year, it still feels surreal to be an MIT student. I don’t think that feeling will ever fade. My mom definitely won’t ever get over telling people about it.”</p><p dir=\"ltr\">Of his MIT Sloan experience, Manning says he didn’t know it was possible to learn so much so quickly. “It’s no exaggeration to say I learned more in my first year as a PhD candidate than in all four years of undergrad. While the pace can be intense, wrestling with so many new ideas has been incredibly rewarding. It’s given me the tools to do novel research in economics and AI — something I never imagined I’d be capable of.”</p><p dir=\"ltr\">As an economist studying AI simulations of humans, for Manning, the future of work not only means understanding how AI acts on our behalf, but also radically improving and accelerating social scientific discovery.</p><p dir=\"ltr\">“Another part of my research agenda explores how well AI systems can simulate human responses. I envision a future where researchers test millions of behavioral simulations in minutes, rapidly prototyping experimental designs, and identifying promising research directions before investing in costly human studies. This isn’t about replacing human insight, but amplifying it: Scientists can focus on asking better questions, developing theory, and interpreting results while AI handles the computational heavy lifting.”</p><p dir=\"ltr\">He’s excited by the prospect: “We are possibly moving toward a world where the pace of understanding may get much closer to the speed of economic change.”</p>",
      "author": "Alyssa Loebig | MIT Sloan School of Management",
      "publishedAt": "2025-12-01T18:35:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "affe7736-2713-4cbf-a1d0-b8c35b4cb5f2",
      "guid": "https://news.mit.edu/2025/shortcoming-makes-llms-less-reliable-1126",
      "title": "Researchers discover a shortcoming that makes LLMs less reliable",
      "link": "https://news.mit.edu/2025/shortcoming-makes-llms-less-reliable-1126",
      "content": "<p>Large language models (LLMs) sometimes learn the wrong lessons, according to an MIT study.</p><p>Rather than answering a query based on domain knowledge, an LLM could respond by leveraging grammatical patterns it learned during training. This can cause a model to fail unexpectedly when deployed on new tasks.</p><p>The researchers found that models can mistakenly link certain sentence patterns to specific topics, so an LLM might give a convincing answer by recognizing familiar phrasing instead of understanding the question.</p><p>Their experiments showed that even the most powerful LLMs can make this mistake.</p><p>This shortcoming could reduce the reliability of LLMs that perform tasks like handling customer inquiries, summarizing clinical notes, and generating financial reports.</p><p>It could also have safety risks. A nefarious actor could exploit this to trick LLMs into producing harmful content, even when the models have safeguards to prevent such responses.</p><p>After identifying this phenomenon and exploring its implications, the researchers developed a benchmarking procedure to evaluate a model’s reliance on these incorrect correlations. The procedure could help developers mitigate the problem before deploying LLMs.</p><p>“This is a byproduct of how we train models, but models are now used in practice in safety-critical domains far beyond the tasks that created these syntactic failure modes. If you’re not familiar with model training as an end-user, this is likely to be unexpected,” says Marzyeh Ghassemi, an associate professor in the MIT Department of Electrical Engineering and Computer Science (EECS), a member of the MIT Institute of Medical Engineering Sciences and the Laboratory for Information and Decision Systems, and the senior author of the study.</p><p>Ghassemi is joined by co-lead authors Chantal Shaib, a graduate student at Northeastern University and visiting student at MIT; and Vinith Suriyakumar, an MIT graduate student; as well as Levent Sagun, a research scientist at Meta; and Byron Wallace, the Sy and Laurie Sternberg Interdisciplinary Associate Professor and associate dean of research at Northeastern University’s Khoury College of Computer Sciences. A <a href=\"https://arxiv.org/pdf/2509.21155\">paper describing the work</a> will be presented at the Conference on Neural Information Processing Systems.</p><p><strong>Stuck on syntax</strong></p><p>LLMs are trained on a massive amount of text from the internet. During this training process, the model learns to understand the relationships between words and phrases — knowledge it uses later when responding to queries.</p><p>In prior work, the researchers found that LLMs pick up patterns in the parts of speech that frequently appear together in training data. They call these part-of-speech patterns “syntactic templates.”</p><p>LLMs need this understanding of syntax, along with semantic knowledge, to answer questions in a particular domain.</p><p>“In the news domain, for instance, there is a particular style of writing. So, not only is the model learning the semantics, it is also learning the underlying structure of how sentences should be put together to follow a specific style for that domain,” Shaib explains.&nbsp; &nbsp;</p><p>But in this research, they determined that LLMs learn to associate these syntactic templates with specific domains. The model may incorrectly rely solely on this learned association when answering questions, rather than on an understanding of the query and subject matter.</p><p>For instance, an LLM might learn that a question like “Where is Paris located?” is structured as adverb/verb/proper noun/verb. If there are many examples of sentence construction in the model’s training data, the LLM may associate that syntactic template with questions about countries.</p><p>So, if the model is given a new question with the same grammatical structure but nonsense words, like “Quickly sit Paris clouded?” it might answer “France” even though that answer makes no sense.</p><p>“This is an overlooked type of association that the model learns in order to answer questions correctly. We should be paying closer attention to not only the semantics but the syntax of the data we use to train our models,” Shaib says.</p><p><strong>Missing the meaning</strong></p><p>The researchers tested this phenomenon by designing synthetic experiments in which only one syntactic template appeared in the model’s training data for each domain. They tested the models by substituting words with synonyms, antonyms, or random words, but kept the underlying syntax the same.</p><p>In each instance, they found that LLMs often still responded with the correct answer, even when the question was complete nonsense.</p><p>When they restructured the same question using a new part-of-speech pattern, the LLMs often failed to give the correct response, even though the underlying meaning of the question remained the same.</p><p>They used this approach to test pre-trained LLMs like GPT-4 and Llama, and found that this same learned behavior significantly lowered their performance.</p><p>Curious about the broader implications of these findings, the researchers studied whether someone could exploit this phenomenon to elicit harmful responses from an LLM that has been deliberately trained to refuse such requests.</p><p>They found that, by phrasing the question using a syntactic template the model associates with a “safe” dataset (one that doesn’t contain harmful information), they could trick the model into overriding its refusal policy and generating harmful content.</p><p>“From this work, it is clear to me that we need more robust defenses to address security vulnerabilities in LLMs. In this paper, we identified a new vulnerability that arises due to the way LLMs learn. So, we need to figure out new defenses based on how LLMs learn language, rather than just ad hoc solutions to different vulnerabilities,” Suriyakumar says.</p><p>While the researchers didn’t explore mitigation strategies in this work, they developed an automatic benchmarking technique one could use to evaluate an LLM’s reliance on this incorrect syntax-domain correlation. This new test could help developers proactively address this shortcoming in their models, reducing safety risks and improving performance.</p><p>In the future, the researchers want to study potential mitigation strategies, which could involve augmenting training data to provide a wider variety of syntactic templates. They are also interested in exploring this phenomenon in reasoning models, special types of LLMs designed to tackle multi-step tasks.</p><p>“I think this is a really creative angle to study failure modes of LLMs. This work highlights the importance of linguistic knowledge and analysis in LLM safety research, an aspect that hasn’t been at the center stage but clearly should be,” says Jessy Li, an associate professor at the University of Texas at Austin, who was not involved with this work.</p><p>This work is funded, in part, by a Bridgewater AIA Labs Fellowship, the National Science Foundation, the Gordon and Betty Moore Foundation, a Google Research Award, and Schmidt Sciences.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-11-26T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "6cd9e6ec-3368-4c70-8042-eb4e69a0d0a3",
      "guid": "https://news.mit.edu/2025/mit-scientists-debut-generative-ai-model-that-could-create-molecules-addressing-hard-to-treat-diseases-1125",
      "title": "MIT scientists debut a generative AI model that could create molecules addressing hard-to-treat diseases",
      "link": "https://news.mit.edu/2025/mit-scientists-debut-generative-ai-model-that-could-create-molecules-addressing-hard-to-treat-diseases-1125",
      "content": "<p dir=\"ltr\">More than 300 people across academia and industry spilled into an auditorium to attend a&nbsp;<a href=\"https://jclinic.mit.edu/events/boltzgen/\">BoltzGen seminar</a> on Thursday, Oct. 30, hosted by the&nbsp;<a href=\"https://jclinic.mit.edu/\">Abdul Latif Jameel Clinic for Machine Learning in Health</a> (MIT Jameel Clinic). Headlining the event was MIT PhD student and BoltzGen’s first author Hannes Stärk, who had announced BoltzGen just a few days prior.</p><p dir=\"ltr\">Building upon&nbsp;<a href=\"https://www.biorxiv.org/content/10.1101/2025.06.14.659707v1\">Boltz-2</a>, an open-source biomolecular structure prediction model predicting protein binding affinity that made waves over the summer,&nbsp;<a href=\"https://www.biorxiv.org/content/10.1101/2025.11.20.689494v1\">BoltzGen</a> (officially released on Sunday, Oct. 26.) is the first model of its kind to go a step further by generating novel protein binders that are ready to enter the drug discovery pipeline.</p><p dir=\"ltr\">Three key innovations make this possible: first, BoltzGen’s ability to carry out a variety of tasks, unifying protein design and structure prediction while maintaining state-of-the-art performance. Next, BoltzGen’s built-in constraints are designed with feedback from wetlab collaborators to ensure the model creates functional proteins that don’t defy the laws of physics or chemistry. Lastly, a rigorous evaluation process tests the model on “undruggable” disease targets, pushing the limits of BoltzGen’s binder generation capabilities.</p><p dir=\"ltr\">Most models used in industry or academia are capable of either structure prediction or protein design. Moreover, they’re limited to generating certain types of proteins that bind successfully to easy “targets.” Much like students responding to a test question that looks like their homework, as long as the training data looks similar to the target during binder design, the models often work. But existing methods are nearly always evaluated on targets for which structures with binders already exist, and end up faltering in performance when used on more challenging targets.</p><p dir=\"ltr\">“There have been models trying to tackle binder design, but the problem is that these models are modality-specific,” Stärk points out. “A general model does not only mean that we can address more tasks. Additionally, we obtain a better model for the individual task since emulating physics is learned by example, and with a more general training scheme, we provide more such examples containing generalizable physical patterns.”</p><p dir=\"ltr\">The BoltzGen researchers went out of their way to test BoltzGen on 26 targets, ranging from therapeutically relevant cases to ones explicitly chosen for their dissimilarity to the training data.&nbsp;</p><p dir=\"ltr\">This comprehensive validation process, which took place in eight wetlabs across academia and industry, demonstrates the model’s breadth and potential for breakthrough drug development.</p><p dir=\"ltr\">Parabilis Medicines, one of the industry collaborators that tested BoltzGen in a wetlab setting, praised BoltzGen’s potential:&nbsp;“we feel that adopting BoltzGen into our existing Helicon peptide computational platform capabilities promises to accelerate our progress to deliver transformational drugs against major human diseases.”</p><p dir=\"ltr\">While the open-source releases of Boltz-1, Boltz-2, and now BoltzGen (which was previewed at the&nbsp;<a href=\"https://www.moml.mit.edu/\">7th Molecular Machine Learning Conference</a> on Oct. 22) bring new opportunities and transparency in drug development, they also signal that biotech and pharmaceutical industries may need to reevaluate their offerings.&nbsp;</p><p dir=\"ltr\">Amid the buzz for BoltzGen on the social media platform X, Justin Grace, a principal machine learning scientist at LabGenius, raised a question. “The private-to-open performance time lag for chat AI systems is [seven] months and falling,” Grace wrote in&nbsp;<a href=\"https://x.com/jusjosgra/status/1982763802920927252\">a post</a>. “It looks to be even shorter in the protein space. How will binder-as-a-service co’s be able to [recoup] investment when we can just wait a few months for the free version?”&nbsp;</p><p dir=\"ltr\">For those in academia, BoltzGen represents an expansion and acceleration of scientific possibility.&nbsp;“A question that my students often ask me is, ‘where can AI change the therapeutics game?’” says senior co-author and MIT Professor Regina Barzilay, AI faculty lead for the Jameel Clinic and an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL). “Unless we identify undruggable targets and propose a solution, we won’t be changing the game,” she adds. “The emphasis here is on unsolved problems, which distinguishes Hannes’ work from others in the field.”&nbsp;</p><p dir=\"ltr\">Senior co-author Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science who is affiliated with the Jameel Clinic and CSAIL, notes that \"models such as BoltzGen that are released fully open-source enable broader community-wide efforts to accelerate drug design capabilities.”</p><p>Looking ahead, Stärk believes that the future of biomolecular design will be upended by AI models.&nbsp;“I want to build tools that help us manipulate biology to solve disease, or perform tasks with molecular machines that we have not even imagined yet,” he says. “I want to provide these tools and enable biologists to imagine things that they have not even thought of before.”</p>",
      "author": "Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health",
      "publishedAt": "2025-11-25T21:25:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "13985b94-0939-4eef-bf50-9050b93bbd19",
      "guid": "https://news.mit.edu/2025/cost-of-thinking-1119",
      "title": "The cost of thinking",
      "link": "https://news.mit.edu/2025/cost-of-thinking-1119",
      "content": "<p>Large language models (LLMs) like ChatGPT can write an essay or plan a menu almost instantly. But until recently, it was also easy to stump them. The models, which rely on language patterns to respond to users’ queries, often failed at math problems and were not good at complex reasoning. Suddenly, however, they’ve gotten a lot better at these things.</p><p>A new generation of LLMs known as reasoning models are being trained to solve complex problems. Like humans, they need some time to think through problems like these — and remarkably, scientists at MIT’s McGovern Institute for Brain Research have found that the kinds of problems that require the most processing from reasoning models are the very same problems that people need take their time with. In other words, they <a href=\"https://www.pnas.org/doi/10.1073/pnas.2520077122\" target=\"_blank\">report today in the journal <em>PNAS</em></a>, the “cost of thinking” for a reasoning model is similar to the cost of thinking for a human.</p><p>The researchers, who were led by <a href=\"https://mcgovern.mit.edu/profile/ev-fedorenko/\" target=\"_blank\">Evelina Fedorenko</a>, an associate professor of brain and cognitive sciences and an investigator at the McGovern Institute, conclude that in at least one important way, reasoning models have a human-like approach to thinking. That, they note, is not by design. “People who build these models don’t care if they do it like humans. They just want a system that will robustly perform under all sorts of conditions and produce correct responses,” Fedorenko says. “The fact that there’s some convergence is really quite striking.”</p><p><strong>Reasoning models</strong></p><p>Like many forms of artificial intelligence, the new reasoning models are artificial neural networks: computational tools that learn how to process information when they are given data and a problem to solve. Artificial neural networks have been very successful at many of the tasks that the brain’s own neural networks do well — and in some cases, neuroscientists have discovered that those that perform best do share certain aspects of information processing in the brain. Still, some scientists argued that artificial intelligence was not ready to take on more sophisticated aspects of human intelligence.</p><p>“Up until recently, I was among the people saying, ‘These models are really good at things like perception and language, but it’s still going to be a long ways off until we have neural network models that can do reasoning,” Fedorenko says. “Then these large reasoning models emerged and they seem to do much better at a lot of these thinking tasks, like solving math problems and writing pieces of computer code.”</p><p>Andrea Gregor de Varda, a <a href=\"https://yangtan.mit.edu/icon/\">K. Lisa Yang ICoN Center</a> Fellow and a postdoc in Fedorenko’s lab, explains that reasoning models work out problems step by step. “At some point, people realized that models needed to have more space to perform the actual computations that are needed to solve complex problems,” he says. “The performance started becoming way, way stronger if you let the models break down the problems into parts.”</p><p>To encourage models to work through complex problems in steps that lead to correct solutions, engineers can use reinforcement learning. During their training, the models are rewarded for correct answers and penalized for wrong ones. “The models explore the problem space themselves,” de Varda says. “The actions that lead to positive rewards are reinforced, so that they produce correct solutions more often.”</p><p>Models trained in this way are much more likely than their predecessors to arrive at the same answers a human would when they are given a reasoning task. Their stepwise problem-solving does mean reasoning models can take a bit longer to find an answer than the LLMs that came before — but since they’re getting right answers where the previous models would have failed, their responses are worth the wait.</p><p>The models’ need to take some time to work through complex problems already hints at a parallel to human thinking: if you demand that a person solve a hard problem instantaneously, they’d probably fail, too. De Varda wanted to examine this relationship more systematically. So he gave reasoning models and human volunteers the same set of problems, and tracked not just whether they got the answers right, but also how much time or effort it took them to get there.</p><p><strong>Time versus tokens</strong></p><p>This meant measuring how long it took people to respond to each question, down to the millisecond. For the models, Varda used a different metric. It didn’t make sense to measure processing time, since this is more dependent on computer hardware than the effort the model puts into solving a problem. So instead, he tracked tokens, which are part of a model’s internal chain of thought. “They produce tokens that are not meant for the user to see and work on, but just to have some track of the internal computation that they’re doing,” de Varda explains. “It’s as if they were talking to themselves.”</p><p>Both humans and reasoning models were asked to solve seven different types of problems, like numeric arithmetic and intuitive reasoning. For each problem class, they were given many problems. The harder a given problem was, the longer it took people to solve it — and the longer it took people to solve a problem, the more tokens a reasoning model generated as it came to its own solution.</p><p>Likewise, the classes of problems that humans took longest to solve were the same classes of problems that required the most tokens for the models: arithmetic problems were the least demanding, whereas a group of problems called the “ARC challenge,” where pairs of colored grids represent a transformation that must be inferred and then applied to a new object, were the most costly for both people and models.</p><p>De Varda and Fedorenko say the striking match in the costs of thinking demonstrates one way in which reasoning models are thinking like humans. That doesn’t mean the models are recreating human intelligence, though. The researchers still want to know whether the models use similar representations of information to the human brain, and how those representations are transformed into solutions to problems. They’re also curious whether the models will be able to handle problems that require world knowledge that is not spelled out in the texts that are used for model training.</p><p>The researchers point out that even though reasoning models generate internal monologues as they solve problems, they are not necessarily using language to think. “If you look at the output that these models produce while reasoning, it often contains errors or some nonsensical bits, even if the model ultimately arrives at a correct answer. So the actual internal computations likely take place in an abstract, non-linguistic representation space, similar to how humans don’t use language to think,” he says.</p>",
      "author": "Jennifer Michalowski | McGovern Institute for Brain Research",
      "publishedAt": "2025-11-19T21:45:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "f188cdf3-0a51-40ce-a293-dc14a0cce3d9",
      "guid": "https://news.mit.edu/2025/new-ai-agent-learns-use-cad-create-3d-objects-sketches-1119",
      "title": "New AI agent learns to use CAD to create 3D objects from sketches",
      "link": "https://news.mit.edu/2025/new-ai-agent-learns-use-cad-create-3d-objects-sketches-1119",
      "content": "<p>Computer-Aided Design (CAD) is the go-to method for designing most of today’s physical products. Engineers use CAD to turn 2D sketches into 3D models that they can then test and refine before sending a final version to a production line. But the software is notoriously complicated to learn, with thousands of commands to choose from. To be truly proficient in the software takes a huge amount of time and practice.</p><p>MIT engineers are looking to ease CAD’s learning curve with an AI model that uses CAD software much like a human would. Given a 2D sketch of an object, the model quickly creates a 3D version by clicking buttons and file options, similar to how an engineer would use the software.</p><p>The MIT team has created a new dataset called VideoCAD, which contains more than 41,000 examples of how 3D models are built in CAD software. By learning from these videos, which illustrate how different shapes and objects are constructed step-by-step, the new AI system can now operate CAD software much like a human user.</p><p>With VideoCAD, the team is building toward an&nbsp;AI-enabled “CAD co-pilot.” They envision that such a tool could not only create 3D versions of a design, but also work with a human user to suggest next steps, or automatically carry out build sequences that would otherwise be tedious and time-consuming to manually click through.</p><p>“There’s an opportunity for AI to increase engineers’ productivity as well as make CAD more accessible to more people,” says Ghadi Nehme, a graduate student in MIT’s Department of Mechanical Engineering.</p><p>“This is significant because it lowers the barrier to entry for design, helping people without years of CAD training to create 3D models more easily and tap into their creativity,” adds Faez Ahmed, associate professor of mechanical engineering at MIT.</p><p>Ahmed and Nehme, along with graduate student Brandon Man and postdoc Ferdous Alam, will present their work at the Conference on Neural Information Processing Systems (NeurIPS) in December.</p><p><strong>Click by click</strong></p><p>The team’s new work expands on recent developments in AI-driven user interface (UI) agents — tools that are trained to use software programs to carry out tasks, such as automatically gathering information online and organizing it in an Excel spreadsheet. Ahmed’s group wondered whether such UI agents could be designed to use CAD, which encompasses many more features and functions, and involves far more complicated tasks than the average UI agent can handle.</p><p>In their new work, the team aimed to design an AI-driven UI agent that takes the reins of the CAD program to create a 3D version of a 2D sketch, click by click. To do so,&nbsp;the team first looked to an existing dataset of objects that were designed in CAD by humans. Each object in the dataset includes the sequence of high-level design commands, such as “sketch line,” “circle,” and “extrude,” that were used to build the final object.</p><p>However, the team realized that these high-level commands alone were not enough to train an AI agent to actually use CAD software. A real agent must also understand the details behind each action. For instance: Which sketch region should it select? When should it zoom in? And what part of a sketch should it extrude? To bridge this gap, the researchers developed a system to translate high-level commands into user-interface interactions.</p><p>“For example, let’s say we drew a sketch by drawing a line from point 1 to point 2,” Nehme says. “We translated those high-level actions to user-interface actions, meaning we say, go from this pixel location, click, and then move to a second pixel location, and click, while having the ‘line’ operation selected.”</p><p>In the end, the team generated over 41,000 videos of human-designed CAD objects, each of which is described in real-time in terms of the specific clicks, mouse-drags, and other keyboard actions that the human originally carried out. They then fed all this data into a model they developed to learn connections between UI actions and CAD object&nbsp;generation.</p><p>Once trained on this dataset, which they dub VideoCAD, the new AI model could take a 2D sketch as input and directly control the CAD software, clicking, dragging, and selecting tools to construct the full 3D shape. The objects ranged in complexity from simple brackets to more complicated house designs. The team is training the model on more complex shapes and envisions that both the model and the dataset could one day enable CAD co-pilots for designers in a wide range of fields.</p><p>“VideoCAD is a valuable first step toward AI assistants that help onboard new users and automate the repetitive modeling work that follows familiar patterns,” says Mehdi Ataei, who was not involved in the study, and is a senior research scientist at Autodesk Research, which develops new design software tools. “This is an early foundation, and I would be excited to see successors that span multiple CAD systems, richer operations like assemblies and constraints, and more realistic, messy human workflows.”</p>",
      "author": "Jennifer Chu | MIT News",
      "publishedAt": "2025-11-19T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "e8da59c6-5b0f-4e61-9419-ba0bfcc6a0cf",
      "guid": "https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111",
      "title": "Understanding the nuances of human-like intelligence",
      "link": "https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111",
      "content": "<p>What can we learn about human intelligence by studying how machines “think?” Can we better understand ourselves if we better understand the artificial intelligence systems that are becoming a more significant part of our everyday lives?</p><p>These questions may be deeply philosophical, but for Phillip Isola, finding the answers is as much about computation as it is about cogitation.</p><p>Isola, the newly tenured associate professor in the Department of Electrical Engineering and Computer Science (EECS), studies the fundamental mechanisms involved in human-like intelligence from a computational perspective.</p><p>While understanding intelligence is the overarching goal, his work focuses mainly on computer vision and machine learning. Isola is particularly interested in exploring how intelligence emerges in AI models, how these models learn to represent the world around them, and what their “brains” share with the brains of their human creators.</p><p>“I see all the different kinds of intelligence as having a lot of commonalities, and I’d like to understand those commonalities. What is it that all animals, humans, and AIs have in common?” says Isola, who is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).</p><p>To Isola, a better scientific understanding of the intelligence that AI agents possess will help the world integrate them safely and effectively into society, maximizing their potential to benefit humanity.</p><p><strong>Asking questions</strong></p><p>Isola began pondering scientific questions at a young age.</p><p>While growing up in San Francisco, he and his father frequently went hiking along the northern California coastline or camping around Point Reyes and in the hills of Marin County.</p><p>He was fascinated by geological processes and often wondered what made the natural world work. In school, Isola was driven by an insatiable curiosity, and while he gravitated toward technical subjects like math and science, there was no limit to what he wanted to learn.</p><p>Not entirely sure what to study as an undergraduate at Yale University, Isola dabbled until he came upon cognitive sciences.</p><p>“My earlier interest had been with nature — how the world works. But then I realized that the brain was even more interesting, and more complex than even the formation of the planets. Now, I wanted to know what makes us tick,” he says.</p><p>As a first-year student, he started working in the lab of his cognitive sciences professor and soon-to-be mentor, Brian Scholl, a member of the Yale Department of Psychology. He remained in that lab throughout his time as an undergraduate.</p><p>After spending a gap year working with some childhood friends at an indie video game company, Isola was ready to dive back into the complex world of the human brain. He enrolled in the graduate program in brain and cognitive sciences at MIT.</p><p>“Grad school was where I felt like I finally found my place. I had a lot of great experiences at Yale and in other phases of my life, but when I got to MIT, I realized this was the work I really loved and these are the people who think similarly to me,” he says.</p><p>Isola credits his PhD advisor, Ted Adelson, the John and Dorothy Wilson Professor of Vision Science, as a major influence on his future path. He was inspired by Adelson’s focus on understanding fundamental principles, rather than only chasing new engineering benchmarks, which are formalized tests used to measure the performance of a system.</p><p><strong>A computational perspective</strong></p><p>At MIT, Isola’s research drifted toward computer science and artificial intelligence.</p><p>“I still loved all those questions from cognitive sciences, but I felt I could make more progress on some of those questions if I came at it from a purely computational perspective,” he says.</p><p>His thesis was focused on perceptual grouping, which involves the mechanisms people and machines use to organize discrete parts of an image as a single, coherent object.</p><p>If machines can learn perceptual groupings on their own, that could enable AI systems to recognize objects without human intervention. This type of self-supervised learning has applications in areas such autonomous vehicles, medical imaging, robotics, and automatic language translation.</p><p>After graduating from MIT, Isola completed a postdoc at the University of California at Berkeley so he could broaden his perspectives by working in a lab solely focused on computer science.</p><p>“That experience helped my work become a lot more impactful because I learned to balance understanding fundamental, abstract principles of intelligence with the pursuit of some more concrete benchmarks,” Isola recalls.</p><p>At Berkeley, he developed image-to-image translation frameworks, an early form of generative AI model that could turn a sketch into a photographic image, for instance, or turn a black-and-white photo into a color one.</p><p>He entered the academic job market and accepted a faculty position at MIT, but Isola deferred for a year to work at a then-small startup called OpenAI.</p><p>“It was a nonprofit, and I liked the idealistic mission at that time. They were really good at reinforcement learning, and I thought that seemed like an important topic to learn more about,” he says.</p><p>He enjoyed working in a lab with so much scientific freedom, but after a year Isola was ready to return to MIT and start his own research group.</p><p><strong>Studying human-like intelligence</strong></p><p>Running a research lab instantly appealed to him.</p><p>“I really love the early stage of an idea. I feel like I am a sort of startup incubator where I am constantly able to do new things and learn new things,” he says.</p><p>Building on his interest in cognitive sciences and desire to understand the human brain, his group studies the fundamental computations involved in the human-like intelligence that emerges in machines.</p><p>One primary focus is representation learning, or the ability of humans and machines to represent and perceive the sensory world around them.</p><p>In recent work, he and his collaborators observed that the many varied types of machine-learning models, from LLMs to computer vision models to audio models, seem to represent the world in similar ways.</p><p>These models are designed to do vastly different tasks, but there are many similarities in their architectures. And as they get bigger and are trained on more data, their internal structures become more alike.</p><p>This led Isola and his team to introduce the Platonic Representation Hypothesis (drawing its name from the Greek philosopher Plato) which says that the representations all these models learn are converging toward a shared, underlying representation of reality.</p><p>“Language, images, sound — all of these are different shadows on the wall from which you can infer that there is some kind of underlying physical process — some kind of causal reality — out there. If you train models on all these different types of data, they should converge on that world model in the end,” Isola says.</p><p>A related area his team studies is self-supervised learning. This involves the ways in which AI models learn to group related pixels in an image or words in a sentence without having labeled examples to learn from.</p><p>Because data are expensive and labels are limited, using only labeled data to train models could hold back the capabilities of AI systems. With self-supervised learning, the goal is to develop models that can come up with an accurate internal representation of the world on their own.</p><p>“If you can come up with a good representation of the world, that should make subsequent problem solving easier,” he explains.</p><p>The focus of Isola’s research is more about finding something new and surprising than about building complex systems that can outdo the latest machine-learning benchmarks.</p><p>While this approach has yielded much success in uncovering innovative techniques and architectures, it means the work sometimes lacks a concrete end goal, which can lead to challenges.</p><p>For instance, keeping a team aligned and the funding flowing can be difficult when the lab is focused on searching for unexpected results, he says.</p><p>“In a sense, we are always working in the dark. It is high-risk and high-reward work. Every once in while, we find some kernel of truth that is new and surprising,” he says.</p><p>In addition to pursuing knowledge, Isola is passionate about imparting knowledge to the next generation of scientists and engineers. Among his favorite courses to teach is 6.7960 (Deep Learning), which he and several other MIT faculty members launched four years ago.</p><p>The class has seen exponential growth, from 30 students in its initial offering to more than 700 this fall.</p><p>And while the popularity of AI means there is no shortage of interested students, the speed at which the field moves can make it difficult to separate the hype from truly significant advances.</p><p>“I tell the students they have to take everything we say in the class with a grain of salt. Maybe in a few years we’ll tell them something different. We are really on the edge of knowledge with this course,” he says.</p><p>But Isola also emphasizes to students that, for all the hype surrounding the latest AI models, intelligent machines are far simpler than most people suspect.</p><p>“Human ingenuity, creativity, and emotions — many people believe these can never be modeled. That might turn out to be true, but I think intelligence is fairly simple once we understand it,” he says.</p><p>Even though his current work focuses on deep-learning models, Isola is still fascinated by the complexity of the human brain and continues to collaborate with researchers who study cognitive sciences.</p><p>All the while, he has remained captivated by the beauty of the natural world that inspired his first interest in science.</p><p>Although he has less time for hobbies these days, Isola enjoys hiking and backpacking in the mountains or on Cape Cod, skiing and kayaking, or finding scenic places to spend time when he travels for scientific conferences.</p><p>And while he looks forward to exploring new questions in his lab at MIT, Isola can’t help but contemplate how the role of intelligent machines might change the course of his work.</p><p>He believes that artificial general intelligence (AGI), or the point where machines can learn and apply their knowledge as well as humans can, is not that far off.</p><p>“I don’t think AIs will just do everything for us and we’ll go and enjoy life at the beach. I think there is going to be this coexistence between smart machines and humans who still have a lot of agency and control. Now, I’m thinking about the interesting questions and applications once that happens. How can I help the world in this post-AGI future? I don’t have any answers yet, but it’s on my mind,” he says.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-11-11T05:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    },
    {
      "id": "35b183ef-0506-4890-a5fa-5749a5321d96",
      "guid": "https://news.mit.edu/2025/teaching-robots-to-map-large-environments-1105",
      "title": "Teaching robots to map large environments",
      "link": "https://news.mit.edu/2025/teaching-robots-to-map-large-environments-1105",
      "content": "<p>A robot searching for workers trapped in a partially collapsed mine shaft must rapidly generate a map of the scene and identify its location within that scene as it navigates the treacherous terrain.</p><p>Researchers have recently started building powerful machine-learning models to perform this complex task using only images from the robot’s onboard cameras, but even the best models can only process a few images at a time. In a real-world disaster where every second counts, a search-and-rescue robot would need to quickly traverse large areas and process thousands of images to complete its mission.</p><p>To overcome this problem, MIT researchers drew on ideas from both recent artificial intelligence vision models and classical computer vision to develop a new system that can process an arbitrary number of images. Their system accurately generates 3D maps of complicated scenes like a crowded office corridor in a matter of seconds.&nbsp;</p><p>The AI-driven system incrementally creates and aligns smaller submaps of the scene, which it stitches together to reconstruct a full 3D map while estimating the robot’s position in real-time.</p><p>Unlike many other approaches, their technique does not require calibrated cameras or an expert to tune a complex system implementation. The simpler nature of their approach, coupled with the speed and quality of the 3D reconstructions, would make it easier to scale up for real-world applications.</p><p>Beyond helping search-and-rescue robots navigate, this method could be used to make extended reality applications for wearable devices like VR headsets or enable industrial robots to quickly find and move goods inside a warehouse.</p><p>“For robots to accomplish increasingly complex tasks, they need much more complex map representations of the world around them. But at the same time, we don’t want to make it harder to implement these maps in practice. We’ve shown that it is possible to generate an accurate 3D reconstruction in a matter of seconds with a tool that works out of the box,” says Dominic Maggio, an MIT graduate student and lead author of a <a href=\"https://arxiv.org/pdf/2505.12549\" target=\"_blank\">paper on this method</a>.</p><p>Maggio is joined on the paper by postdoc Hyungtae Lim and senior author Luca Carlone, associate professor in MIT’s Department of Aeronautics and Astronautics (AeroAstro), principal investigator in the Laboratory for Information and Decision Systems (LIDS), and director of the MIT SPARK Laboratory. The research will be presented at the Conference on Neural Information Processing Systems.</p><p><strong>Mapping out a solution</strong></p><p>For years, researchers have been grappling with an essential element of robotic navigation called simultaneous localization and mapping (SLAM). In SLAM, a robot recreates a map of its environment while orienting itself within the space.</p><p>Traditional optimization methods for this task tend to fail in challenging scenes, or they require the robot’s onboard cameras to be calibrated beforehand. To avoid these pitfalls, researchers train machine-learning models to learn this task from data.</p><p>While they are simpler to implement, even the best models can only process about 60 camera images at a time, making them infeasible for applications where a robot needs to move quickly through a varied environment while processing thousands of images.</p><p>To solve this problem, the MIT researchers designed a system that generates smaller submaps of the scene instead of the entire map. Their method “glues” these submaps together into one overall 3D reconstruction. The model is still only processing a few images at a time, but the system can recreate larger scenes much faster by stitching smaller submaps together.</p><p>“This seemed like a very simple solution, but when I first tried it, I was surprised that it didn’t work that well,” Maggio says.</p><p>Searching for an explanation, he dug into computer vision research papers from the 1980s and 1990s. Through this analysis, Maggio realized that errors in the way the machine-learning models process images made aligning submaps a more complex problem.</p><p>Traditional methods align submaps by applying rotations and translations until they line up. But these new models can introduce some ambiguity into the submaps, which makes them harder to align. For instance, a 3D submap of a one side of a room might have walls that are slightly bent or stretched. Simply rotating and translating these deformed submaps to align them doesn’t work.</p><p>“We need to make sure all the submaps are deformed in a consistent way so we can align them well with each other,” Carlone explains.</p><p><strong>A more flexible approach</strong></p><p>Borrowing ideas from classical computer vision, the researchers developed a more flexible, mathematical technique that can represent all the deformations in these submaps. By applying mathematical transformations to each submap, this more flexible method can align them in a way that addresses the ambiguity.</p><p>Based on input images, the system outputs a 3D reconstruction of the scene and estimates of the camera locations, which the robot would use to localize itself in the space.</p><p>“Once Dominic had the intuition to bridge these two worlds — learning-based approaches and traditional optimization methods — the implementation was fairly straightforward,” Carlone says. “Coming up with something this effective and simple has potential for a lot of applications.</p><p>Their system performed faster with less reconstruction error than other methods, without requiring special cameras or additional tools to process data. The researchers generated close-to-real-time 3D reconstructions of complex scenes like the inside of the MIT Chapel using only short videos captured on a cell phone.</p><p>The average error in these 3D reconstructions was less than 5 centimeters.</p><p>In the future, the researchers want to make their method more reliable for especially complicated scenes and work toward implementing it on real robots in challenging settings.</p><p>“Knowing about traditional geometry pays off. If you understand deeply what is going on in the model, you can get much better results and make things much more scalable,” Carlone says.</p><p>This work is supported, in part, by the U.S. National Science Foundation, U.S. Office of Naval Research, and the National Research Foundation of Korea. Carlone, currently on sabbatical as an Amazon Scholar, completed this work before he joined Amazon.</p>",
      "author": "Adam Zewe | MIT News",
      "publishedAt": "2025-11-05T15:00:00.000Z",
      "isRead": false,
      "createdAt": "2026-02-16T01:51:36.598Z"
    }
  ]
}